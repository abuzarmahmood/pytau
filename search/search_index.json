{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTau This is a python package to perform streamlined, batched inference for changepoint models over parameter grids and datasets, and efficiently analyze sets of fitted models. Models PyTau uses changepoint models to detect shifts in data distributions. These models are designed to identify points in time where the statistical properties of a sequence of observations change. The package supports various types of changepoint models, including Gaussian and Poisson models. Each model is optimized for different types of data and use cases. Available Models Gaussian Changepoint Models These models detect changes in both mean and variance or only in the mean for Gaussian data. They are suitable for continuous data where changes in statistical properties like mean and variance are of interest. gaussian_changepoint_mean_var_2d : Detects changes in both mean and variance. gaussian_changepoint_mean_dirichlet : Uses a Dirichlet process prior to determine the number of states, detecting changes only in the mean. gaussian_changepoint_mean_2d : Detects changes only in the mean. Poisson Changepoint Models These models are used for spike train data, detecting changes in the rate of events. They are ideal for count data where the rate of occurrence is the primary focus. single_taste_poisson : Models changepoints for single taste without hierarchical structure. single_taste_poisson_dirichlet : Uses a Dirichlet process prior for single taste spike train data. single_taste_poisson_varsig : Uses variable sigmoid slope inferred from data. single_taste_poisson_varsig_fixed : Uses sigmoid with given slope. all_taste_poisson : Fits changepoints across multiple tastes. all_taste_poisson_varsig_fixed : Fits changepoints across multiple tastes with fixed sigmoid slope. single_taste_poisson_trial_switch : Assumes only emissions change across trials. all_taste_poisson_trial_switch : Fits changepoints across multiple tastes with trial switching. Fitting Methods PyTau provides several fitting methods for these models: ADVI (Automatic Differentiation Variational Inference) : A method for approximating the posterior distribution. MCMC (Markov Chain Monte Carlo) : A sampling method for drawing samples from the posterior distribution. DPP (Dirichlet Process Prior) : Used for models with a Dirichlet process prior to determine the number of states. Installation To set up PyTau, follow these steps: # Create and activate conda environment conda create -n \"pytau_env\" python=3.6.13 ipython notebook -y conda activate pytau_env # Clone repository cd ~/Desktop git clone https://github.com/abuzarmahmood/pytau.git # Install requirements from specified file cd pytau pip install -r requirements.txt # Test everything is working by running notebook cd pytau/how_to bash scripts/download_test_data.sh cd notebooks jupyter notebook # Run a notebook Usage PyTau provides a streamlined workflow for fitting changepoint models to neural data. Here's a basic example of how to use the package: import pytau from pytau.changepoint_model import single_taste_poisson, advi_fit # Load your spike data (trials x neurons x time) spike_array = load_your_data() # Create a model with desired number of states model = single_taste_poisson(spike_array, states=3) # Fit the model using ADVI model, approx, lambda_stack, tau_samples, data = advi_fit(model, fit=10000, samples=1000) # Analyze the results # tau_samples contains the changepoint times # lambda_stack contains the emission rates For more detailed usage examples, refer to the notebooks in the pytau/how_to/notebooks directory. Workflows A typical workflow with PyTau involves the following steps: Data Preparation : Organize your spike train data into the required format (typically trials x neurons x time). Model Selection : Choose the appropriate changepoint model based on your data characteristics (e.g., single_taste_poisson for single taste data). Inference : Run the model to detect changepoints using methods like ADVI or MCMC. Analysis : Analyze the results to understand the detected changepoints and their implications. PyTau also provides tools for batch processing and database management to streamline the analysis of large datasets: FitHandler : Handles the pipeline of model fitting including loading data, preprocessing, fitting models, and saving results. DatabaseHandler : Manages transactions with the model database, allowing for efficient storage and retrieval of fitted models. Data Organization PyTau organizes fitted models in a central location, accessed by indexing an info file that contains model parameters and metadata. The metadata is also stored in the model file to enable recreation of the info file if needed. The database for fitted models includes columns for: - Model save path - Animal Name - Session date - Taste Name - Region Name - Experiment name - Fit Date - Model parameters (Model Type, Data Type, States Num, Fit steps, Time lims, Bin Width) Data stored in models includes: - Model - Approximation - Lambda (emission rates) - Tau (changepoint times) - Data used to fit model - Raw data (before preprocessing) - Model and data details/parameters Pipeline PyTau's pipeline consists of several components: Model file : Generates models and performs inference (e.g., changepoint_model.py) Data pre-processing code : Handles different data types (shuffle, simulated, actual) I/O helper code : Loads data, processes it, and feeds it to modeling code Run script : Iterates over data for batch processing Parallelization Parallelization is currently performed using GNU Parallel by setting a separate Theano compiledir for each job. This prevents compilation clashes and allows for efficient batch processing of multiple models.","title":"Start"},{"location":"#pytau","text":"This is a python package to perform streamlined, batched inference for changepoint models over parameter grids and datasets, and efficiently analyze sets of fitted models.","title":"PyTau"},{"location":"#models","text":"PyTau uses changepoint models to detect shifts in data distributions. These models are designed to identify points in time where the statistical properties of a sequence of observations change. The package supports various types of changepoint models, including Gaussian and Poisson models. Each model is optimized for different types of data and use cases.","title":"Models"},{"location":"#available-models","text":"","title":"Available Models"},{"location":"#gaussian-changepoint-models","text":"These models detect changes in both mean and variance or only in the mean for Gaussian data. They are suitable for continuous data where changes in statistical properties like mean and variance are of interest. gaussian_changepoint_mean_var_2d : Detects changes in both mean and variance. gaussian_changepoint_mean_dirichlet : Uses a Dirichlet process prior to determine the number of states, detecting changes only in the mean. gaussian_changepoint_mean_2d : Detects changes only in the mean.","title":"Gaussian Changepoint Models"},{"location":"#poisson-changepoint-models","text":"These models are used for spike train data, detecting changes in the rate of events. They are ideal for count data where the rate of occurrence is the primary focus. single_taste_poisson : Models changepoints for single taste without hierarchical structure. single_taste_poisson_dirichlet : Uses a Dirichlet process prior for single taste spike train data. single_taste_poisson_varsig : Uses variable sigmoid slope inferred from data. single_taste_poisson_varsig_fixed : Uses sigmoid with given slope. all_taste_poisson : Fits changepoints across multiple tastes. all_taste_poisson_varsig_fixed : Fits changepoints across multiple tastes with fixed sigmoid slope. single_taste_poisson_trial_switch : Assumes only emissions change across trials. all_taste_poisson_trial_switch : Fits changepoints across multiple tastes with trial switching.","title":"Poisson Changepoint Models"},{"location":"#fitting-methods","text":"PyTau provides several fitting methods for these models: ADVI (Automatic Differentiation Variational Inference) : A method for approximating the posterior distribution. MCMC (Markov Chain Monte Carlo) : A sampling method for drawing samples from the posterior distribution. DPP (Dirichlet Process Prior) : Used for models with a Dirichlet process prior to determine the number of states.","title":"Fitting Methods"},{"location":"#installation","text":"To set up PyTau, follow these steps: # Create and activate conda environment conda create -n \"pytau_env\" python=3.6.13 ipython notebook -y conda activate pytau_env # Clone repository cd ~/Desktop git clone https://github.com/abuzarmahmood/pytau.git # Install requirements from specified file cd pytau pip install -r requirements.txt # Test everything is working by running notebook cd pytau/how_to bash scripts/download_test_data.sh cd notebooks jupyter notebook # Run a notebook","title":"Installation"},{"location":"#usage","text":"PyTau provides a streamlined workflow for fitting changepoint models to neural data. Here's a basic example of how to use the package: import pytau from pytau.changepoint_model import single_taste_poisson, advi_fit # Load your spike data (trials x neurons x time) spike_array = load_your_data() # Create a model with desired number of states model = single_taste_poisson(spike_array, states=3) # Fit the model using ADVI model, approx, lambda_stack, tau_samples, data = advi_fit(model, fit=10000, samples=1000) # Analyze the results # tau_samples contains the changepoint times # lambda_stack contains the emission rates For more detailed usage examples, refer to the notebooks in the pytau/how_to/notebooks directory.","title":"Usage"},{"location":"#workflows","text":"A typical workflow with PyTau involves the following steps: Data Preparation : Organize your spike train data into the required format (typically trials x neurons x time). Model Selection : Choose the appropriate changepoint model based on your data characteristics (e.g., single_taste_poisson for single taste data). Inference : Run the model to detect changepoints using methods like ADVI or MCMC. Analysis : Analyze the results to understand the detected changepoints and their implications. PyTau also provides tools for batch processing and database management to streamline the analysis of large datasets: FitHandler : Handles the pipeline of model fitting including loading data, preprocessing, fitting models, and saving results. DatabaseHandler : Manages transactions with the model database, allowing for efficient storage and retrieval of fitted models.","title":"Workflows"},{"location":"#data-organization","text":"PyTau organizes fitted models in a central location, accessed by indexing an info file that contains model parameters and metadata. The metadata is also stored in the model file to enable recreation of the info file if needed. The database for fitted models includes columns for: - Model save path - Animal Name - Session date - Taste Name - Region Name - Experiment name - Fit Date - Model parameters (Model Type, Data Type, States Num, Fit steps, Time lims, Bin Width) Data stored in models includes: - Model - Approximation - Lambda (emission rates) - Tau (changepoint times) - Data used to fit model - Raw data (before preprocessing) - Model and data details/parameters","title":"Data Organization"},{"location":"#pipeline","text":"PyTau's pipeline consists of several components: Model file : Generates models and performs inference (e.g., changepoint_model.py) Data pre-processing code : Handles different data types (shuffle, simulated, actual) I/O helper code : Loads data, processes it, and feeds it to modeling code Run script : Iterates over data for batch processing","title":"Pipeline"},{"location":"#parallelization","text":"Parallelization is currently performed using GNU Parallel by setting a separate Theano compiledir for each job. This prevents compilation clashes and allows for efficient batch processing of multiple models.","title":"Parallelization"},{"location":"api/","text":"References === Analysis functions === Helper classes and functions to perform analysis on fitted models PklHandler Helper class to handle metadata and fit data from pkl file Source code in pytau/changepoint_analysis.py class PklHandler: \"\"\"Helper class to handle metadata and fit data from pkl file\"\"\" def __init__(self, file_path): \"\"\"Initialize PklHandler class Args: file_path (str): Path to pkl file \"\"\" self.dir_name = os.path.dirname(file_path) file_name = os.path.basename(file_path) self.file_name_base = file_name.split(\".\")[0] self.pkl_file_path = os.path.join( self.dir_name, self.file_name_base + \".pkl\") with open(self.pkl_file_path, \"rb\") as this_file: self.data = pkl.load(this_file) model_keys = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] key_savenames = [ \"_model_structure\", \"_fit_model\", \"lambda_array\", \"tau_array\", \"processed_spikes\", ] data_map = dict(zip(model_keys, key_savenames)) for key, var_name in data_map.items(): if key in self.data[\"model_data\"]: setattr(self, var_name, self.data[\"model_data\"][key]) else: # Set to None if key is missing (e.g., due to pickling fallback) setattr(self, var_name, None) self.metadata = self.data[\"metadata\"] self.pretty_metadata = pd.json_normalize(self.data[\"metadata\"]).T # Get number of trials from processed_spikes for proper tau formatting n_trials = self.processed_spikes.shape[0] if hasattr( self.processed_spikes, 'shape') else None self.tau = _tau(self.tau_array, self.metadata, n_trials) self.firing = _firing(self.tau, self.processed_spikes, self.metadata) __init__(file_path) Initialize PklHandler class Parameters: Name Type Description Default file_path str Path to pkl file required Source code in pytau/changepoint_analysis.py def __init__(self, file_path): \"\"\"Initialize PklHandler class Args: file_path (str): Path to pkl file \"\"\" self.dir_name = os.path.dirname(file_path) file_name = os.path.basename(file_path) self.file_name_base = file_name.split(\".\")[0] self.pkl_file_path = os.path.join( self.dir_name, self.file_name_base + \".pkl\") with open(self.pkl_file_path, \"rb\") as this_file: self.data = pkl.load(this_file) model_keys = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] key_savenames = [ \"_model_structure\", \"_fit_model\", \"lambda_array\", \"tau_array\", \"processed_spikes\", ] data_map = dict(zip(model_keys, key_savenames)) for key, var_name in data_map.items(): if key in self.data[\"model_data\"]: setattr(self, var_name, self.data[\"model_data\"][key]) else: # Set to None if key is missing (e.g., due to pickling fallback) setattr(self, var_name, None) self.metadata = self.data[\"metadata\"] self.pretty_metadata = pd.json_normalize(self.data[\"metadata\"]).T # Get number of trials from processed_spikes for proper tau formatting n_trials = self.processed_spikes.shape[0] if hasattr( self.processed_spikes, 'shape') else None self.tau = _tau(self.tau_array, self.metadata, n_trials) self.firing = _firing(self.tau, self.processed_spikes, self.metadata) calc_significant_neurons_firing(state_firing, p_val=0.05) Calculate significant changes in firing rate between states Iterate ANOVA over neurons for all states With Bonferroni correction Args state_firing (3D Numpy array): trials x states x nrns p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: Name Type Description anova_p_val_array 1D Numpy array p-values for each neuron anova_sig_neurons 1D Numpy array indices of significant neurons Source code in pytau/changepoint_analysis.py def calc_significant_neurons_firing(state_firing, p_val=0.05): \"\"\"Calculate significant changes in firing rate between states Iterate ANOVA over neurons for all states With Bonferroni correction Args state_firing (3D Numpy array): trials x states x nrns p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: anova_p_val_array (1D Numpy array): p-values for each neuron anova_sig_neurons (1D Numpy array): indices of significant neurons \"\"\" n_neurons = state_firing.shape[-1] # Calculate ANOVA p-values for each neuron anova_p_val_array = np.zeros(state_firing.shape[-1]) for neuron in range(state_firing.shape[-1]): anova_p_val_array[neuron] = f_oneway(*state_firing[:, :, neuron].T)[1] anova_sig_neurons = np.where(anova_p_val_array < p_val / n_neurons)[0] return anova_p_val_array, anova_sig_neurons calc_significant_neurons_snippets(transition_snips, p_val=0.05) Calculate pairwise t-tests to detect differences between each transition With Bonferroni correction Args transition_snips (4D Numpy array): trials x nrns x bins x transitions p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: Name Type Description anova_p_val_array ( neurons , transition ) p-values for each neuron anova_sig_neurons ( neurons , transition ) indices of significant neurons Source code in pytau/changepoint_analysis.py def calc_significant_neurons_snippets(transition_snips, p_val=0.05): \"\"\"Calculate pairwise t-tests to detect differences between each transition With Bonferroni correction Args transition_snips (4D Numpy array): trials x nrns x bins x transitions p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: anova_p_val_array (neurons, transition): p-values for each neuron anova_sig_neurons (neurons, transition): indices of significant neurons \"\"\" # Calculate pairwise t-tests for each transition # shape : [before, after] x trials x neurons x transitions mean_transition_snips = np.stack(np.array_split( transition_snips, 2, axis=2)).mean(axis=3) pairwise_p_val_array = np.zeros(mean_transition_snips.shape[2:]) n_neuron, n_transitions = pairwise_p_val_array.shape for neuron in range(n_neuron): for transition in range(n_transitions): pairwise_p_val_array[neuron, transition] = ttest_rel( *mean_transition_snips[:, :, neuron, transition] )[1] pairwise_sig_neurons = pairwise_p_val_array < p_val # /n_neuron return pairwise_p_val_array, pairwise_sig_neurons get_state_firing(spike_array, tau_array) Calculate firing rates within states given changepoint positions on data Parameters: Name Type Description Default spike_array 3D Numpy array trials x nrns x bins required tau_array 2D Numpy array trials x switchpoints required Returns: Name Type Description state_firing 3D Numpy array trials x states x nrns Source code in pytau/changepoint_analysis.py def get_state_firing(spike_array, tau_array): \"\"\"Calculate firing rates within states given changepoint positions on data Args: spike_array (3D Numpy array): trials x nrns x bins tau_array (2D Numpy array): trials x switchpoints Returns: state_firing (3D Numpy array): trials x states x nrns \"\"\" states = tau_array.shape[-1] + 1 # Get mean firing rate for each STATE using model state_inds = np.hstack( [ np.zeros((tau_array.shape[0], 1)), tau_array, np.ones((tau_array.shape[0], 1)) * spike_array.shape[-1], ] ) state_lims = np.array([state_inds[:, x: x + 2] for x in range(states)]) state_lims = np.vectorize(int)(state_lims) state_lims = np.swapaxes(state_lims, 0, 1) state_firing = np.array( [ [np.mean(trial_dat[:, start:end], axis=-1) for start, end in trial_lims] for trial_dat, trial_lims in zip(spike_array, state_lims) ] ) state_firing = np.nan_to_num(state_firing) return state_firing get_transition_snips(spike_array, tau_array, window_radius=300) Get snippets of activty around changepoints for each trial Parameters: Name Type Description Default spike_array 3D Numpy array trials x nrns x bins required tau_array 2D Numpy array trials x switchpoints required Returns: Type Description Numpy array: Transition snippets : trials x nrns x bins x transitions Make sure none of the snippets are outside the bounds of the data Source code in pytau/changepoint_analysis.py def get_transition_snips(spike_array, tau_array, window_radius=300): \"\"\"Get snippets of activty around changepoints for each trial Args: spike_array (3D Numpy array): trials x nrns x bins tau_array (2D Numpy array): trials x switchpoints Returns: Numpy array: Transition snippets : trials x nrns x bins x transitions Make sure none of the snippets are outside the bounds of the data \"\"\" # Get snippets of activity around changepoints for each trial n_trials, n_neurons, n_bins = spike_array.shape n_transitions = tau_array.shape[1] transition_snips = np.zeros( (n_trials, n_neurons, 2 * window_radius, n_transitions)) window_lims = np.stack( [tau_array - window_radius, tau_array + window_radius], axis=-1) # Make sure no lims are outside the bounds of the data if (window_lims < 0).sum(axis=None) or (window_lims > n_bins).sum(axis=None): raise ValueError(\"Transition window extends outside data bounds\") # Pull out snippets for trial in range(n_trials): for transition in range(n_transitions): transition_snips[trial, :, :, transition] = spike_array[ trial, :, window_lims[trial, transition, 0]: window_lims[trial, transition, 1], ] return transition_snips === I/O functions === Pipeline to handle model fitting from data extraction to saving results DatabaseHandler Class to handle transactions with model database Source code in pytau/changepoint_io.py class DatabaseHandler: \"\"\"Class to handle transactions with model database\"\"\" def __init__(self): \"\"\"Initialize DatabaseHandler class\"\"\" self.unique_cols = [\"exp.model_id\", \"exp.save_path\", \"exp.fit_date\"] self.model_database_path = MODEL_DATABASE_PATH self.model_save_base_dir = MODEL_SAVE_DIR if os.path.exists(self.model_database_path): self.fit_database = pd.read_csv( self.model_database_path, index_col=0) all_na = [all(x) for num, x in self.fit_database.isna().iterrows()] if all_na: print(f\"{sum(all_na)} rows found with all NA, removing...\") self.fit_database = self.fit_database.dropna(how=\"all\") else: print(\"Fit database does not exist yet\") def show_duplicates(self, keep=\"first\"): \"\"\"Find duplicates in database Args: keep (str, optional): Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. Returns: pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows \"\"\" dup_inds = self.fit_database.drop( self.unique_cols, axis=1).duplicated(keep=keep) return self.fit_database.loc[dup_inds], dup_inds def drop_duplicates(self): \"\"\"Remove duplicated rows from database\"\"\" _, dup_inds = self.show_duplicates() print(f\"Removing {sum(dup_inds)} duplicate rows\") self.fit_database = self.fit_database.loc[~dup_inds] def check_mismatched_paths(self): \"\"\"Check if there are any mismatched pkl files between database and directory Returns: pandas dataframe: Dataframe containing rows for which pkl file not present list: pkl files which cannot be matched to model in database list: all files in save directory \"\"\" mismatch_from_database = [ not os.path.exists(x + \".pkl\") for x in self.fit_database[\"exp.save_path\"] ] file_list = glob(os.path.join(self.model_save_base_dir, \"*/*.pkl\")) # Only split basename by '.' in case there are multiple '.' in filenpath mismatch_from_file = [ not ( os.path.join( os.path.dirname(x), os.path.basename(x).split(\".\")[0]) in list(self.fit_database[\"exp.save_path\"])) for x in file_list ] print( f\"{sum(mismatch_from_database)} mismatches from database\" + \"\\n\" + f\"{sum(mismatch_from_file)} mismatches from files\" ) return mismatch_from_database, mismatch_from_file, file_list def clear_mismatched_paths(self): \"\"\"Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found \"\"\" ( mismatch_from_database, mismatch_from_file, file_list, ) = self.check_mismatched_paths() mismatch_from_file = np.array(mismatch_from_file) mismatch_from_database = np.array(mismatch_from_database) self.fit_database = self.fit_database.loc[~mismatch_from_database] mismatched_files = [x for x, y in zip( file_list, mismatch_from_file) if y] for x in mismatched_files: os.remove(x) print(\"==== Clearing Completed ====\") def write_updated_database(self): \"\"\"Can be called following clear_mismatched_entries to update current database\"\"\" database_backup_dir = os.path.join( self.model_save_base_dir, \".database_backups\") if not os.path.exists(database_backup_dir): os.makedirs(database_backup_dir) # current_date = date.today().strftime(\"%m-%d-%y\") current_date = str(datetime.now()).replace(\" \", \"_\") shutil.copy( self.model_database_path, os.path.join(database_backup_dir, f\"database_backup_{current_date}\"), ) self.fit_database.to_csv(self.model_database_path, mode=\"w\") def set_run_params(self, data_dir, experiment_name, taste_num, laser_type, region_name): \"\"\"Store metadata related to inference run Args: data_dir (str): Path to directory containing HDF5 file experiment_name (str): Name given to fitted batch (for metedata). Defaults to None. taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) laser_type (None or str): None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) region_name (str): Region on which to perform fit on (must match regions in .info file) \"\"\" self.data_dir = data_dir self.data_basename = os.path.basename(self.data_dir) self.animal_name = self.data_basename.split(\"_\")[0] self.session_date = self.data_basename.split(\"_\")[-1] self.experiment_name = experiment_name self.model_save_dir = os.path.join( self.model_save_base_dir, experiment_name) if not os.path.exists(self.model_save_dir): os.makedirs(self.model_save_dir) self.model_id = str(uuid.uuid4()).split(\"-\")[0] self.model_save_path = os.path.join( self.model_save_dir, self.experiment_name + \"_\" + self.model_id ) self.fit_date = date.today().strftime(\"%m-%d-%y\") self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.fit_exists = None def ingest_fit_data(self, met_dict): \"\"\"Load external metadata Args: met_dict (dict): Dictionary of metadata from FitHandler class \"\"\" self.external_metadata = met_dict def aggregate_metadata(self): \"\"\"Collects information regarding data and current \"experiment\" Raises: Exception: If 'external_metadata' has not been ingested, that needs to be done first Returns: dict: Dictionary of metadata given to FitHandler class \"\"\" if \"external_metadata\" not in dir(self): raise Exception( \"Fit run metdata needs to be ingested \" \"into data_handler first\") data_details = dict( zip( [ \"data_dir\", \"basename\", \"animal_name\", \"session_date\", \"taste_num\", \"laser_type\", \"region_name\", ], [ self.data_dir, self.data_basename, self.animal_name, self.session_date, self.taste_num, self.laser_type, self.region_name, ], ) ) exp_details = dict( zip( [\"exp_name\", \"model_id\", \"save_path\", \"fit_date\"], [ self.experiment_name, self.model_id, self.model_save_path, self.fit_date, ], ) ) module_details = dict( zip( [\"pymc_version\", \"theano_version\"], [pymc.__version__, theano.__version__], ) ) temp_ext_met = self.external_metadata temp_ext_met[\"data\"] = data_details temp_ext_met[\"exp\"] = exp_details temp_ext_met[\"module\"] = module_details return temp_ext_met def write_to_database(self): \"\"\"Write out metadata to database\"\"\" agg_metadata = self.aggregate_metadata() # Convert model_kwargs to str so that they are save appropriately agg_metadata[\"model\"][\"model_kwargs\"] = str( agg_metadata[\"model\"][\"model_kwargs\"]) flat_metadata = pd.json_normalize(agg_metadata) if not os.path.isfile(self.model_database_path): flat_metadata.to_csv(self.model_database_path, mode=\"a\") else: flat_metadata.to_csv(self.model_database_path, mode=\"a\", header=False) print(f\"Updated model database @ {self.model_database_path}\") def check_exists(self): \"\"\"Check if the given fit already exists in database Returns: bool: Boolean for whether fit already exists or not \"\"\" if self.fit_exists is not None: return self.fit_exists __init__() Initialize DatabaseHandler class Source code in pytau/changepoint_io.py def __init__(self): \"\"\"Initialize DatabaseHandler class\"\"\" self.unique_cols = [\"exp.model_id\", \"exp.save_path\", \"exp.fit_date\"] self.model_database_path = MODEL_DATABASE_PATH self.model_save_base_dir = MODEL_SAVE_DIR if os.path.exists(self.model_database_path): self.fit_database = pd.read_csv( self.model_database_path, index_col=0) all_na = [all(x) for num, x in self.fit_database.isna().iterrows()] if all_na: print(f\"{sum(all_na)} rows found with all NA, removing...\") self.fit_database = self.fit_database.dropna(how=\"all\") else: print(\"Fit database does not exist yet\") aggregate_metadata() Collects information regarding data and current \"experiment\" Raises: Type Description Exception If 'external_metadata' has not been ingested, that needs to be done first Returns: Name Type Description dict Dictionary of metadata given to FitHandler class Source code in pytau/changepoint_io.py def aggregate_metadata(self): \"\"\"Collects information regarding data and current \"experiment\" Raises: Exception: If 'external_metadata' has not been ingested, that needs to be done first Returns: dict: Dictionary of metadata given to FitHandler class \"\"\" if \"external_metadata\" not in dir(self): raise Exception( \"Fit run metdata needs to be ingested \" \"into data_handler first\") data_details = dict( zip( [ \"data_dir\", \"basename\", \"animal_name\", \"session_date\", \"taste_num\", \"laser_type\", \"region_name\", ], [ self.data_dir, self.data_basename, self.animal_name, self.session_date, self.taste_num, self.laser_type, self.region_name, ], ) ) exp_details = dict( zip( [\"exp_name\", \"model_id\", \"save_path\", \"fit_date\"], [ self.experiment_name, self.model_id, self.model_save_path, self.fit_date, ], ) ) module_details = dict( zip( [\"pymc_version\", \"theano_version\"], [pymc.__version__, theano.__version__], ) ) temp_ext_met = self.external_metadata temp_ext_met[\"data\"] = data_details temp_ext_met[\"exp\"] = exp_details temp_ext_met[\"module\"] = module_details return temp_ext_met check_exists() Check if the given fit already exists in database Returns: Name Type Description bool Boolean for whether fit already exists or not Source code in pytau/changepoint_io.py def check_exists(self): \"\"\"Check if the given fit already exists in database Returns: bool: Boolean for whether fit already exists or not \"\"\" if self.fit_exists is not None: return self.fit_exists check_mismatched_paths() Check if there are any mismatched pkl files between database and directory Returns: Name Type Description pandas dataframe: Dataframe containing rows for which pkl file not present list pkl files which cannot be matched to model in database list all files in save directory Source code in pytau/changepoint_io.py def check_mismatched_paths(self): \"\"\"Check if there are any mismatched pkl files between database and directory Returns: pandas dataframe: Dataframe containing rows for which pkl file not present list: pkl files which cannot be matched to model in database list: all files in save directory \"\"\" mismatch_from_database = [ not os.path.exists(x + \".pkl\") for x in self.fit_database[\"exp.save_path\"] ] file_list = glob(os.path.join(self.model_save_base_dir, \"*/*.pkl\")) # Only split basename by '.' in case there are multiple '.' in filenpath mismatch_from_file = [ not ( os.path.join( os.path.dirname(x), os.path.basename(x).split(\".\")[0]) in list(self.fit_database[\"exp.save_path\"])) for x in file_list ] print( f\"{sum(mismatch_from_database)} mismatches from database\" + \"\\n\" + f\"{sum(mismatch_from_file)} mismatches from files\" ) return mismatch_from_database, mismatch_from_file, file_list clear_mismatched_paths() Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found Source code in pytau/changepoint_io.py def clear_mismatched_paths(self): \"\"\"Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found \"\"\" ( mismatch_from_database, mismatch_from_file, file_list, ) = self.check_mismatched_paths() mismatch_from_file = np.array(mismatch_from_file) mismatch_from_database = np.array(mismatch_from_database) self.fit_database = self.fit_database.loc[~mismatch_from_database] mismatched_files = [x for x, y in zip( file_list, mismatch_from_file) if y] for x in mismatched_files: os.remove(x) print(\"==== Clearing Completed ====\") drop_duplicates() Remove duplicated rows from database Source code in pytau/changepoint_io.py def drop_duplicates(self): \"\"\"Remove duplicated rows from database\"\"\" _, dup_inds = self.show_duplicates() print(f\"Removing {sum(dup_inds)} duplicate rows\") self.fit_database = self.fit_database.loc[~dup_inds] ingest_fit_data(met_dict) Load external metadata Parameters: Name Type Description Default met_dict dict Dictionary of metadata from FitHandler class required Source code in pytau/changepoint_io.py def ingest_fit_data(self, met_dict): \"\"\"Load external metadata Args: met_dict (dict): Dictionary of metadata from FitHandler class \"\"\" self.external_metadata = met_dict set_run_params(data_dir, experiment_name, taste_num, laser_type, region_name) Store metadata related to inference run Parameters: Name Type Description Default data_dir str Path to directory containing HDF5 file required experiment_name str Name given to fitted batch (for metedata). Defaults to None. required taste_num int Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) required laser_type None or str None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) required region_name str Region on which to perform fit on (must match regions in .info file) required Source code in pytau/changepoint_io.py def set_run_params(self, data_dir, experiment_name, taste_num, laser_type, region_name): \"\"\"Store metadata related to inference run Args: data_dir (str): Path to directory containing HDF5 file experiment_name (str): Name given to fitted batch (for metedata). Defaults to None. taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) laser_type (None or str): None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) region_name (str): Region on which to perform fit on (must match regions in .info file) \"\"\" self.data_dir = data_dir self.data_basename = os.path.basename(self.data_dir) self.animal_name = self.data_basename.split(\"_\")[0] self.session_date = self.data_basename.split(\"_\")[-1] self.experiment_name = experiment_name self.model_save_dir = os.path.join( self.model_save_base_dir, experiment_name) if not os.path.exists(self.model_save_dir): os.makedirs(self.model_save_dir) self.model_id = str(uuid.uuid4()).split(\"-\")[0] self.model_save_path = os.path.join( self.model_save_dir, self.experiment_name + \"_\" + self.model_id ) self.fit_date = date.today().strftime(\"%m-%d-%y\") self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.fit_exists = None show_duplicates(keep='first') Find duplicates in database Parameters: Name Type Description Default keep str Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. 'first' Returns: Type Description pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows Source code in pytau/changepoint_io.py def show_duplicates(self, keep=\"first\"): \"\"\"Find duplicates in database Args: keep (str, optional): Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. Returns: pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows \"\"\" dup_inds = self.fit_database.drop( self.unique_cols, axis=1).duplicated(keep=keep) return self.fit_database.loc[dup_inds], dup_inds write_to_database() Write out metadata to database Source code in pytau/changepoint_io.py def write_to_database(self): \"\"\"Write out metadata to database\"\"\" agg_metadata = self.aggregate_metadata() # Convert model_kwargs to str so that they are save appropriately agg_metadata[\"model\"][\"model_kwargs\"] = str( agg_metadata[\"model\"][\"model_kwargs\"]) flat_metadata = pd.json_normalize(agg_metadata) if not os.path.isfile(self.model_database_path): flat_metadata.to_csv(self.model_database_path, mode=\"a\") else: flat_metadata.to_csv(self.model_database_path, mode=\"a\", header=False) print(f\"Updated model database @ {self.model_database_path}\") write_updated_database() Can be called following clear_mismatched_entries to update current database Source code in pytau/changepoint_io.py def write_updated_database(self): \"\"\"Can be called following clear_mismatched_entries to update current database\"\"\" database_backup_dir = os.path.join( self.model_save_base_dir, \".database_backups\") if not os.path.exists(database_backup_dir): os.makedirs(database_backup_dir) # current_date = date.today().strftime(\"%m-%d-%y\") current_date = str(datetime.now()).replace(\" \", \"_\") shutil.copy( self.model_database_path, os.path.join(database_backup_dir, f\"database_backup_{current_date}\"), ) self.fit_database.to_csv(self.model_database_path, mode=\"w\") FitHandler Class to handle pipeline of model fitting including: 1) Loading data 2) Preprocessing loaded arrays 3) Fitting model 4) Writing out fitted parameters to pkl file Source code in pytau/changepoint_io.py class FitHandler: \"\"\"Class to handle pipeline of model fitting including: 1) Loading data 2) Preprocessing loaded arrays 3) Fitting model 4) Writing out fitted parameters to pkl file \"\"\" def __init__( self, data_dir, taste_num, region_name, laser_type=None, experiment_name=None, model_params_path=None, preprocess_params_path=None, ): \"\"\"Initialize FitHandler class Args: data_dir (str): Path to directory containing HDF5 file taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) region_name (str): Region on which to perform fit on (must match regions in .info file) experiment_name (str, optional): Name given to fitted batch (for metedata). Defaults to None. model_params_path (str, optional): Path to json file containing model parameters. Defaults to None. preprocess_params_path (str, optional): Path to json file containing preprocessing parameters. Defaults to None. Raises: Exception: If \"experiment_name\" is None Exception: If \"laser_type\" is not in [None, 'on', 'off'] Exception: If \"taste_num\" is not integer or \"all\" \"\"\" # =============== Check for exceptions =============== if experiment_name is None: raise Exception(\"Please specify an experiment name\") if laser_type not in [None, \"on\", \"off\"]: raise Exception('laser_type must be from [None, \"on\",\"off\"]') if not (isinstance(taste_num, int) or taste_num == \"all\"): raise Exception('taste_num must be an integer or \"all\"') # =============== Save relevant arguments =============== self.data_dir = data_dir self.EphysData = EphysData(self.data_dir) # self.data = self.EphysData.get_spikes({\"bla\",\"gc\",\"all\"}) self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.experiment_name = experiment_name data_handler_init_kwargs = dict( zip( [ \"data_dir\", \"experiment_name\", \"taste_num\", \"laser_type\", \"region_name\", ], [data_dir, experiment_name, taste_num, laser_type, region_name], ) ) self.database_handler = DatabaseHandler() self.database_handler.set_run_params(**data_handler_init_kwargs) if model_params_path is None: print(\"MODEL_PARAMS will have to be set\") else: self.set_model_params(file_path=model_params_path) if preprocess_params_path is None: print(\"PREPROCESS_PARAMS will have to be set\") else: self.set_preprocess_params(file_path=preprocess_params_path) ######################################## # SET PARAMS ######################################## def set_preprocess_params(self, time_lims, bin_width, data_transform, file_path=None): \"\"\"Load given params as \"preprocess_params\" attribute Args: time_lims (array/tuple/list): Start and end of where to cut spike train array bin_width (int): Bin width for binning spikes to counts data_transform (str): Indicator for which transformation to use (refer to changepoint_preprocess) file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: preprocess_params_dict = dict( zip( [\"time_lims\", \"bin_width\", \"data_transform\"], [time_lims, bin_width, data_transform], ) ) self.preprocess_params = preprocess_params_dict print(\"Set preprocess params to: {}\".format(preprocess_params_dict)) else: # Load json and save dict pass def set_model_params(self, states, fit, samples, model_kwargs=None, file_path=None): \"\"\"Load given params as \"model_params\" attribute Args: states (int): Number of states to use in model fit (int): Iterations to use for model fitting (given ADVI fit) samples (int): Number of samples to return from fitten model model_kwargs (dict) : Additional paramters for model file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: model_params_dict = dict( zip( [\"states\", \"fit\", \"samples\", \"model_kwargs\"], [states, fit, samples, model_kwargs], ) ) self.model_params = model_params_dict print(\"Set model params to: {}\".format(model_params_dict)) else: # Load json and save dict pass ######################################## # SET PIPELINE FUNCS ######################################## def set_preprocessor(self, preprocessing_func): \"\"\"Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Args: preprocessing_func (func): Function to preprocess data (refer to changepoint_preprocess) \"\"\" self.preprocessor = preprocessing_func def preprocess_selector(self): \"\"\"Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): self.set_preprocessor( changepoint_preprocess.preprocess_single_taste) elif self.taste_num == \"all\": self.set_preprocessor(changepoint_preprocess.preprocess_all_taste) else: raise Exception(\"Something went wrong\") def set_model_template(self, model_template): \"\"\"Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Args: model_template (func): Function to generate model template for data] \"\"\" self.model_template = model_template def model_template_selector(self): \"\"\"Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): # self.set_model_template(changepoint_model.single_taste_poisson_varsig) self.set_model_template(changepoint_model.single_taste_poisson) elif self.taste_num == \"all\": self.set_model_template(changepoint_model.all_taste_poisson) else: raise Exception(\"Something went wrong\") def set_inference(self, inference_func): \"\"\"Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Args: inference_func (func): Function to use for fitting model \"\"\" self.inference_func = changepoint_model.advi_fit def inference_func_selector(self): \"\"\"Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" \"\"\" self.set_inference(changepoint_model.advi_fit) ######################################## # PIPELINE FUNCS ######################################## def load_spike_trains(self): \"\"\"Helper function to load spike trains from data_dir using EphysData module\"\"\" full_spike_array = self.EphysData.return_region_spikes( region_name=self.region_name, laser=self.laser_type ) if isinstance(self.taste_num, int): self.data = full_spike_array[self.taste_num] if self.taste_num == \"all\": self.data = full_spike_array print( f\"Loading spike trains from {self.database_handler.data_basename}, \" f\"dig_in {self.taste_num}, laser {str(self.laser_type)}\" ) def preprocess_data(self): \"\"\"Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected \"\"\" if \"data\" not in dir(self): self.load_spike_trains() if \"preprocessor\" not in dir(self): self.preprocess_selector() print( \"Preprocessing spike trains, \" f\"preprocessing func: <{self.preprocessor.__name__}>\") self.preprocessed_data = self.preprocessor( self.data, **self.preprocess_params) def create_model(self): \"\"\"Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected \"\"\" if \"preprocessed_data\" not in dir(self): self.preprocess_data() if \"model_template\" not in dir(self): self.model_template_selector() # In future iterations, before fitting model, # check that a similar entry doesn't exist print( f\"Generating Model, model func: <{self.model_template.__name__}>\") self.model = self.model_template( self.preprocessed_data, self.model_params[\"states\"], **self.model_params[\"model_kwargs\"], ) def run_inference(self): \"\"\"Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected \"\"\" if \"model\" not in dir(self): self.create_model() if \"inference_func\" not in dir(self): self.inference_func_selector() print( \"Running inference, inference func: \" f\"<{self.inference_func.__name__}>\") temp_outs = self.inference_func( self.model, self.model_params[\"fit\"], self.model_params[\"samples\"] ) varnames = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] self.inference_outs = dict(zip(varnames, temp_outs)) def _gen_fit_metadata(self): \"\"\"Generate metadata for fit Generate metadat by compiling: 1) Preprocess parameters given as input 2) Model parameters given as input 3) Functions used in inference pipeline for : preprocessing, model generation, fitting Returns: dict: Dictionary containing compiled metadata for different parts of inference pipeline \"\"\" pre_params = self.preprocess_params model_params = self.model_params pre_params[\"preprocessor_name\"] = self.preprocessor.__name__ model_params[\"model_template_name\"] = self.model_template.__name__ model_params[\"inference_func_name\"] = self.inference_func.__name__ fin_dict = dict(zip([\"preprocess\", \"model\"], [pre_params, model_params])) return fin_dict def _pass_metadata_to_handler(self): \"\"\"Function to coordinate transfer of metadata to DatabaseHandler\"\"\" self.database_handler.ingest_fit_data(self._gen_fit_metadata()) def _return_fit_output(self): \"\"\"Compile data, model, fit, and metadata to save output Returns: dict: Dictionary containing fitted model data and metadata \"\"\" self._pass_metadata_to_handler() agg_metadata = self.database_handler.aggregate_metadata() return {\"model_data\": self.inference_outs, \"metadata\": agg_metadata} def save_fit_output(self): \"\"\"Save fit output (fitted data + metadata) to pkl file\"\"\" if \"inference_outs\" not in dir(self): self.run_inference() out_dict = self._return_fit_output() # Save output to pkl file with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: pickle.dump(out_dict, buff) print( f\"Saved full output to {self.database_handler.model_save_path}.pkl\") # # Create a copy without the model to avoid pickling issues with PyMC5 # picklable_dict = out_dict.copy() # if \"model_data\" in picklable_dict and \"model\" in picklable_dict[\"model_data\"]: # picklable_model_data = picklable_dict[\"model_data\"].copy() # # Remove the model object as it contains unpicklable local functions in PyMC5 # picklable_model_data.pop(\"model\", None) # picklable_dict[\"model_data\"] = picklable_model_data # # with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: # try: # pickle.dump(picklable_dict, buff) # except (TypeError, AttributeError) as e: # print( # f\"Warning: Full pickling failed ({e}). Saving metadata-only version.\") # # If pickling fails, save only metadata and basic info # model_data_fallback = { # \"tau_array\": picklable_dict.get(\"model_data\", {}).get(\"tau_array\"), # \"processed_spikes\": picklable_dict.get(\"model_data\", {}).get(\"processed_spikes\"), # } # # # Try to save approx.hist for ELBO plotting if available # approx_obj = picklable_dict.get(\"model_data\", {}).get(\"approx\") # if approx_obj and hasattr(approx_obj, 'hist'): # try: # # Create a simple object with just the hist attribute # model_data_fallback[\"approx\"] = SimpleApprox( # approx_obj.hist) # except Exception: # # If even hist fails to pickle, skip it # pass # # metadata_only_dict = { # \"metadata\": picklable_dict.get(\"metadata\", {}), # \"model_data\": model_data_fallback # } # pickle.dump(metadata_only_dict, buff) json_file_name = os.path.join( self.database_handler.model_save_path + \".info\") with open(json_file_name, \"w\") as file: json.dump(out_dict[\"metadata\"], file, indent=4) self.database_handler.write_to_database() print( \"Saving inference output to : \\n\" f\"{self.database_handler.model_save_dir}\" \"\\n\" + \"================================\" + \"\\n\" ) __init__(data_dir, taste_num, region_name, laser_type=None, experiment_name=None, model_params_path=None, preprocess_params_path=None) Initialize FitHandler class Parameters: Name Type Description Default data_dir str Path to directory containing HDF5 file required taste_num int Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) required region_name str Region on which to perform fit on (must match regions in .info file) required experiment_name str Name given to fitted batch (for metedata). Defaults to None. None model_params_path str Path to json file containing model parameters. Defaults to None. None preprocess_params_path str Path to json file containing preprocessing parameters. Defaults to None. None Raises: Type Description Exception If \"experiment_name\" is None Exception If \"laser_type\" is not in [None, 'on', 'off'] Exception If \"taste_num\" is not integer or \"all\" Source code in pytau/changepoint_io.py def __init__( self, data_dir, taste_num, region_name, laser_type=None, experiment_name=None, model_params_path=None, preprocess_params_path=None, ): \"\"\"Initialize FitHandler class Args: data_dir (str): Path to directory containing HDF5 file taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) region_name (str): Region on which to perform fit on (must match regions in .info file) experiment_name (str, optional): Name given to fitted batch (for metedata). Defaults to None. model_params_path (str, optional): Path to json file containing model parameters. Defaults to None. preprocess_params_path (str, optional): Path to json file containing preprocessing parameters. Defaults to None. Raises: Exception: If \"experiment_name\" is None Exception: If \"laser_type\" is not in [None, 'on', 'off'] Exception: If \"taste_num\" is not integer or \"all\" \"\"\" # =============== Check for exceptions =============== if experiment_name is None: raise Exception(\"Please specify an experiment name\") if laser_type not in [None, \"on\", \"off\"]: raise Exception('laser_type must be from [None, \"on\",\"off\"]') if not (isinstance(taste_num, int) or taste_num == \"all\"): raise Exception('taste_num must be an integer or \"all\"') # =============== Save relevant arguments =============== self.data_dir = data_dir self.EphysData = EphysData(self.data_dir) # self.data = self.EphysData.get_spikes({\"bla\",\"gc\",\"all\"}) self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.experiment_name = experiment_name data_handler_init_kwargs = dict( zip( [ \"data_dir\", \"experiment_name\", \"taste_num\", \"laser_type\", \"region_name\", ], [data_dir, experiment_name, taste_num, laser_type, region_name], ) ) self.database_handler = DatabaseHandler() self.database_handler.set_run_params(**data_handler_init_kwargs) if model_params_path is None: print(\"MODEL_PARAMS will have to be set\") else: self.set_model_params(file_path=model_params_path) if preprocess_params_path is None: print(\"PREPROCESS_PARAMS will have to be set\") else: self.set_preprocess_params(file_path=preprocess_params_path) create_model() Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected Source code in pytau/changepoint_io.py def create_model(self): \"\"\"Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected \"\"\" if \"preprocessed_data\" not in dir(self): self.preprocess_data() if \"model_template\" not in dir(self): self.model_template_selector() # In future iterations, before fitting model, # check that a similar entry doesn't exist print( f\"Generating Model, model func: <{self.model_template.__name__}>\") self.model = self.model_template( self.preprocessed_data, self.model_params[\"states\"], **self.model_params[\"model_kwargs\"], ) inference_func_selector() Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" Source code in pytau/changepoint_io.py def inference_func_selector(self): \"\"\"Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" \"\"\" self.set_inference(changepoint_model.advi_fit) load_spike_trains() Helper function to load spike trains from data_dir using EphysData module Source code in pytau/changepoint_io.py def load_spike_trains(self): \"\"\"Helper function to load spike trains from data_dir using EphysData module\"\"\" full_spike_array = self.EphysData.return_region_spikes( region_name=self.region_name, laser=self.laser_type ) if isinstance(self.taste_num, int): self.data = full_spike_array[self.taste_num] if self.taste_num == \"all\": self.data = full_spike_array print( f\"Loading spike trains from {self.database_handler.data_basename}, \" f\"dig_in {self.taste_num}, laser {str(self.laser_type)}\" ) model_template_selector() Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Type Description Exception If self.taste_num is neither int nor str Source code in pytau/changepoint_io.py def model_template_selector(self): \"\"\"Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): # self.set_model_template(changepoint_model.single_taste_poisson_varsig) self.set_model_template(changepoint_model.single_taste_poisson) elif self.taste_num == \"all\": self.set_model_template(changepoint_model.all_taste_poisson) else: raise Exception(\"Something went wrong\") preprocess_data() Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected Source code in pytau/changepoint_io.py def preprocess_data(self): \"\"\"Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected \"\"\" if \"data\" not in dir(self): self.load_spike_trains() if \"preprocessor\" not in dir(self): self.preprocess_selector() print( \"Preprocessing spike trains, \" f\"preprocessing func: <{self.preprocessor.__name__}>\") self.preprocessed_data = self.preprocessor( self.data, **self.preprocess_params) preprocess_selector() Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Type Description Exception If self.taste_num is neither int nor str Source code in pytau/changepoint_io.py def preprocess_selector(self): \"\"\"Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): self.set_preprocessor( changepoint_preprocess.preprocess_single_taste) elif self.taste_num == \"all\": self.set_preprocessor(changepoint_preprocess.preprocess_all_taste) else: raise Exception(\"Something went wrong\") run_inference() Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected Source code in pytau/changepoint_io.py def run_inference(self): \"\"\"Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected \"\"\" if \"model\" not in dir(self): self.create_model() if \"inference_func\" not in dir(self): self.inference_func_selector() print( \"Running inference, inference func: \" f\"<{self.inference_func.__name__}>\") temp_outs = self.inference_func( self.model, self.model_params[\"fit\"], self.model_params[\"samples\"] ) varnames = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] self.inference_outs = dict(zip(varnames, temp_outs)) save_fit_output() Save fit output (fitted data + metadata) to pkl file Source code in pytau/changepoint_io.py def save_fit_output(self): \"\"\"Save fit output (fitted data + metadata) to pkl file\"\"\" if \"inference_outs\" not in dir(self): self.run_inference() out_dict = self._return_fit_output() # Save output to pkl file with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: pickle.dump(out_dict, buff) print( f\"Saved full output to {self.database_handler.model_save_path}.pkl\") # # Create a copy without the model to avoid pickling issues with PyMC5 # picklable_dict = out_dict.copy() # if \"model_data\" in picklable_dict and \"model\" in picklable_dict[\"model_data\"]: # picklable_model_data = picklable_dict[\"model_data\"].copy() # # Remove the model object as it contains unpicklable local functions in PyMC5 # picklable_model_data.pop(\"model\", None) # picklable_dict[\"model_data\"] = picklable_model_data # # with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: # try: # pickle.dump(picklable_dict, buff) # except (TypeError, AttributeError) as e: # print( # f\"Warning: Full pickling failed ({e}). Saving metadata-only version.\") # # If pickling fails, save only metadata and basic info # model_data_fallback = { # \"tau_array\": picklable_dict.get(\"model_data\", {}).get(\"tau_array\"), # \"processed_spikes\": picklable_dict.get(\"model_data\", {}).get(\"processed_spikes\"), # } # # # Try to save approx.hist for ELBO plotting if available # approx_obj = picklable_dict.get(\"model_data\", {}).get(\"approx\") # if approx_obj and hasattr(approx_obj, 'hist'): # try: # # Create a simple object with just the hist attribute # model_data_fallback[\"approx\"] = SimpleApprox( # approx_obj.hist) # except Exception: # # If even hist fails to pickle, skip it # pass # # metadata_only_dict = { # \"metadata\": picklable_dict.get(\"metadata\", {}), # \"model_data\": model_data_fallback # } # pickle.dump(metadata_only_dict, buff) json_file_name = os.path.join( self.database_handler.model_save_path + \".info\") with open(json_file_name, \"w\") as file: json.dump(out_dict[\"metadata\"], file, indent=4) self.database_handler.write_to_database() print( \"Saving inference output to : \\n\" f\"{self.database_handler.model_save_dir}\" \"\\n\" + \"================================\" + \"\\n\" ) set_inference(inference_func) Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Parameters: Name Type Description Default inference_func func Function to use for fitting model required Source code in pytau/changepoint_io.py def set_inference(self, inference_func): \"\"\"Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Args: inference_func (func): Function to use for fitting model \"\"\" self.inference_func = changepoint_model.advi_fit set_model_params(states, fit, samples, model_kwargs=None, file_path=None) Load given params as \"model_params\" attribute Parameters: Name Type Description Default states int Number of states to use in model required fit int Iterations to use for model fitting (given ADVI fit) required samples int Number of samples to return from fitten model required model_kwargs (dict) Additional paramters for model required file_path str Path to json file containing preprocess parameters. Defaults to None. None Source code in pytau/changepoint_io.py def set_model_params(self, states, fit, samples, model_kwargs=None, file_path=None): \"\"\"Load given params as \"model_params\" attribute Args: states (int): Number of states to use in model fit (int): Iterations to use for model fitting (given ADVI fit) samples (int): Number of samples to return from fitten model model_kwargs (dict) : Additional paramters for model file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: model_params_dict = dict( zip( [\"states\", \"fit\", \"samples\", \"model_kwargs\"], [states, fit, samples, model_kwargs], ) ) self.model_params = model_params_dict print(\"Set model params to: {}\".format(model_params_dict)) else: # Load json and save dict pass set_model_template(model_template) Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Parameters: Name Type Description Default model_template func Function to generate model template for data] required Source code in pytau/changepoint_io.py def set_model_template(self, model_template): \"\"\"Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Args: model_template (func): Function to generate model template for data] \"\"\" self.model_template = model_template set_preprocess_params(time_lims, bin_width, data_transform, file_path=None) Load given params as \"preprocess_params\" attribute Parameters: Name Type Description Default time_lims array / tuple / list Start and end of where to cut spike train array required bin_width int Bin width for binning spikes to counts required data_transform str Indicator for which transformation to use (refer to changepoint_preprocess) required file_path str Path to json file containing preprocess parameters. Defaults to None. None Source code in pytau/changepoint_io.py def set_preprocess_params(self, time_lims, bin_width, data_transform, file_path=None): \"\"\"Load given params as \"preprocess_params\" attribute Args: time_lims (array/tuple/list): Start and end of where to cut spike train array bin_width (int): Bin width for binning spikes to counts data_transform (str): Indicator for which transformation to use (refer to changepoint_preprocess) file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: preprocess_params_dict = dict( zip( [\"time_lims\", \"bin_width\", \"data_transform\"], [time_lims, bin_width, data_transform], ) ) self.preprocess_params = preprocess_params_dict print(\"Set preprocess params to: {}\".format(preprocess_params_dict)) else: # Load json and save dict pass set_preprocessor(preprocessing_func) Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Parameters: Name Type Description Default preprocessing_func func Function to preprocess data (refer to changepoint_preprocess) required Source code in pytau/changepoint_io.py def set_preprocessor(self, preprocessing_func): \"\"\"Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Args: preprocessing_func (func): Function to preprocess data (refer to changepoint_preprocess) \"\"\" self.preprocessor = preprocessing_func SimpleApprox Simple approximation object that only stores the hist attribute for ELBO plotting Source code in pytau/changepoint_io.py class SimpleApprox: \"\"\"Simple approximation object that only stores the hist attribute for ELBO plotting\"\"\" def __init__(self, hist): self.hist = hist === Model building functions === pymc Blackbox Variational Inference implementation of Poisson Likelihood Changepoint for spike trains. AllTastePoisson Bases: ChangepointModel ** Model to fit changepoint to all tastes ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" Source code in pytau/changepoint_model.py class AllTastePoisson(ChangepointModel): \"\"\" ** Model to fit changepoint to all tastes ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoisson passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoisson passed\") return True AllTastePoissonTrialSwitch Bases: ChangepointModel Assuming only emissions change across trials Changepoint distribution remains constant Source code in pytau/changepoint_model.py class AllTastePoissonTrialSwitch(ChangepointModel): \"\"\" Assuming only emissions change across trials Changepoint distribution remains constant \"\"\" def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states tastes, trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # ================================================= # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # tastes x nrns taste_lambda = pm.Exponential( \"taste_lambda\", nrn_lambda.dimshuffle(\"x\", 0), shape=(tastes, nrn_num) ) # tastes x nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", taste_lambda.dimshuffle(0, 1, \"x\"), shape=(tastes, nrn_num, switch_components), ) # tastes x nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, 2, \"x\"), shape=(tastes, nrn_num, switch_components, n_states), ) # Define Changepoints # ================================================= # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(tastes, trial_num, n_states - 1), ).sort(axis=-1) # Tasets x Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints # a_trial = pm.HalfCauchy('a_trial', 3., shape = switch_components - 1) # b_trial = pm.HalfCauchy('b_trial', 3., shape = switch_components - 1) even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints # ================================================= tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: tastes x nrns x switch_comps x states # selected_trial_lambda : tastes x nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"tastes\" x \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", \"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, 3, \"x\"), axis=2, ), ) # First, we can \"select\" sets of emissions depending on trial_changepoints # ================================================= trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # Then, we can select state_emissions for every trial # ================================================= idx = np.arange(time_bins) # tau : Tastes x Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes, trial_num, 1, time_bins)), weight_stack], axis=2 ) inverse_stack = 1 - weight_stack[:, :, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes, trial_num, 1, time_bins))], axis=2 ) # Tastes x Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Putting everything together # ================================================= # selected_trial_lambda : tastes x nrns x states x trials # Convert selected_trial_lambda --> tastes x trials x nrns x states x \"time\" # weight_stack : tastes x trials x states x time # Convert weight_stack --> tastes x trials x \"nrns\" x states x time # tastes x trials x nrns x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 3, 1, 2, \"x\") * weight_stack.dimshuffle(0, 1, \"x\", 2, 3), axis=3, ) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames assert \"taste_lambda\" in trace.varnames print(\"Test for AllTastePoissonTrialSwitch passed\") return True __init__(data_array, switch_components, n_states, **kwargs) Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required switch_components int Number of trial switch components required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states tastes, trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # ================================================= # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # tastes x nrns taste_lambda = pm.Exponential( \"taste_lambda\", nrn_lambda.dimshuffle(\"x\", 0), shape=(tastes, nrn_num) ) # tastes x nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", taste_lambda.dimshuffle(0, 1, \"x\"), shape=(tastes, nrn_num, switch_components), ) # tastes x nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, 2, \"x\"), shape=(tastes, nrn_num, switch_components, n_states), ) # Define Changepoints # ================================================= # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(tastes, trial_num, n_states - 1), ).sort(axis=-1) # Tasets x Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints # a_trial = pm.HalfCauchy('a_trial', 3., shape = switch_components - 1) # b_trial = pm.HalfCauchy('b_trial', 3., shape = switch_components - 1) even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints # ================================================= tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: tastes x nrns x switch_comps x states # selected_trial_lambda : tastes x nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"tastes\" x \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", \"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, 3, \"x\"), axis=2, ), ) # First, we can \"select\" sets of emissions depending on trial_changepoints # ================================================= trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # Then, we can select state_emissions for every trial # ================================================= idx = np.arange(time_bins) # tau : Tastes x Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes, trial_num, 1, time_bins)), weight_stack], axis=2 ) inverse_stack = 1 - weight_stack[:, :, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes, trial_num, 1, time_bins))], axis=2 ) # Tastes x Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Putting everything together # ================================================= # selected_trial_lambda : tastes x nrns x states x trials # Convert selected_trial_lambda --> tastes x trials x nrns x states x \"time\" # weight_stack : tastes x trials x states x time # Convert weight_stack --> tastes x trials x \"nrns\" x states x time # tastes x trials x nrns x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 3, 1, 2, \"x\") * weight_stack.dimshuffle(0, 1, \"x\", 2, 3), axis=3, ) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames assert \"taste_lambda\" in trace.varnames print(\"Test for AllTastePoissonTrialSwitch passed\") return True AllTastePoissonVarsigFixed Bases: ChangepointModel ** Model to fit changepoint to all tastes with fixed sigmoid ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" Source code in pytau/changepoint_model.py class AllTastePoissonVarsigFixed(ChangepointModel): \"\"\" ** Model to fit changepoint to all tastes with fixed sigmoid ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" \"\"\" def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model inds_span(float): Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = sigmoid(idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoissonVarsigFixed passed\") return True __init__(data_array, n_states, inds_span=1, **kwargs) Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required n_states int Number of states to model required inds_span float Number of indices to cover 5-95% change in sigmoid 1 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model inds_span(float): Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = sigmoid(idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoissonVarsigFixed passed\") return True CategoricalChangepoint2D Bases: ChangepointModel Model for categorical data changepoint detection on 2D arrays. Source code in pytau/changepoint_model.py class CategoricalChangepoint2D(ChangepointModel): \"\"\"Model for categorical data changepoint detection on 2D arrays.\"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): trials x length - Each element is a postive integer representing a category n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) # Make sure data array is int if not np.issubdtype(data_array.dtype, np.integer): raise ValueError( \"Data array must contain integer category values.\") # Check that data_array is 2D if data_array.ndim != 2: # If 3D, take the first trial/dimension to make it 2D if data_array.ndim == 3: data_array = data_array[0] else: raise ValueError(\"Data array must be 2D (trials x length).\") self.data_array = data_array self.n_states = n_states def generate_model(self): data_array = self.data_array n_states = self.n_states trials, length = data_array.shape features = len(np.unique(data_array)) # If features in data_array are not continuous integer values, map them feature_set = np.unique(data_array) if not np.array_equal(feature_set, np.arange(len(feature_set))): # Create a mapping from original categories to continuous integers category_map = {cat: i for i, cat in enumerate(feature_set)} data_array = np.vectorize(category_map.get)(data_array) idx = np.arange(length) flat_data_array = data_array.reshape((trials * length,)) with pm.Model() as model: p = pm.Dirichlet(\"p\", a=np.ones( (n_states, features)), shape=(n_states, features)) # Infer changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Shape: trials x changepoints tau_latent = pm.Beta(\"tau_latent\", a_tau, b_tau, shape=(trials, n_states - 1)).sort( axis=-1 ) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) # shapes: # - weight_stack: trials x states x length # - p : states x features # shape: trials x length x features lambda_ = tt.tensordot(weight_stack, p, [1, 0]) flat_lambda = lambda_.reshape((trials * length, features)) # Use categorical likelihood # data_array = trials x length category = pm.Categorical( \"category\", p=flat_lambda, observed=flat_data_array) return model def test(self): test_data = np.random.randint(0, self.n_states, size=(5, 100)) test_model = CategoricalChangepoint2D(test_data, self.n_states) model = test_model.generate_model() with model: inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) assert \"p\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for CategoricalChangepoint2D passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 2D Numpy array trials x length - Each element is a postive integer representing a category required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): trials x length - Each element is a postive integer representing a category n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) # Make sure data array is int if not np.issubdtype(data_array.dtype, np.integer): raise ValueError( \"Data array must contain integer category values.\") # Check that data_array is 2D if data_array.ndim != 2: # If 3D, take the first trial/dimension to make it 2D if data_array.ndim == 3: data_array = data_array[0] else: raise ValueError(\"Data array must be 2D (trials x length).\") self.data_array = data_array self.n_states = n_states ChangepointModel Base class for all changepoint models Source code in pytau/changepoint_model.py class ChangepointModel: \"\"\"Base class for all changepoint models\"\"\" def __init__(self, **kwargs): \"\"\"Initialize model with keyword arguments\"\"\" self.kwargs = kwargs def generate_model(self): \"\"\"Generate pymc model - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement generate_model()\") def test(self): \"\"\"Test model functionality - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement test()\") __init__(**kwargs) Initialize model with keyword arguments Source code in pytau/changepoint_model.py def __init__(self, **kwargs): \"\"\"Initialize model with keyword arguments\"\"\" self.kwargs = kwargs generate_model() Generate pymc model - to be implemented by subclasses Source code in pytau/changepoint_model.py def generate_model(self): \"\"\"Generate pymc model - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement generate_model()\") test() Test model functionality - to be implemented by subclasses Source code in pytau/changepoint_model.py def test(self): \"\"\"Test model functionality - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement test()\") GaussianChangepointMean2D Bases: ChangepointModel Model for gaussian data on 2D array detecting changes only in the mean. Source code in pytau/changepoint_model.py class GaussianChangepointMean2D(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes only in the mean. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dimshuffle(0, \"x\") observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMean2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMean2D passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 2D Numpy array x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dimshuffle(0, \"x\") observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMean2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMean2D passed\") return True GaussianChangepointMeanDirichlet Bases: ChangepointModel Model for gaussian data on 2D array detecting changes only in the mean. Number of states determined using dirichlet process prior. Source code in pytau/changepoint_model.py class GaussianChangepointMeanDirichlet(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes only in the mean. Number of states determined using dirichlet process prior. \"\"\" def __init__(self, data_array, max_states=15, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time max_states (int): Max number of states to include in truncated dirichlet process **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob test_std = np.std(data_array, axis=-1) with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Normal( \"lambda\", mu=mean_vals, sigma=10, shape=(y_dim, max_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", test_std, shape=(y_dim)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=max_states) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic(\"w_raw\", stick_breaking(beta)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic(\"w_latent\", w_raw / w_raw.sum()) tau = pm.Deterministic(\"tau\", tt.cumsum(w_latent * length)[:-1]) # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) # Create timeseries for latent variable (mean emission) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot( lambda_latent, weight_stack, axes=(1, 0)) ) sigma_latent = sigma.dimshuffle(0, \"x\") # Likelihood for observations observation = pm.Normal( \"obs\", mu=lambda_, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((10, 100), n_states=3, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for GaussianChangepointMeanDirichlet passed\") return True __init__(data_array, max_states=15, **kwargs) Parameters: Name Type Description Default data_array 2D Numpy array x time required max_states int Max number of states to include in truncated dirichlet process 15 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, max_states=15, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time max_states (int): Max number of states to include in truncated dirichlet process **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob test_std = np.std(data_array, axis=-1) with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Normal( \"lambda\", mu=mean_vals, sigma=10, shape=(y_dim, max_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", test_std, shape=(y_dim)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=max_states) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic(\"w_raw\", stick_breaking(beta)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic(\"w_latent\", w_raw / w_raw.sum()) tau = pm.Deterministic(\"tau\", tt.cumsum(w_latent * length)[:-1]) # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) # Create timeseries for latent variable (mean emission) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot( lambda_latent, weight_stack, axes=(1, 0)) ) sigma_latent = sigma.dimshuffle(0, \"x\") # Likelihood for observations observation = pm.Normal( \"obs\", mu=lambda_, sigma=sigma_latent, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((10, 100), n_states=3, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for GaussianChangepointMeanDirichlet passed\") return True GaussianChangepointMeanVar2D Bases: ChangepointModel Model for gaussian data on 2D array detecting changes in both mean and variance. Source code in pytau/changepoint_model.py class GaussianChangepointMeanVar2D(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes in both mean and variance. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dot(weight_stack) observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanVar2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMeanVar2D passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 2D Numpy array x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dot(weight_stack) observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanVar2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMeanVar2D passed\") return True PoissonChangepoint1D Bases: ChangepointModel Model for changepoint detection in 1D Poisson time series This model detects changepoints in 1D time series data using a Poisson likelihood. It assumes the data follows a Poisson distribution with different rates in different segments separated by changepoints. Source code in pytau/changepoint_model.py class PoissonChangepoint1D(ChangepointModel): \"\"\"Model for changepoint detection in 1D Poisson time series This model detects changepoints in 1D time series data using a Poisson likelihood. It assumes the data follows a Poisson distribution with different rates in different segments separated by changepoints. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (1D Numpy array): Time series data n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = np.asarray(data_array) if self.data_array.ndim != 1: raise ValueError(\"data_array must be 1-dimensional\") self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Calculate initial lambda values by splitting data into segments mean_vals = np.array([ np.mean(x) for x in np.array_split(data_array, n_states) ]) mean_vals += 0.01 # To avoid zero starting prob idx = np.arange(len(data_array)) length = len(data_array) with pm.Model() as model: # Lambda parameters for each state (Poisson rates) lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=n_states ) # Changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Initialize changepoints evenly across the time series even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) # Convert to actual time indices tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent ) # Create weight matrix for smooth transitions between states weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis] ) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0 ) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0 ) weight_stack = weight_stack * inverse_stack # Calculate time-varying lambda lambda_t = lambda_latent.dot(weight_stack) # Observation model observation = pm.Poisson(\"obs\", lambda_t, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data - 1D array with 100 time points test_data = gen_test_array(100, n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = PoissonChangepoint1D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for PoissonChangepoint1D passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 1D Numpy array Time series data required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (1D Numpy array): Time series data n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = np.asarray(data_array) if self.data_array.ndim != 1: raise ValueError(\"data_array must be 1-dimensional\") self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Calculate initial lambda values by splitting data into segments mean_vals = np.array([ np.mean(x) for x in np.array_split(data_array, n_states) ]) mean_vals += 0.01 # To avoid zero starting prob idx = np.arange(len(data_array)) length = len(data_array) with pm.Model() as model: # Lambda parameters for each state (Poisson rates) lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=n_states ) # Changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Initialize changepoints evenly across the time series even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) # Convert to actual time indices tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent ) # Create weight matrix for smooth transitions between states weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis] ) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0 ) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0 ) weight_stack = weight_stack * inverse_stack # Calculate time-varying lambda lambda_t = lambda_latent.dot(weight_stack) # Observation model observation = pm.Poisson(\"obs\", lambda_t, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data - 1D array with 100 time points test_data = gen_test_array(100, n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = PoissonChangepoint1D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for PoissonChangepoint1D passed\") return True SingleTastePoisson Bases: ChangepointModel Model for changepoint on single taste ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoisson(ChangepointModel): \"\"\"Model for changepoint on single taste ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, # initval=even_switches, shape=(trials, n_states - 1), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = tt.tensordot(weight_stack, lambda_latent, [ 1, 1]).swapaxes(1, 2) observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for SingleTastePoisson passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, # initval=even_switches, shape=(trials, n_states - 1), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = tt.tensordot(weight_stack, lambda_latent, [ 1, 1]).swapaxes(1, 2) observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for SingleTastePoisson passed\") return True SingleTastePoissonDirichlet Bases: ChangepointModel Model for changepoint on single taste using dirichlet process prior Source code in pytau/changepoint_model.py class SingleTastePoissonDirichlet(ChangepointModel): \"\"\" Model for changepoint on single taste using dirichlet process prior \"\"\" def __init__(self, data_array, max_states=10, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time max_states (int): Maximum number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, max_states)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=(trials, max_states)) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic( \"w_raw\", stick_breaking_trial(beta, trials)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic( \"w_latent\", w_raw / w_raw.sum(axis=-1)[:, None]) tau = pm.Deterministic(\"tau\", tt.cumsum( w_latent * length, axis=-1)[:, :-1]) # ===================== # Rate over time # ===================== # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) # Trials x States x Time weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot(weight_stack, lambda_latent, [1, 1]).swapaxes(1, 2), ) # ===================== # Likelihood # ===================== observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for SingleTastePoissonDirichlet passed\") return True __init__(data_array, max_states=10, **kwargs) Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required max_states int Maximum number of states to model 10 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, max_states=10, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time max_states (int): Maximum number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, max_states)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=(trials, max_states)) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic( \"w_raw\", stick_breaking_trial(beta, trials)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic( \"w_latent\", w_raw / w_raw.sum(axis=-1)[:, None]) tau = pm.Deterministic(\"tau\", tt.cumsum( w_latent * length, axis=-1)[:, :-1]) # ===================== # Rate over time # ===================== # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) # Trials x States x Time weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot(weight_stack, lambda_latent, [1, 1]).swapaxes(1, 2), ) # ===================== # Likelihood # ===================== observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for SingleTastePoissonDirichlet passed\") return True SingleTastePoissonTrialSwitch Bases: ChangepointModel Assuming only emissions change across trials Changepoint distribution remains constant Source code in pytau/changepoint_model.py class SingleTastePoissonTrialSwitch(ChangepointModel): \"\"\" Assuming only emissions change across trials Changepoint distribution remains constant \"\"\" def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", nrn_lambda.dimshuffle(0, \"x\"), shape=(nrn_num, switch_components), ) # nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, \"x\"), shape=(nrn_num, switch_components, n_states), ) # Define Changepoints # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trial_num, n_states - 1) ).sort(axis=-1) # Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # First, we can \"select\" sets of emissions depending on trial_changepoints # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: nrns x switch_comps x states # selected_trial_lambda : nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, \"x\"), axis=1, ), ) # Then, we can select state_emissions for every trial idx = np.arange(time_bins) # tau : Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trial_num, 1, time_bins)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trial_num, 1, time_bins))], axis=1 ) # Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Convert selected_trial_lambda : nrns x trials x states x \"time\" # nrns x trials x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 2, 1, \"x\") * weight_stack.dimshuffle(\"x\", 0, 1, 2), axis=2, ) # Convert to : trials x nrns x time lambda_ = lambda_.dimshuffle(1, 0, 2) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames print(\"Test for SingleTastePoissonTrialSwitch passed\") return True __init__(data_array, switch_components, n_states, **kwargs) Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required switch_components int Number of trial switch components required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", nrn_lambda.dimshuffle(0, \"x\"), shape=(nrn_num, switch_components), ) # nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, \"x\"), shape=(nrn_num, switch_components, n_states), ) # Define Changepoints # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trial_num, n_states - 1) ).sort(axis=-1) # Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # First, we can \"select\" sets of emissions depending on trial_changepoints # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: nrns x switch_comps x states # selected_trial_lambda : nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, \"x\"), axis=1, ), ) # Then, we can select state_emissions for every trial idx = np.arange(time_bins) # tau : Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trial_num, 1, time_bins)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trial_num, 1, time_bins))], axis=1 ) # Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Convert selected_trial_lambda : nrns x trials x states x \"time\" # nrns x trials x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 2, 1, \"x\") * weight_stack.dimshuffle(\"x\", 0, 1, 2), axis=2, ) # Convert to : trials x nrns x time lambda_ = lambda_.dimshuffle(1, 0, 2) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames print(\"Test for SingleTastePoissonTrialSwitch passed\") return True SingleTastePoissonVarsig Bases: ChangepointModel Model for changepoint on single taste **Uses variables sigmoid slope inferred from data ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoissonVarsig(ChangepointModel): \"\"\"Model for changepoint on single taste **Uses variables sigmoid slope inferred from data ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # Sigmoid slope sig_b = pm.Normal(\"sig_b\", -1, 2, shape=n_states - 1) # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) sig_b_temp = tt.tile( sig_b[np.newaxis, :, np.newaxis], (trials, 1, len(idx))) weight_stack = var_sig_exp_tt(idx_temp - tau_temp, sig_b_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsig(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"sig_b\" in trace.varnames print(\"Test for SingleTastePoissonVarsig passed\") return True __init__(data_array, n_states, **kwargs) Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # Sigmoid slope sig_b = pm.Normal(\"sig_b\", -1, 2, shape=n_states - 1) # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) sig_b_temp = tt.tile( sig_b[np.newaxis, :, np.newaxis], (trials, 1, len(idx))) weight_stack = var_sig_exp_tt(idx_temp - tau_temp, sig_b_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsig(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"sig_b\" in trace.varnames print(\"Test for SingleTastePoissonVarsig passed\") return True SingleTastePoissonVarsigFixed Bases: ChangepointModel Model for changepoint on single taste **Uses sigmoid with given slope ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoissonVarsigFixed(ChangepointModel): \"\"\"Model for changepoint on single taste **Uses sigmoid with given slope ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model inds_span(float) : Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) with pm.Model() as model: # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) weight_stack = sigmoid(idx_temp - tau_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"state0\" in trace.varnames print(\"Test for SingleTastePoissonVarsigFixed passed\") return True __init__(data_array, n_states, inds_span=1, **kwargs) Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required inds_span(float) Number of indices to cover 5-95% change in sigmoid required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model inds_span(float) : Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span generate_model() Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) with pm.Model() as model: # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) weight_stack = sigmoid(idx_temp - tau_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model test() Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"state0\" in trace.varnames print(\"Test for SingleTastePoissonVarsigFixed passed\") return True advi_fit(model, fit, samples, convergence_tol=None) Convenience function to perform ADVI fit on model Parameters: Name Type Description Default model pymc model model object to run inference on required fit int Number of iterationst to fit the model for required samples int Number of samples to draw from fitted model required Returns: Name Type Description model original model on which inference was run, approx fitted model, lambda_stack array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run Source code in pytau/changepoint_model.py def advi_fit(model, fit, samples, convergence_tol=None): \"\"\"Convenience function to perform ADVI fit on model Args: model (pymc model): model object to run inference on fit (int): Number of iterationst to fit the model for samples (int): Number of samples to draw from fitted model Returns: model: original model on which inference was run, approx: fitted model, lambda_stack: array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run \"\"\" if convergence_tol is not None: callbacks = [pm.callbacks.CheckParametersConvergence( tolerance=convergence_tol)] print(\"Using convergence callback with tolerance:\", convergence_tol) else: callbacks = None with model: inference = pm.ADVI(\"full-rank\") approx = pm.fit(n=fit, method=inference, callbacks=callbacks) idata = approx.sample(draws=samples) # Check if tau exists in posterior samples (PyMC5 uses InferenceData) if \"tau\" not in idata.posterior.data_vars: available_vars = list(idata.posterior.data_vars.keys()) raise KeyError( f\"'tau' not found in posterior samples. Available variables: {available_vars}\") # Extract relevant variables from InferenceData posterior try: tau_samples = idata.posterior[\"tau\"].values # Handle potential dimension issues if tau_samples.ndim > 2: tau_samples = tau_samples.reshape(-1, tau_samples.shape[-1]) except Exception as e: print(f\"Error extracting tau samples: {e}\") tau_samples = None # Get observed data from model (PyMC5 compatible) # Since notebooks don't use fit_data, return None to avoid compatibility issues observed_data = None if \"lambda\" in idata.posterior.data_vars: try: lambda_stack = idata.posterior[\"lambda\"].values # Handle potential dimension issues if lambda_stack.ndim > 3: lambda_stack = lambda_stack.reshape(-1, *lambda_stack.shape[-2:]) lambda_stack = lambda_stack.swapaxes(0, 1) return model, approx, lambda_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting lambda samples: {e}\") return model, approx, None, tau_samples, observed_data if \"mu\" in idata.posterior.data_vars: try: mu_stack = idata.posterior[\"mu\"].values sigma_stack = idata.posterior[\"sigma\"].values # Handle potential dimension issues if mu_stack.ndim > 3: mu_stack = mu_stack.reshape(-1, *mu_stack.shape[-2:]) if sigma_stack.ndim > 3: sigma_stack = sigma_stack.reshape(-1, *sigma_stack.shape[-2:]) mu_stack = mu_stack.swapaxes(0, 1) sigma_stack = sigma_stack.swapaxes(0, 1) return model, approx, mu_stack, sigma_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting mu/sigma samples: {e}\") return model, approx, None, None, tau_samples, observed_data # Fallback - return what we can return model, approx, None, tau_samples, observed_data all_taste_poisson(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoisson(data_array, n_states, **kwargs) return model_class.generate_model() all_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoissonTrialSwitch( data_array, switch_components, n_states, **kwargs) return model_class.generate_model() all_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoissonVarsigFixed( data_array, n_states, inds_span, **kwargs) return model_class.generate_model() dpp_fit(model, n_chains=24, n_cores=1, tune=500, draws=500, use_numpyro=False) Convenience function to fit DPP model Source code in pytau/changepoint_model.py def dpp_fit(model, n_chains=24, n_cores=1, tune=500, draws=500, use_numpyro=False): \"\"\"Convenience function to fit DPP model\"\"\" if not use_numpyro: with model: dpp_trace = pm.sample( tune=tune, draws=draws, target_accept=0.95, chains=n_chains, cores=n_cores, return_inferencedata=False, ) else: with model: dpp_trace = pm.sample( nuts_sampler=\"numpyro\", tune=tune, draws=draws, target_accept=0.95, chains=n_chains, cores=n_cores, return_inferencedata=False, ) return dpp_trace extract_inferred_values(trace) Convenience function to extract inferred values from ADVI fit Parameters: Name Type Description Default trace dict trace required Returns: Name Type Description dict dictionary of inferred values Source code in pytau/changepoint_model.py def extract_inferred_values(trace): \"\"\"Convenience function to extract inferred values from ADVI fit Args: trace (dict): trace Returns: dict: dictionary of inferred values \"\"\" # Extract relevant variables from trace out_dict = dict(tau_samples=trace[\"tau\"]) if \"lambda\" in trace.varnames: out_dict[\"lambda_stack\"] = trace[\"lambda\"].swapaxes(0, 1) if \"mu\" in trace.varnames: out_dict[\"mu_stack\"] = trace[\"mu\"].swapaxes(0, 1) out_dict[\"sigma_stack\"] = trace[\"sigma\"].swapaxes(0, 1) return out_dict find_best_states(data, model_generator, n_fit, n_samples, min_states=2, max_states=10, convergence_tol=None) Convenience function to find best number of states for model Parameters: Name Type Description Default data array array on which to run inference required model_generator function function that generates model required n_fit int Number of iterationst to fit the model for required n_samples int Number of samples to draw from fitted model required min_states int Minimum number of states to test 2 max_states int Maximum number of states to test 10 convergence_tol float Tolerance for convergence. If None, will not check for convergence. None Returns: Name Type Description best_model model with best number of states, model_list list of models with different number of states, elbo_values list of elbo values for different number of states Source code in pytau/changepoint_model.py def find_best_states( data, model_generator, n_fit, n_samples, min_states=2, max_states=10, convergence_tol=None, ): \"\"\"Convenience function to find best number of states for model Args: data (array): array on which to run inference model_generator (function): function that generates model n_fit (int): Number of iterationst to fit the model for n_samples (int): Number of samples to draw from fitted model min_states (int): Minimum number of states to test max_states (int): Maximum number of states to test convergence_tol (float): Tolerance for convergence. If None, will not check for convergence. Returns: best_model: model with best number of states, model_list: list of models with different number of states, elbo_values: list of elbo values for different number of states \"\"\" n_state_array = np.arange(min_states, max_states + 1) elbo_values = [] model_list = [] for n_states in tqdm(n_state_array): print(f\"Fitting model with {n_states} states\") # Have to use int instead of np.int64 model = model_generator(data, int(n_states)) model, approx = advi_fit(model, n_fit, n_samples, convergence_tol)[:2] elbo_values.append(approx.hist[-1]) model_list.append(model) best_model = model_list[np.argmin(elbo_values)] return best_model, model_list, elbo_values gaussian_changepoint_mean_2d(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_2d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMean2D(data_array, n_states, **kwargs) return model_class.generate_model() gaussian_changepoint_mean_dirichlet(data_array, max_states=15, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_dirichlet(data_array, max_states=15, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMeanDirichlet( data_array, max_states, **kwargs) return model_class.generate_model() gaussian_changepoint_mean_var_2d(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_var_2d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMeanVar2D(data_array, n_states, **kwargs) return model_class.generate_model() gen_test_array(array_size, n_states, type='poisson') Generate test array for model fitting Last 2 dimensions consist of a single trial Time will always be last dimension Parameters: Name Type Description Default array_size tuple or int Size of array to generate. If int, generates 1D array. required n_states int Number of states to generate required type str Type of data to generate - normal - poisson 'poisson' Source code in pytau/changepoint_model.py def gen_test_array(array_size, n_states, type=\"poisson\"): \"\"\" Generate test array for model fitting Last 2 dimensions consist of a single trial Time will always be last dimension Args: array_size (tuple or int): Size of array to generate. If int, generates 1D array. n_states (int): Number of states to generate type (str): Type of data to generate - normal - poisson \"\"\" # Handle 1D case if isinstance(array_size, int): assert array_size > n_states, \"Array too small for states\" assert type in [ \"normal\", \"poisson\"], \"Invalid type, please use normal or poisson\" # Generate transition times for 1D case transition_times = np.random.random(n_states) transition_times = np.cumsum(transition_times) transition_times = transition_times / transition_times.max() transition_times *= array_size transition_times = transition_times.astype(int) # Generate state bounds state_bounds = np.zeros(n_states + 1, dtype=int) state_bounds[1:] = transition_times state_bounds[-1] = array_size # Generate state rates lambda_vals = np.random.exponential(2.0, n_states) + 0.5 # Generate 1D array rate_array = np.zeros(array_size) for i in range(n_states): start_idx = state_bounds[i] end_idx = state_bounds[i + 1] rate_array[start_idx:end_idx] = lambda_vals[i] if type == \"poisson\": return np.random.poisson(rate_array) else: return np.random.normal(loc=rate_array, scale=0.1) # Handle multi-dimensional case (existing code) assert array_size[-1] > n_states, \"Array too small for states\" assert type in [ \"normal\", \"poisson\"], \"Invalid type, please use normal or poisson\" # Generate transition times transition_times = np.random.random((*array_size[:-2], n_states)) transition_times = np.cumsum(transition_times, axis=-1) transition_times = transition_times / \\ transition_times.max(axis=-1, keepdims=True) transition_times *= array_size[-1] transition_times = np.vectorize(int)(transition_times) # Generate state bounds state_bounds = np.zeros((*array_size[:-2], n_states + 1), dtype=int) state_bounds[..., 1:] = transition_times # Generate state rates lambda_vals = np.random.random((*array_size[:-1], n_states)) # Generate array rate_array = np.zeros(array_size) inds = list(np.ndindex(lambda_vals.shape)) for this_ind in inds: this_lambda = lambda_vals[this_ind[:-2]][:, this_ind[-1]] this_state_bounds = [ state_bounds[(*this_ind[:-2], this_ind[-1])], state_bounds[(*this_ind[:-2], this_ind[-1] + 1)], ] rate_array[this_ind[:-2]][:, slice(*this_state_bounds)] = this_lambda[:, None] if type == \"poisson\": return np.random.poisson(rate_array) else: return np.random.normal(loc=rate_array, scale=0.1) mcmc_fit(model, samples) Convenience function to perform ADVI fit on model Parameters: Name Type Description Default model pymc model model object to run inference on required samples int Number of samples to draw using MCMC required Returns: Name Type Description model original model on which inference was run, trace samples drawn from MCMC, lambda_stack array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run Source code in pytau/changepoint_model.py def mcmc_fit(model, samples): \"\"\"Convenience function to perform ADVI fit on model Args: model (pymc model): model object to run inference on samples (int): Number of samples to draw using MCMC Returns: model: original model on which inference was run, trace: samples drawn from MCMC, lambda_stack: array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run \"\"\" with model: sampler_kwargs = {\"cores\": 1, \"chains\": 4} idata = pm.sample(draws=samples, **sampler_kwargs) # Thin the samples (every 10th sample) idata_thinned = idata.sel(draw=slice(None, None, 10)) # Extract relevant variables from InferenceData posterior try: tau_samples = idata_thinned.posterior[\"tau\"].values # Handle potential dimension issues if tau_samples.ndim > 2: tau_samples = tau_samples.reshape(-1, tau_samples.shape[-1]) except Exception as e: print(f\"Error extracting tau samples: {e}\") tau_samples = None # Get observed data from model (PyMC5 compatible) # Since notebooks don't use fit_data, return None to avoid compatibility issues observed_data = None if \"lambda\" in idata_thinned.posterior.data_vars: try: lambda_stack = idata_thinned.posterior[\"lambda\"].values # Handle potential dimension issues if lambda_stack.ndim > 3: lambda_stack = lambda_stack.reshape(-1, *lambda_stack.shape[-2:]) lambda_stack = lambda_stack.swapaxes(0, 1) return model, idata_thinned, lambda_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting lambda samples: {e}\") return model, idata_thinned, None, tau_samples, observed_data if \"mu\" in idata_thinned.posterior.data_vars: try: mu_stack = idata_thinned.posterior[\"mu\"].values sigma_stack = idata_thinned.posterior[\"sigma\"].values # Handle potential dimension issues if mu_stack.ndim > 3: mu_stack = mu_stack.reshape(-1, *mu_stack.shape[-2:]) if sigma_stack.ndim > 3: sigma_stack = sigma_stack.reshape(-1, *sigma_stack.shape[-2:]) mu_stack = mu_stack.swapaxes(0, 1) sigma_stack = sigma_stack.swapaxes(0, 1) return model, idata_thinned, mu_stack, sigma_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting mu/sigma samples: {e}\") return model, idata_thinned, None, None, tau_samples, observed_data # Fallback - return what we can return model, idata_thinned, None, tau_samples, observed_data poisson_changepoint_1d(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def poisson_changepoint_1d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = PoissonChangepoint1D(data_array, n_states, **kwargs) return model_class.generate_model() run_all_tests() Run tests for all model classes Source code in pytau/changepoint_model.py def run_all_tests(): \"\"\"Run tests for all model classes\"\"\" # Create test data test_data_1d = gen_test_array(100, n_states=3, type=\"poisson\") test_data_2d = gen_test_array((10, 100), n_states=3, type=\"normal\") test_data_3d = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") test_data_4d = gen_test_array((2, 5, 10, 100), n_states=3, type=\"poisson\") # Test each model class models_to_test = [ PoissonChangepoint1D(test_data_1d, 3), GaussianChangepointMeanVar2D(test_data_2d, 3), GaussianChangepointMeanDirichlet(test_data_2d, 5), GaussianChangepointMean2D(test_data_2d, 3), SingleTastePoissonDirichlet(test_data_3d, 5), SingleTastePoisson(test_data_3d, 3), SingleTastePoissonVarsig(test_data_3d, 3), SingleTastePoissonVarsigFixed(test_data_3d, 3, 1), SingleTastePoissonTrialSwitch(test_data_3d, 2, 3), AllTastePoisson(test_data_4d, 3), AllTastePoissonVarsigFixed(test_data_4d, 3, 1), AllTastePoissonTrialSwitch(test_data_4d, 2, 3), ] failed_tests = [] pbar = tqdm(models_to_test, total=len(models_to_test)) for model in pbar: try: model.test() pbar.set_description(f\"Test passed for {model.__class__.__name__}\") except Exception as e: failed_tests.append(model.__class__.__name__) print(f\"Test failed for {model.__class__.__name__}: {str(e)}\") print(\"All tests completed\") if failed_tests: print(\"Failed tests:\", failed_tests) single_taste_poisson(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoisson(data_array, n_states, **kwargs) return model_class.generate_model() single_taste_poisson_dirichlet(data_array, max_states=10, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_dirichlet(data_array, max_states=10, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonDirichlet(data_array, max_states, **kwargs) return model_class.generate_model() single_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonTrialSwitch( data_array, switch_components, n_states, **kwargs) return model_class.generate_model() single_taste_poisson_varsig(data_array, n_states, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_varsig(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonVarsig(data_array, n_states, **kwargs) return model_class.generate_model() single_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs) Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonVarsigFixed( data_array, n_states, inds_span, **kwargs) return model_class.generate_model() var_sig_exp_tt(x, b) x --> b --> Source code in pytau/changepoint_model.py def var_sig_exp_tt(x, b): \"\"\" x --> b --> \"\"\" return 1 / (1 + tt.exp(-tt.exp(b) * x)) var_sig_tt(x, b) x --> b --> Source code in pytau/changepoint_model.py def var_sig_tt(x, b): \"\"\" x --> b --> \"\"\" return 1 / (1 + tt.exp(-b * x)) === Preprocessing functions === Code to preprocess spike trains before feeding into model preprocess_all_taste(spike_array, time_lims, bin_width, data_transform) Preprocess array containing trials for all tastes (in blocks) concatenated Parameters: Name Type Description Default spike_array 4D Numpy Array Taste x Trials x Neurons x Time required time_lims List/Tuple/Numpy Array 2-element object indicating limits of array required bin_width int Width to use for binning required data_transform str Data-type to return {actual, shuffled, simulated} required Raises: Type Description Exception If transforms do not belong to ['shuffled','simulated','None',None] Returns: Type Description 4D Numpy Array Of processed data Source code in pytau/changepoint_preprocess.py def preprocess_all_taste(spike_array, time_lims, bin_width, data_transform): \"\"\"Preprocess array containing trials for all tastes (in blocks) concatenated Args: spike_array (4D Numpy Array): Taste x Trials x Neurons x Time time_lims (List/Tuple/Numpy Array): 2-element object indicating limits of array bin_width (int): Width to use for binning data_transform (str): Data-type to return {actual, shuffled, simulated} Raises: Exception: If transforms do not belong to ['shuffled','simulated','None',None] Returns: (4D Numpy Array): Of processed data \"\"\" accepted_transforms = [ \"trial_shuffled\", \"spike_shuffled\", \"simulated\", \"None\", None, ] if data_transform not in accepted_transforms: raise Exception( f\"data_transform must be of type {accepted_transforms}\") ################################################## # Create shuffled data ################################################## # Shuffle neurons across trials FOR SAME TASTE if data_transform == \"trial_shuffled\": transformed_dat = np.array( [np.random.permutation(neuron) for neuron in np.swapaxes(spike_array, 2, 0)] ) transformed_dat = np.swapaxes(transformed_dat, 0, 2) if data_transform == \"spike_shuffled\": transformed_dat = spike_array.swapaxes(-1, 0) transformed_dat = np.stack([np.random.permutation(x) for x in transformed_dat]) transformed_dat = transformed_dat.swapaxes(0, -1) ################################################## # Create simulated data ################################################## # Inhomogeneous poisson process using mean firing rates elif data_transform == \"simulated\": mean_firing = np.mean(spike_array, axis=1) mean_firing = np.broadcast_to(mean_firing[:, None], spike_array.shape) # Simulate spikes transformed_dat = (np.random.random( spike_array.shape) < mean_firing) * 1 ################################################## # Null Transform Case ################################################## elif data_transform == None or data_transform == \"None\": transformed_dat = spike_array ################################################## # Bin Data ################################################## spike_binned = np.sum( transformed_dat[..., time_lims[0]: time_lims[1]].reshape( *transformed_dat.shape[:-1], -1, bin_width ), axis=-1, ) spike_binned = spike_binned.astype(int) return spike_binned preprocess_single_taste(spike_array, time_lims, bin_width, data_transform) Preprocess array containing trials for all tastes (in blocks) concatenated ** Note, it may be useful to use x-arrays here to keep track of coordinates Parameters: Name Type Description Default spike_array 3D Numpy array trials x neurons x time required time_lims List/Tuple/Numpy Array 2-element object indicating limits of array required bin_width int Width to use for binning required data_transform str Data-type to return {actual, trial_shuffled, spike_shuffled, simulated} required Raises: Type Description Exception If transforms do not belong to Returns: Type Description 3D Numpy Array Of processed data Source code in pytau/changepoint_preprocess.py def preprocess_single_taste(spike_array, time_lims, bin_width, data_transform): \"\"\"Preprocess array containing trials for all tastes (in blocks) concatenated ** Note, it may be useful to use x-arrays here to keep track of coordinates Args: spike_array (3D Numpy array): trials x neurons x time time_lims (List/Tuple/Numpy Array): 2-element object indicating limits of array bin_width (int): Width to use for binning data_transform (str): Data-type to return {actual, trial_shuffled, spike_shuffled, simulated} Raises: Exception: If transforms do not belong to ['trial_shuffled','spike_shuffled','simulated','None',None] Returns: (3D Numpy Array): Of processed data \"\"\" accepted_transforms = [ \"trial_shuffled\", \"spike_shuffled\", \"simulated\", \"None\", None, ] if data_transform not in accepted_transforms: raise Exception( f\"data_transform must be of type {accepted_transforms}\") ################################################## # Create shuffled data ################################################## # Shuffle neurons across trials FOR SAME TASTE if data_transform == \"trial_shuffled\": transformed_dat = np.array( [np.random.permutation(neuron) for neuron in np.swapaxes(spike_array, 1, 0)] ) transformed_dat = np.swapaxes(transformed_dat, 0, 1) if data_transform == \"spike_shuffled\": transformed_dat = np.moveaxis(spike_array, -1, 0) transformed_dat = np.stack([np.random.permutation(x) for x in transformed_dat]) transformed_dat = np.moveaxis(transformed_dat, 0, -1) ################################################## # Create simulated data ################################################## # Inhomogeneous poisson process using mean firing rates elif data_transform == \"simulated\": mean_firing = np.mean(spike_array, axis=0) # Simulate spikes transformed_dat = ( np.array( [ np.random.random(mean_firing.shape) < mean_firing for trial in range(spike_array.shape[0]) ] ) * 1 ) ################################################## # Null Transform Case ################################################## elif data_transform in (None, \"None\"): transformed_dat = spike_array ################################################## # Bin Data ################################################## spike_binned = np.sum( transformed_dat[..., time_lims[0]: time_lims[1]].reshape( *spike_array.shape[:-1], -1, bin_width ), axis=-1, ) spike_binned = spike_binned.astype(int) return spike_binned","title":"API"},{"location":"api/#references","text":"","title":"References"},{"location":"api/#analysis-functions","text":"Helper classes and functions to perform analysis on fitted models","title":"=== Analysis functions ==="},{"location":"api/#pytau.changepoint_analysis.PklHandler","text":"Helper class to handle metadata and fit data from pkl file Source code in pytau/changepoint_analysis.py class PklHandler: \"\"\"Helper class to handle metadata and fit data from pkl file\"\"\" def __init__(self, file_path): \"\"\"Initialize PklHandler class Args: file_path (str): Path to pkl file \"\"\" self.dir_name = os.path.dirname(file_path) file_name = os.path.basename(file_path) self.file_name_base = file_name.split(\".\")[0] self.pkl_file_path = os.path.join( self.dir_name, self.file_name_base + \".pkl\") with open(self.pkl_file_path, \"rb\") as this_file: self.data = pkl.load(this_file) model_keys = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] key_savenames = [ \"_model_structure\", \"_fit_model\", \"lambda_array\", \"tau_array\", \"processed_spikes\", ] data_map = dict(zip(model_keys, key_savenames)) for key, var_name in data_map.items(): if key in self.data[\"model_data\"]: setattr(self, var_name, self.data[\"model_data\"][key]) else: # Set to None if key is missing (e.g., due to pickling fallback) setattr(self, var_name, None) self.metadata = self.data[\"metadata\"] self.pretty_metadata = pd.json_normalize(self.data[\"metadata\"]).T # Get number of trials from processed_spikes for proper tau formatting n_trials = self.processed_spikes.shape[0] if hasattr( self.processed_spikes, 'shape') else None self.tau = _tau(self.tau_array, self.metadata, n_trials) self.firing = _firing(self.tau, self.processed_spikes, self.metadata)","title":"PklHandler"},{"location":"api/#pytau.changepoint_analysis.PklHandler.__init__","text":"Initialize PklHandler class Parameters: Name Type Description Default file_path str Path to pkl file required Source code in pytau/changepoint_analysis.py def __init__(self, file_path): \"\"\"Initialize PklHandler class Args: file_path (str): Path to pkl file \"\"\" self.dir_name = os.path.dirname(file_path) file_name = os.path.basename(file_path) self.file_name_base = file_name.split(\".\")[0] self.pkl_file_path = os.path.join( self.dir_name, self.file_name_base + \".pkl\") with open(self.pkl_file_path, \"rb\") as this_file: self.data = pkl.load(this_file) model_keys = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] key_savenames = [ \"_model_structure\", \"_fit_model\", \"lambda_array\", \"tau_array\", \"processed_spikes\", ] data_map = dict(zip(model_keys, key_savenames)) for key, var_name in data_map.items(): if key in self.data[\"model_data\"]: setattr(self, var_name, self.data[\"model_data\"][key]) else: # Set to None if key is missing (e.g., due to pickling fallback) setattr(self, var_name, None) self.metadata = self.data[\"metadata\"] self.pretty_metadata = pd.json_normalize(self.data[\"metadata\"]).T # Get number of trials from processed_spikes for proper tau formatting n_trials = self.processed_spikes.shape[0] if hasattr( self.processed_spikes, 'shape') else None self.tau = _tau(self.tau_array, self.metadata, n_trials) self.firing = _firing(self.tau, self.processed_spikes, self.metadata)","title":"__init__"},{"location":"api/#pytau.changepoint_analysis.calc_significant_neurons_firing","text":"Calculate significant changes in firing rate between states Iterate ANOVA over neurons for all states With Bonferroni correction Args state_firing (3D Numpy array): trials x states x nrns p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: Name Type Description anova_p_val_array 1D Numpy array p-values for each neuron anova_sig_neurons 1D Numpy array indices of significant neurons Source code in pytau/changepoint_analysis.py def calc_significant_neurons_firing(state_firing, p_val=0.05): \"\"\"Calculate significant changes in firing rate between states Iterate ANOVA over neurons for all states With Bonferroni correction Args state_firing (3D Numpy array): trials x states x nrns p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: anova_p_val_array (1D Numpy array): p-values for each neuron anova_sig_neurons (1D Numpy array): indices of significant neurons \"\"\" n_neurons = state_firing.shape[-1] # Calculate ANOVA p-values for each neuron anova_p_val_array = np.zeros(state_firing.shape[-1]) for neuron in range(state_firing.shape[-1]): anova_p_val_array[neuron] = f_oneway(*state_firing[:, :, neuron].T)[1] anova_sig_neurons = np.where(anova_p_val_array < p_val / n_neurons)[0] return anova_p_val_array, anova_sig_neurons","title":"calc_significant_neurons_firing"},{"location":"api/#pytau.changepoint_analysis.calc_significant_neurons_snippets","text":"Calculate pairwise t-tests to detect differences between each transition With Bonferroni correction Args transition_snips (4D Numpy array): trials x nrns x bins x transitions p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: Name Type Description anova_p_val_array ( neurons , transition ) p-values for each neuron anova_sig_neurons ( neurons , transition ) indices of significant neurons Source code in pytau/changepoint_analysis.py def calc_significant_neurons_snippets(transition_snips, p_val=0.05): \"\"\"Calculate pairwise t-tests to detect differences between each transition With Bonferroni correction Args transition_snips (4D Numpy array): trials x nrns x bins x transitions p_val (float, optional): p-value to use for significance. Defaults to 0.05. Returns: anova_p_val_array (neurons, transition): p-values for each neuron anova_sig_neurons (neurons, transition): indices of significant neurons \"\"\" # Calculate pairwise t-tests for each transition # shape : [before, after] x trials x neurons x transitions mean_transition_snips = np.stack(np.array_split( transition_snips, 2, axis=2)).mean(axis=3) pairwise_p_val_array = np.zeros(mean_transition_snips.shape[2:]) n_neuron, n_transitions = pairwise_p_val_array.shape for neuron in range(n_neuron): for transition in range(n_transitions): pairwise_p_val_array[neuron, transition] = ttest_rel( *mean_transition_snips[:, :, neuron, transition] )[1] pairwise_sig_neurons = pairwise_p_val_array < p_val # /n_neuron return pairwise_p_val_array, pairwise_sig_neurons","title":"calc_significant_neurons_snippets"},{"location":"api/#pytau.changepoint_analysis.get_state_firing","text":"Calculate firing rates within states given changepoint positions on data Parameters: Name Type Description Default spike_array 3D Numpy array trials x nrns x bins required tau_array 2D Numpy array trials x switchpoints required Returns: Name Type Description state_firing 3D Numpy array trials x states x nrns Source code in pytau/changepoint_analysis.py def get_state_firing(spike_array, tau_array): \"\"\"Calculate firing rates within states given changepoint positions on data Args: spike_array (3D Numpy array): trials x nrns x bins tau_array (2D Numpy array): trials x switchpoints Returns: state_firing (3D Numpy array): trials x states x nrns \"\"\" states = tau_array.shape[-1] + 1 # Get mean firing rate for each STATE using model state_inds = np.hstack( [ np.zeros((tau_array.shape[0], 1)), tau_array, np.ones((tau_array.shape[0], 1)) * spike_array.shape[-1], ] ) state_lims = np.array([state_inds[:, x: x + 2] for x in range(states)]) state_lims = np.vectorize(int)(state_lims) state_lims = np.swapaxes(state_lims, 0, 1) state_firing = np.array( [ [np.mean(trial_dat[:, start:end], axis=-1) for start, end in trial_lims] for trial_dat, trial_lims in zip(spike_array, state_lims) ] ) state_firing = np.nan_to_num(state_firing) return state_firing","title":"get_state_firing"},{"location":"api/#pytau.changepoint_analysis.get_transition_snips","text":"Get snippets of activty around changepoints for each trial Parameters: Name Type Description Default spike_array 3D Numpy array trials x nrns x bins required tau_array 2D Numpy array trials x switchpoints required Returns: Type Description Numpy array: Transition snippets : trials x nrns x bins x transitions Make sure none of the snippets are outside the bounds of the data Source code in pytau/changepoint_analysis.py def get_transition_snips(spike_array, tau_array, window_radius=300): \"\"\"Get snippets of activty around changepoints for each trial Args: spike_array (3D Numpy array): trials x nrns x bins tau_array (2D Numpy array): trials x switchpoints Returns: Numpy array: Transition snippets : trials x nrns x bins x transitions Make sure none of the snippets are outside the bounds of the data \"\"\" # Get snippets of activity around changepoints for each trial n_trials, n_neurons, n_bins = spike_array.shape n_transitions = tau_array.shape[1] transition_snips = np.zeros( (n_trials, n_neurons, 2 * window_radius, n_transitions)) window_lims = np.stack( [tau_array - window_radius, tau_array + window_radius], axis=-1) # Make sure no lims are outside the bounds of the data if (window_lims < 0).sum(axis=None) or (window_lims > n_bins).sum(axis=None): raise ValueError(\"Transition window extends outside data bounds\") # Pull out snippets for trial in range(n_trials): for transition in range(n_transitions): transition_snips[trial, :, :, transition] = spike_array[ trial, :, window_lims[trial, transition, 0]: window_lims[trial, transition, 1], ] return transition_snips","title":"get_transition_snips"},{"location":"api/#io-functions","text":"Pipeline to handle model fitting from data extraction to saving results","title":"=== I/O functions ==="},{"location":"api/#pytau.changepoint_io.DatabaseHandler","text":"Class to handle transactions with model database Source code in pytau/changepoint_io.py class DatabaseHandler: \"\"\"Class to handle transactions with model database\"\"\" def __init__(self): \"\"\"Initialize DatabaseHandler class\"\"\" self.unique_cols = [\"exp.model_id\", \"exp.save_path\", \"exp.fit_date\"] self.model_database_path = MODEL_DATABASE_PATH self.model_save_base_dir = MODEL_SAVE_DIR if os.path.exists(self.model_database_path): self.fit_database = pd.read_csv( self.model_database_path, index_col=0) all_na = [all(x) for num, x in self.fit_database.isna().iterrows()] if all_na: print(f\"{sum(all_na)} rows found with all NA, removing...\") self.fit_database = self.fit_database.dropna(how=\"all\") else: print(\"Fit database does not exist yet\") def show_duplicates(self, keep=\"first\"): \"\"\"Find duplicates in database Args: keep (str, optional): Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. Returns: pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows \"\"\" dup_inds = self.fit_database.drop( self.unique_cols, axis=1).duplicated(keep=keep) return self.fit_database.loc[dup_inds], dup_inds def drop_duplicates(self): \"\"\"Remove duplicated rows from database\"\"\" _, dup_inds = self.show_duplicates() print(f\"Removing {sum(dup_inds)} duplicate rows\") self.fit_database = self.fit_database.loc[~dup_inds] def check_mismatched_paths(self): \"\"\"Check if there are any mismatched pkl files between database and directory Returns: pandas dataframe: Dataframe containing rows for which pkl file not present list: pkl files which cannot be matched to model in database list: all files in save directory \"\"\" mismatch_from_database = [ not os.path.exists(x + \".pkl\") for x in self.fit_database[\"exp.save_path\"] ] file_list = glob(os.path.join(self.model_save_base_dir, \"*/*.pkl\")) # Only split basename by '.' in case there are multiple '.' in filenpath mismatch_from_file = [ not ( os.path.join( os.path.dirname(x), os.path.basename(x).split(\".\")[0]) in list(self.fit_database[\"exp.save_path\"])) for x in file_list ] print( f\"{sum(mismatch_from_database)} mismatches from database\" + \"\\n\" + f\"{sum(mismatch_from_file)} mismatches from files\" ) return mismatch_from_database, mismatch_from_file, file_list def clear_mismatched_paths(self): \"\"\"Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found \"\"\" ( mismatch_from_database, mismatch_from_file, file_list, ) = self.check_mismatched_paths() mismatch_from_file = np.array(mismatch_from_file) mismatch_from_database = np.array(mismatch_from_database) self.fit_database = self.fit_database.loc[~mismatch_from_database] mismatched_files = [x for x, y in zip( file_list, mismatch_from_file) if y] for x in mismatched_files: os.remove(x) print(\"==== Clearing Completed ====\") def write_updated_database(self): \"\"\"Can be called following clear_mismatched_entries to update current database\"\"\" database_backup_dir = os.path.join( self.model_save_base_dir, \".database_backups\") if not os.path.exists(database_backup_dir): os.makedirs(database_backup_dir) # current_date = date.today().strftime(\"%m-%d-%y\") current_date = str(datetime.now()).replace(\" \", \"_\") shutil.copy( self.model_database_path, os.path.join(database_backup_dir, f\"database_backup_{current_date}\"), ) self.fit_database.to_csv(self.model_database_path, mode=\"w\") def set_run_params(self, data_dir, experiment_name, taste_num, laser_type, region_name): \"\"\"Store metadata related to inference run Args: data_dir (str): Path to directory containing HDF5 file experiment_name (str): Name given to fitted batch (for metedata). Defaults to None. taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) laser_type (None or str): None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) region_name (str): Region on which to perform fit on (must match regions in .info file) \"\"\" self.data_dir = data_dir self.data_basename = os.path.basename(self.data_dir) self.animal_name = self.data_basename.split(\"_\")[0] self.session_date = self.data_basename.split(\"_\")[-1] self.experiment_name = experiment_name self.model_save_dir = os.path.join( self.model_save_base_dir, experiment_name) if not os.path.exists(self.model_save_dir): os.makedirs(self.model_save_dir) self.model_id = str(uuid.uuid4()).split(\"-\")[0] self.model_save_path = os.path.join( self.model_save_dir, self.experiment_name + \"_\" + self.model_id ) self.fit_date = date.today().strftime(\"%m-%d-%y\") self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.fit_exists = None def ingest_fit_data(self, met_dict): \"\"\"Load external metadata Args: met_dict (dict): Dictionary of metadata from FitHandler class \"\"\" self.external_metadata = met_dict def aggregate_metadata(self): \"\"\"Collects information regarding data and current \"experiment\" Raises: Exception: If 'external_metadata' has not been ingested, that needs to be done first Returns: dict: Dictionary of metadata given to FitHandler class \"\"\" if \"external_metadata\" not in dir(self): raise Exception( \"Fit run metdata needs to be ingested \" \"into data_handler first\") data_details = dict( zip( [ \"data_dir\", \"basename\", \"animal_name\", \"session_date\", \"taste_num\", \"laser_type\", \"region_name\", ], [ self.data_dir, self.data_basename, self.animal_name, self.session_date, self.taste_num, self.laser_type, self.region_name, ], ) ) exp_details = dict( zip( [\"exp_name\", \"model_id\", \"save_path\", \"fit_date\"], [ self.experiment_name, self.model_id, self.model_save_path, self.fit_date, ], ) ) module_details = dict( zip( [\"pymc_version\", \"theano_version\"], [pymc.__version__, theano.__version__], ) ) temp_ext_met = self.external_metadata temp_ext_met[\"data\"] = data_details temp_ext_met[\"exp\"] = exp_details temp_ext_met[\"module\"] = module_details return temp_ext_met def write_to_database(self): \"\"\"Write out metadata to database\"\"\" agg_metadata = self.aggregate_metadata() # Convert model_kwargs to str so that they are save appropriately agg_metadata[\"model\"][\"model_kwargs\"] = str( agg_metadata[\"model\"][\"model_kwargs\"]) flat_metadata = pd.json_normalize(agg_metadata) if not os.path.isfile(self.model_database_path): flat_metadata.to_csv(self.model_database_path, mode=\"a\") else: flat_metadata.to_csv(self.model_database_path, mode=\"a\", header=False) print(f\"Updated model database @ {self.model_database_path}\") def check_exists(self): \"\"\"Check if the given fit already exists in database Returns: bool: Boolean for whether fit already exists or not \"\"\" if self.fit_exists is not None: return self.fit_exists","title":"DatabaseHandler"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.__init__","text":"Initialize DatabaseHandler class Source code in pytau/changepoint_io.py def __init__(self): \"\"\"Initialize DatabaseHandler class\"\"\" self.unique_cols = [\"exp.model_id\", \"exp.save_path\", \"exp.fit_date\"] self.model_database_path = MODEL_DATABASE_PATH self.model_save_base_dir = MODEL_SAVE_DIR if os.path.exists(self.model_database_path): self.fit_database = pd.read_csv( self.model_database_path, index_col=0) all_na = [all(x) for num, x in self.fit_database.isna().iterrows()] if all_na: print(f\"{sum(all_na)} rows found with all NA, removing...\") self.fit_database = self.fit_database.dropna(how=\"all\") else: print(\"Fit database does not exist yet\")","title":"__init__"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.aggregate_metadata","text":"Collects information regarding data and current \"experiment\" Raises: Type Description Exception If 'external_metadata' has not been ingested, that needs to be done first Returns: Name Type Description dict Dictionary of metadata given to FitHandler class Source code in pytau/changepoint_io.py def aggregate_metadata(self): \"\"\"Collects information regarding data and current \"experiment\" Raises: Exception: If 'external_metadata' has not been ingested, that needs to be done first Returns: dict: Dictionary of metadata given to FitHandler class \"\"\" if \"external_metadata\" not in dir(self): raise Exception( \"Fit run metdata needs to be ingested \" \"into data_handler first\") data_details = dict( zip( [ \"data_dir\", \"basename\", \"animal_name\", \"session_date\", \"taste_num\", \"laser_type\", \"region_name\", ], [ self.data_dir, self.data_basename, self.animal_name, self.session_date, self.taste_num, self.laser_type, self.region_name, ], ) ) exp_details = dict( zip( [\"exp_name\", \"model_id\", \"save_path\", \"fit_date\"], [ self.experiment_name, self.model_id, self.model_save_path, self.fit_date, ], ) ) module_details = dict( zip( [\"pymc_version\", \"theano_version\"], [pymc.__version__, theano.__version__], ) ) temp_ext_met = self.external_metadata temp_ext_met[\"data\"] = data_details temp_ext_met[\"exp\"] = exp_details temp_ext_met[\"module\"] = module_details return temp_ext_met","title":"aggregate_metadata"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.check_exists","text":"Check if the given fit already exists in database Returns: Name Type Description bool Boolean for whether fit already exists or not Source code in pytau/changepoint_io.py def check_exists(self): \"\"\"Check if the given fit already exists in database Returns: bool: Boolean for whether fit already exists or not \"\"\" if self.fit_exists is not None: return self.fit_exists","title":"check_exists"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.check_mismatched_paths","text":"Check if there are any mismatched pkl files between database and directory Returns: Name Type Description pandas dataframe: Dataframe containing rows for which pkl file not present list pkl files which cannot be matched to model in database list all files in save directory Source code in pytau/changepoint_io.py def check_mismatched_paths(self): \"\"\"Check if there are any mismatched pkl files between database and directory Returns: pandas dataframe: Dataframe containing rows for which pkl file not present list: pkl files which cannot be matched to model in database list: all files in save directory \"\"\" mismatch_from_database = [ not os.path.exists(x + \".pkl\") for x in self.fit_database[\"exp.save_path\"] ] file_list = glob(os.path.join(self.model_save_base_dir, \"*/*.pkl\")) # Only split basename by '.' in case there are multiple '.' in filenpath mismatch_from_file = [ not ( os.path.join( os.path.dirname(x), os.path.basename(x).split(\".\")[0]) in list(self.fit_database[\"exp.save_path\"])) for x in file_list ] print( f\"{sum(mismatch_from_database)} mismatches from database\" + \"\\n\" + f\"{sum(mismatch_from_file)} mismatches from files\" ) return mismatch_from_database, mismatch_from_file, file_list","title":"check_mismatched_paths"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.clear_mismatched_paths","text":"Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found Source code in pytau/changepoint_io.py def clear_mismatched_paths(self): \"\"\"Remove mismatched files and rows in database i.e. Remove 1) Files for which no entry can be found in database 2) Database entries for which no corresponding file can be found \"\"\" ( mismatch_from_database, mismatch_from_file, file_list, ) = self.check_mismatched_paths() mismatch_from_file = np.array(mismatch_from_file) mismatch_from_database = np.array(mismatch_from_database) self.fit_database = self.fit_database.loc[~mismatch_from_database] mismatched_files = [x for x, y in zip( file_list, mismatch_from_file) if y] for x in mismatched_files: os.remove(x) print(\"==== Clearing Completed ====\")","title":"clear_mismatched_paths"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.drop_duplicates","text":"Remove duplicated rows from database Source code in pytau/changepoint_io.py def drop_duplicates(self): \"\"\"Remove duplicated rows from database\"\"\" _, dup_inds = self.show_duplicates() print(f\"Removing {sum(dup_inds)} duplicate rows\") self.fit_database = self.fit_database.loc[~dup_inds]","title":"drop_duplicates"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.ingest_fit_data","text":"Load external metadata Parameters: Name Type Description Default met_dict dict Dictionary of metadata from FitHandler class required Source code in pytau/changepoint_io.py def ingest_fit_data(self, met_dict): \"\"\"Load external metadata Args: met_dict (dict): Dictionary of metadata from FitHandler class \"\"\" self.external_metadata = met_dict","title":"ingest_fit_data"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.set_run_params","text":"Store metadata related to inference run Parameters: Name Type Description Default data_dir str Path to directory containing HDF5 file required experiment_name str Name given to fitted batch (for metedata). Defaults to None. required taste_num int Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) required laser_type None or str None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) required region_name str Region on which to perform fit on (must match regions in .info file) required Source code in pytau/changepoint_io.py def set_run_params(self, data_dir, experiment_name, taste_num, laser_type, region_name): \"\"\"Store metadata related to inference run Args: data_dir (str): Path to directory containing HDF5 file experiment_name (str): Name given to fitted batch (for metedata). Defaults to None. taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) laser_type (None or str): None, 'on', or 'off' (For a laser session, which set of trials are wanted, None indicated return all trials) region_name (str): Region on which to perform fit on (must match regions in .info file) \"\"\" self.data_dir = data_dir self.data_basename = os.path.basename(self.data_dir) self.animal_name = self.data_basename.split(\"_\")[0] self.session_date = self.data_basename.split(\"_\")[-1] self.experiment_name = experiment_name self.model_save_dir = os.path.join( self.model_save_base_dir, experiment_name) if not os.path.exists(self.model_save_dir): os.makedirs(self.model_save_dir) self.model_id = str(uuid.uuid4()).split(\"-\")[0] self.model_save_path = os.path.join( self.model_save_dir, self.experiment_name + \"_\" + self.model_id ) self.fit_date = date.today().strftime(\"%m-%d-%y\") self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.fit_exists = None","title":"set_run_params"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.show_duplicates","text":"Find duplicates in database Parameters: Name Type Description Default keep str Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. 'first' Returns: Type Description pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows Source code in pytau/changepoint_io.py def show_duplicates(self, keep=\"first\"): \"\"\"Find duplicates in database Args: keep (str, optional): Which duplicate to keep (refer to pandas duplicated). Defaults to 'first'. Returns: pandas dataframe: Dataframe containing duplicated rows pandas series : Indices of duplicated rows \"\"\" dup_inds = self.fit_database.drop( self.unique_cols, axis=1).duplicated(keep=keep) return self.fit_database.loc[dup_inds], dup_inds","title":"show_duplicates"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.write_to_database","text":"Write out metadata to database Source code in pytau/changepoint_io.py def write_to_database(self): \"\"\"Write out metadata to database\"\"\" agg_metadata = self.aggregate_metadata() # Convert model_kwargs to str so that they are save appropriately agg_metadata[\"model\"][\"model_kwargs\"] = str( agg_metadata[\"model\"][\"model_kwargs\"]) flat_metadata = pd.json_normalize(agg_metadata) if not os.path.isfile(self.model_database_path): flat_metadata.to_csv(self.model_database_path, mode=\"a\") else: flat_metadata.to_csv(self.model_database_path, mode=\"a\", header=False) print(f\"Updated model database @ {self.model_database_path}\")","title":"write_to_database"},{"location":"api/#pytau.changepoint_io.DatabaseHandler.write_updated_database","text":"Can be called following clear_mismatched_entries to update current database Source code in pytau/changepoint_io.py def write_updated_database(self): \"\"\"Can be called following clear_mismatched_entries to update current database\"\"\" database_backup_dir = os.path.join( self.model_save_base_dir, \".database_backups\") if not os.path.exists(database_backup_dir): os.makedirs(database_backup_dir) # current_date = date.today().strftime(\"%m-%d-%y\") current_date = str(datetime.now()).replace(\" \", \"_\") shutil.copy( self.model_database_path, os.path.join(database_backup_dir, f\"database_backup_{current_date}\"), ) self.fit_database.to_csv(self.model_database_path, mode=\"w\")","title":"write_updated_database"},{"location":"api/#pytau.changepoint_io.FitHandler","text":"Class to handle pipeline of model fitting including: 1) Loading data 2) Preprocessing loaded arrays 3) Fitting model 4) Writing out fitted parameters to pkl file Source code in pytau/changepoint_io.py class FitHandler: \"\"\"Class to handle pipeline of model fitting including: 1) Loading data 2) Preprocessing loaded arrays 3) Fitting model 4) Writing out fitted parameters to pkl file \"\"\" def __init__( self, data_dir, taste_num, region_name, laser_type=None, experiment_name=None, model_params_path=None, preprocess_params_path=None, ): \"\"\"Initialize FitHandler class Args: data_dir (str): Path to directory containing HDF5 file taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) region_name (str): Region on which to perform fit on (must match regions in .info file) experiment_name (str, optional): Name given to fitted batch (for metedata). Defaults to None. model_params_path (str, optional): Path to json file containing model parameters. Defaults to None. preprocess_params_path (str, optional): Path to json file containing preprocessing parameters. Defaults to None. Raises: Exception: If \"experiment_name\" is None Exception: If \"laser_type\" is not in [None, 'on', 'off'] Exception: If \"taste_num\" is not integer or \"all\" \"\"\" # =============== Check for exceptions =============== if experiment_name is None: raise Exception(\"Please specify an experiment name\") if laser_type not in [None, \"on\", \"off\"]: raise Exception('laser_type must be from [None, \"on\",\"off\"]') if not (isinstance(taste_num, int) or taste_num == \"all\"): raise Exception('taste_num must be an integer or \"all\"') # =============== Save relevant arguments =============== self.data_dir = data_dir self.EphysData = EphysData(self.data_dir) # self.data = self.EphysData.get_spikes({\"bla\",\"gc\",\"all\"}) self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.experiment_name = experiment_name data_handler_init_kwargs = dict( zip( [ \"data_dir\", \"experiment_name\", \"taste_num\", \"laser_type\", \"region_name\", ], [data_dir, experiment_name, taste_num, laser_type, region_name], ) ) self.database_handler = DatabaseHandler() self.database_handler.set_run_params(**data_handler_init_kwargs) if model_params_path is None: print(\"MODEL_PARAMS will have to be set\") else: self.set_model_params(file_path=model_params_path) if preprocess_params_path is None: print(\"PREPROCESS_PARAMS will have to be set\") else: self.set_preprocess_params(file_path=preprocess_params_path) ######################################## # SET PARAMS ######################################## def set_preprocess_params(self, time_lims, bin_width, data_transform, file_path=None): \"\"\"Load given params as \"preprocess_params\" attribute Args: time_lims (array/tuple/list): Start and end of where to cut spike train array bin_width (int): Bin width for binning spikes to counts data_transform (str): Indicator for which transformation to use (refer to changepoint_preprocess) file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: preprocess_params_dict = dict( zip( [\"time_lims\", \"bin_width\", \"data_transform\"], [time_lims, bin_width, data_transform], ) ) self.preprocess_params = preprocess_params_dict print(\"Set preprocess params to: {}\".format(preprocess_params_dict)) else: # Load json and save dict pass def set_model_params(self, states, fit, samples, model_kwargs=None, file_path=None): \"\"\"Load given params as \"model_params\" attribute Args: states (int): Number of states to use in model fit (int): Iterations to use for model fitting (given ADVI fit) samples (int): Number of samples to return from fitten model model_kwargs (dict) : Additional paramters for model file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: model_params_dict = dict( zip( [\"states\", \"fit\", \"samples\", \"model_kwargs\"], [states, fit, samples, model_kwargs], ) ) self.model_params = model_params_dict print(\"Set model params to: {}\".format(model_params_dict)) else: # Load json and save dict pass ######################################## # SET PIPELINE FUNCS ######################################## def set_preprocessor(self, preprocessing_func): \"\"\"Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Args: preprocessing_func (func): Function to preprocess data (refer to changepoint_preprocess) \"\"\" self.preprocessor = preprocessing_func def preprocess_selector(self): \"\"\"Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): self.set_preprocessor( changepoint_preprocess.preprocess_single_taste) elif self.taste_num == \"all\": self.set_preprocessor(changepoint_preprocess.preprocess_all_taste) else: raise Exception(\"Something went wrong\") def set_model_template(self, model_template): \"\"\"Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Args: model_template (func): Function to generate model template for data] \"\"\" self.model_template = model_template def model_template_selector(self): \"\"\"Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): # self.set_model_template(changepoint_model.single_taste_poisson_varsig) self.set_model_template(changepoint_model.single_taste_poisson) elif self.taste_num == \"all\": self.set_model_template(changepoint_model.all_taste_poisson) else: raise Exception(\"Something went wrong\") def set_inference(self, inference_func): \"\"\"Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Args: inference_func (func): Function to use for fitting model \"\"\" self.inference_func = changepoint_model.advi_fit def inference_func_selector(self): \"\"\"Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" \"\"\" self.set_inference(changepoint_model.advi_fit) ######################################## # PIPELINE FUNCS ######################################## def load_spike_trains(self): \"\"\"Helper function to load spike trains from data_dir using EphysData module\"\"\" full_spike_array = self.EphysData.return_region_spikes( region_name=self.region_name, laser=self.laser_type ) if isinstance(self.taste_num, int): self.data = full_spike_array[self.taste_num] if self.taste_num == \"all\": self.data = full_spike_array print( f\"Loading spike trains from {self.database_handler.data_basename}, \" f\"dig_in {self.taste_num}, laser {str(self.laser_type)}\" ) def preprocess_data(self): \"\"\"Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected \"\"\" if \"data\" not in dir(self): self.load_spike_trains() if \"preprocessor\" not in dir(self): self.preprocess_selector() print( \"Preprocessing spike trains, \" f\"preprocessing func: <{self.preprocessor.__name__}>\") self.preprocessed_data = self.preprocessor( self.data, **self.preprocess_params) def create_model(self): \"\"\"Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected \"\"\" if \"preprocessed_data\" not in dir(self): self.preprocess_data() if \"model_template\" not in dir(self): self.model_template_selector() # In future iterations, before fitting model, # check that a similar entry doesn't exist print( f\"Generating Model, model func: <{self.model_template.__name__}>\") self.model = self.model_template( self.preprocessed_data, self.model_params[\"states\"], **self.model_params[\"model_kwargs\"], ) def run_inference(self): \"\"\"Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected \"\"\" if \"model\" not in dir(self): self.create_model() if \"inference_func\" not in dir(self): self.inference_func_selector() print( \"Running inference, inference func: \" f\"<{self.inference_func.__name__}>\") temp_outs = self.inference_func( self.model, self.model_params[\"fit\"], self.model_params[\"samples\"] ) varnames = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] self.inference_outs = dict(zip(varnames, temp_outs)) def _gen_fit_metadata(self): \"\"\"Generate metadata for fit Generate metadat by compiling: 1) Preprocess parameters given as input 2) Model parameters given as input 3) Functions used in inference pipeline for : preprocessing, model generation, fitting Returns: dict: Dictionary containing compiled metadata for different parts of inference pipeline \"\"\" pre_params = self.preprocess_params model_params = self.model_params pre_params[\"preprocessor_name\"] = self.preprocessor.__name__ model_params[\"model_template_name\"] = self.model_template.__name__ model_params[\"inference_func_name\"] = self.inference_func.__name__ fin_dict = dict(zip([\"preprocess\", \"model\"], [pre_params, model_params])) return fin_dict def _pass_metadata_to_handler(self): \"\"\"Function to coordinate transfer of metadata to DatabaseHandler\"\"\" self.database_handler.ingest_fit_data(self._gen_fit_metadata()) def _return_fit_output(self): \"\"\"Compile data, model, fit, and metadata to save output Returns: dict: Dictionary containing fitted model data and metadata \"\"\" self._pass_metadata_to_handler() agg_metadata = self.database_handler.aggregate_metadata() return {\"model_data\": self.inference_outs, \"metadata\": agg_metadata} def save_fit_output(self): \"\"\"Save fit output (fitted data + metadata) to pkl file\"\"\" if \"inference_outs\" not in dir(self): self.run_inference() out_dict = self._return_fit_output() # Save output to pkl file with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: pickle.dump(out_dict, buff) print( f\"Saved full output to {self.database_handler.model_save_path}.pkl\") # # Create a copy without the model to avoid pickling issues with PyMC5 # picklable_dict = out_dict.copy() # if \"model_data\" in picklable_dict and \"model\" in picklable_dict[\"model_data\"]: # picklable_model_data = picklable_dict[\"model_data\"].copy() # # Remove the model object as it contains unpicklable local functions in PyMC5 # picklable_model_data.pop(\"model\", None) # picklable_dict[\"model_data\"] = picklable_model_data # # with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: # try: # pickle.dump(picklable_dict, buff) # except (TypeError, AttributeError) as e: # print( # f\"Warning: Full pickling failed ({e}). Saving metadata-only version.\") # # If pickling fails, save only metadata and basic info # model_data_fallback = { # \"tau_array\": picklable_dict.get(\"model_data\", {}).get(\"tau_array\"), # \"processed_spikes\": picklable_dict.get(\"model_data\", {}).get(\"processed_spikes\"), # } # # # Try to save approx.hist for ELBO plotting if available # approx_obj = picklable_dict.get(\"model_data\", {}).get(\"approx\") # if approx_obj and hasattr(approx_obj, 'hist'): # try: # # Create a simple object with just the hist attribute # model_data_fallback[\"approx\"] = SimpleApprox( # approx_obj.hist) # except Exception: # # If even hist fails to pickle, skip it # pass # # metadata_only_dict = { # \"metadata\": picklable_dict.get(\"metadata\", {}), # \"model_data\": model_data_fallback # } # pickle.dump(metadata_only_dict, buff) json_file_name = os.path.join( self.database_handler.model_save_path + \".info\") with open(json_file_name, \"w\") as file: json.dump(out_dict[\"metadata\"], file, indent=4) self.database_handler.write_to_database() print( \"Saving inference output to : \\n\" f\"{self.database_handler.model_save_dir}\" \"\\n\" + \"================================\" + \"\\n\" )","title":"FitHandler"},{"location":"api/#pytau.changepoint_io.FitHandler.__init__","text":"Initialize FitHandler class Parameters: Name Type Description Default data_dir str Path to directory containing HDF5 file required taste_num int Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) required region_name str Region on which to perform fit on (must match regions in .info file) required experiment_name str Name given to fitted batch (for metedata). Defaults to None. None model_params_path str Path to json file containing model parameters. Defaults to None. None preprocess_params_path str Path to json file containing preprocessing parameters. Defaults to None. None Raises: Type Description Exception If \"experiment_name\" is None Exception If \"laser_type\" is not in [None, 'on', 'off'] Exception If \"taste_num\" is not integer or \"all\" Source code in pytau/changepoint_io.py def __init__( self, data_dir, taste_num, region_name, laser_type=None, experiment_name=None, model_params_path=None, preprocess_params_path=None, ): \"\"\"Initialize FitHandler class Args: data_dir (str): Path to directory containing HDF5 file taste_num (int): Index of taste to perform fit on (Corresponds to INDEX of taste in spike array, not actual dig_ins) region_name (str): Region on which to perform fit on (must match regions in .info file) experiment_name (str, optional): Name given to fitted batch (for metedata). Defaults to None. model_params_path (str, optional): Path to json file containing model parameters. Defaults to None. preprocess_params_path (str, optional): Path to json file containing preprocessing parameters. Defaults to None. Raises: Exception: If \"experiment_name\" is None Exception: If \"laser_type\" is not in [None, 'on', 'off'] Exception: If \"taste_num\" is not integer or \"all\" \"\"\" # =============== Check for exceptions =============== if experiment_name is None: raise Exception(\"Please specify an experiment name\") if laser_type not in [None, \"on\", \"off\"]: raise Exception('laser_type must be from [None, \"on\",\"off\"]') if not (isinstance(taste_num, int) or taste_num == \"all\"): raise Exception('taste_num must be an integer or \"all\"') # =============== Save relevant arguments =============== self.data_dir = data_dir self.EphysData = EphysData(self.data_dir) # self.data = self.EphysData.get_spikes({\"bla\",\"gc\",\"all\"}) self.taste_num = taste_num self.laser_type = laser_type self.region_name = region_name self.experiment_name = experiment_name data_handler_init_kwargs = dict( zip( [ \"data_dir\", \"experiment_name\", \"taste_num\", \"laser_type\", \"region_name\", ], [data_dir, experiment_name, taste_num, laser_type, region_name], ) ) self.database_handler = DatabaseHandler() self.database_handler.set_run_params(**data_handler_init_kwargs) if model_params_path is None: print(\"MODEL_PARAMS will have to be set\") else: self.set_model_params(file_path=model_params_path) if preprocess_params_path is None: print(\"PREPROCESS_PARAMS will have to be set\") else: self.set_preprocess_params(file_path=preprocess_params_path)","title":"__init__"},{"location":"api/#pytau.changepoint_io.FitHandler.create_model","text":"Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected Source code in pytau/changepoint_io.py def create_model(self): \"\"\"Create model and save as attribute Will check for and complete: 1) Data preprocessed 2) Model template selected \"\"\" if \"preprocessed_data\" not in dir(self): self.preprocess_data() if \"model_template\" not in dir(self): self.model_template_selector() # In future iterations, before fitting model, # check that a similar entry doesn't exist print( f\"Generating Model, model func: <{self.model_template.__name__}>\") self.model = self.model_template( self.preprocessed_data, self.model_params[\"states\"], **self.model_params[\"model_kwargs\"], )","title":"create_model"},{"location":"api/#pytau.changepoint_io.FitHandler.inference_func_selector","text":"Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" Source code in pytau/changepoint_io.py def inference_func_selector(self): \"\"\"Function to return model based off of input flag Currently hard-coded to use \"advi_fit\" \"\"\" self.set_inference(changepoint_model.advi_fit)","title":"inference_func_selector"},{"location":"api/#pytau.changepoint_io.FitHandler.load_spike_trains","text":"Helper function to load spike trains from data_dir using EphysData module Source code in pytau/changepoint_io.py def load_spike_trains(self): \"\"\"Helper function to load spike trains from data_dir using EphysData module\"\"\" full_spike_array = self.EphysData.return_region_spikes( region_name=self.region_name, laser=self.laser_type ) if isinstance(self.taste_num, int): self.data = full_spike_array[self.taste_num] if self.taste_num == \"all\": self.data = full_spike_array print( f\"Loading spike trains from {self.database_handler.data_basename}, \" f\"dig_in {self.taste_num}, laser {str(self.laser_type)}\" )","title":"load_spike_trains"},{"location":"api/#pytau.changepoint_io.FitHandler.model_template_selector","text":"Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Type Description Exception If self.taste_num is neither int nor str Source code in pytau/changepoint_io.py def model_template_selector(self): \"\"\"Function to set model based off of input flag Models can be set manually but it is preferred to go through model selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): # self.set_model_template(changepoint_model.single_taste_poisson_varsig) self.set_model_template(changepoint_model.single_taste_poisson) elif self.taste_num == \"all\": self.set_model_template(changepoint_model.all_taste_poisson) else: raise Exception(\"Something went wrong\")","title":"model_template_selector"},{"location":"api/#pytau.changepoint_io.FitHandler.preprocess_data","text":"Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected Source code in pytau/changepoint_io.py def preprocess_data(self): \"\"\"Perform data preprocessing Will check for and complete: 1) Raw data loaded 2) Preprocessor selected \"\"\" if \"data\" not in dir(self): self.load_spike_trains() if \"preprocessor\" not in dir(self): self.preprocess_selector() print( \"Preprocessing spike trains, \" f\"preprocessing func: <{self.preprocessor.__name__}>\") self.preprocessed_data = self.preprocessor( self.data, **self.preprocess_params)","title":"preprocess_data"},{"location":"api/#pytau.changepoint_io.FitHandler.preprocess_selector","text":"Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Type Description Exception If self.taste_num is neither int nor str Source code in pytau/changepoint_io.py def preprocess_selector(self): \"\"\"Function to return preprocess function based off of input flag Preprocessing can be set manually but it is preferred to go through preprocess selector Raises: Exception: If self.taste_num is neither int nor str \"\"\" if isinstance(self.taste_num, int): self.set_preprocessor( changepoint_preprocess.preprocess_single_taste) elif self.taste_num == \"all\": self.set_preprocessor(changepoint_preprocess.preprocess_all_taste) else: raise Exception(\"Something went wrong\")","title":"preprocess_selector"},{"location":"api/#pytau.changepoint_io.FitHandler.run_inference","text":"Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected Source code in pytau/changepoint_io.py def run_inference(self): \"\"\"Perform inference on data Will check for and complete: 1) Model created 2) Inference function selected \"\"\" if \"model\" not in dir(self): self.create_model() if \"inference_func\" not in dir(self): self.inference_func_selector() print( \"Running inference, inference func: \" f\"<{self.inference_func.__name__}>\") temp_outs = self.inference_func( self.model, self.model_params[\"fit\"], self.model_params[\"samples\"] ) varnames = [\"model\", \"approx\", \"lambda\", \"tau\", \"data\"] self.inference_outs = dict(zip(varnames, temp_outs))","title":"run_inference"},{"location":"api/#pytau.changepoint_io.FitHandler.save_fit_output","text":"Save fit output (fitted data + metadata) to pkl file Source code in pytau/changepoint_io.py def save_fit_output(self): \"\"\"Save fit output (fitted data + metadata) to pkl file\"\"\" if \"inference_outs\" not in dir(self): self.run_inference() out_dict = self._return_fit_output() # Save output to pkl file with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: pickle.dump(out_dict, buff) print( f\"Saved full output to {self.database_handler.model_save_path}.pkl\") # # Create a copy without the model to avoid pickling issues with PyMC5 # picklable_dict = out_dict.copy() # if \"model_data\" in picklable_dict and \"model\" in picklable_dict[\"model_data\"]: # picklable_model_data = picklable_dict[\"model_data\"].copy() # # Remove the model object as it contains unpicklable local functions in PyMC5 # picklable_model_data.pop(\"model\", None) # picklable_dict[\"model_data\"] = picklable_model_data # # with open(self.database_handler.model_save_path + \".pkl\", \"wb\") as buff: # try: # pickle.dump(picklable_dict, buff) # except (TypeError, AttributeError) as e: # print( # f\"Warning: Full pickling failed ({e}). Saving metadata-only version.\") # # If pickling fails, save only metadata and basic info # model_data_fallback = { # \"tau_array\": picklable_dict.get(\"model_data\", {}).get(\"tau_array\"), # \"processed_spikes\": picklable_dict.get(\"model_data\", {}).get(\"processed_spikes\"), # } # # # Try to save approx.hist for ELBO plotting if available # approx_obj = picklable_dict.get(\"model_data\", {}).get(\"approx\") # if approx_obj and hasattr(approx_obj, 'hist'): # try: # # Create a simple object with just the hist attribute # model_data_fallback[\"approx\"] = SimpleApprox( # approx_obj.hist) # except Exception: # # If even hist fails to pickle, skip it # pass # # metadata_only_dict = { # \"metadata\": picklable_dict.get(\"metadata\", {}), # \"model_data\": model_data_fallback # } # pickle.dump(metadata_only_dict, buff) json_file_name = os.path.join( self.database_handler.model_save_path + \".info\") with open(json_file_name, \"w\") as file: json.dump(out_dict[\"metadata\"], file, indent=4) self.database_handler.write_to_database() print( \"Saving inference output to : \\n\" f\"{self.database_handler.model_save_dir}\" \"\\n\" + \"================================\" + \"\\n\" )","title":"save_fit_output"},{"location":"api/#pytau.changepoint_io.FitHandler.set_inference","text":"Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Parameters: Name Type Description Default inference_func func Function to use for fitting model required Source code in pytau/changepoint_io.py def set_inference(self, inference_func): \"\"\"Manually set inference function for model fit e.g. FitHandler.set_inference(changepoint_model.advi_fit) Args: inference_func (func): Function to use for fitting model \"\"\" self.inference_func = changepoint_model.advi_fit","title":"set_inference"},{"location":"api/#pytau.changepoint_io.FitHandler.set_model_params","text":"Load given params as \"model_params\" attribute Parameters: Name Type Description Default states int Number of states to use in model required fit int Iterations to use for model fitting (given ADVI fit) required samples int Number of samples to return from fitten model required model_kwargs (dict) Additional paramters for model required file_path str Path to json file containing preprocess parameters. Defaults to None. None Source code in pytau/changepoint_io.py def set_model_params(self, states, fit, samples, model_kwargs=None, file_path=None): \"\"\"Load given params as \"model_params\" attribute Args: states (int): Number of states to use in model fit (int): Iterations to use for model fitting (given ADVI fit) samples (int): Number of samples to return from fitten model model_kwargs (dict) : Additional paramters for model file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: model_params_dict = dict( zip( [\"states\", \"fit\", \"samples\", \"model_kwargs\"], [states, fit, samples, model_kwargs], ) ) self.model_params = model_params_dict print(\"Set model params to: {}\".format(model_params_dict)) else: # Load json and save dict pass","title":"set_model_params"},{"location":"api/#pytau.changepoint_io.FitHandler.set_model_template","text":"Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Parameters: Name Type Description Default model_template func Function to generate model template for data] required Source code in pytau/changepoint_io.py def set_model_template(self, model_template): \"\"\"Manually set model_template for data e.g. FitHandler.set_model(changepoint_model.single_taste_poisson) Args: model_template (func): Function to generate model template for data] \"\"\" self.model_template = model_template","title":"set_model_template"},{"location":"api/#pytau.changepoint_io.FitHandler.set_preprocess_params","text":"Load given params as \"preprocess_params\" attribute Parameters: Name Type Description Default time_lims array / tuple / list Start and end of where to cut spike train array required bin_width int Bin width for binning spikes to counts required data_transform str Indicator for which transformation to use (refer to changepoint_preprocess) required file_path str Path to json file containing preprocess parameters. Defaults to None. None Source code in pytau/changepoint_io.py def set_preprocess_params(self, time_lims, bin_width, data_transform, file_path=None): \"\"\"Load given params as \"preprocess_params\" attribute Args: time_lims (array/tuple/list): Start and end of where to cut spike train array bin_width (int): Bin width for binning spikes to counts data_transform (str): Indicator for which transformation to use (refer to changepoint_preprocess) file_path (str, optional): Path to json file containing preprocess parameters. Defaults to None. \"\"\" if file_path is None: preprocess_params_dict = dict( zip( [\"time_lims\", \"bin_width\", \"data_transform\"], [time_lims, bin_width, data_transform], ) ) self.preprocess_params = preprocess_params_dict print(\"Set preprocess params to: {}\".format(preprocess_params_dict)) else: # Load json and save dict pass","title":"set_preprocess_params"},{"location":"api/#pytau.changepoint_io.FitHandler.set_preprocessor","text":"Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Parameters: Name Type Description Default preprocessing_func func Function to preprocess data (refer to changepoint_preprocess) required Source code in pytau/changepoint_io.py def set_preprocessor(self, preprocessing_func): \"\"\"Manually set preprocessor for data e.g. FitHandler.set_preprocessor( changepoint_preprocess.preprocess_single_taste) Args: preprocessing_func (func): Function to preprocess data (refer to changepoint_preprocess) \"\"\" self.preprocessor = preprocessing_func","title":"set_preprocessor"},{"location":"api/#pytau.changepoint_io.SimpleApprox","text":"Simple approximation object that only stores the hist attribute for ELBO plotting Source code in pytau/changepoint_io.py class SimpleApprox: \"\"\"Simple approximation object that only stores the hist attribute for ELBO plotting\"\"\" def __init__(self, hist): self.hist = hist","title":"SimpleApprox"},{"location":"api/#model-building-functions","text":"pymc Blackbox Variational Inference implementation of Poisson Likelihood Changepoint for spike trains.","title":"=== Model building functions ==="},{"location":"api/#pytau.changepoint_model.AllTastePoisson","text":"Bases: ChangepointModel ** Model to fit changepoint to all tastes ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" Source code in pytau/changepoint_model.py class AllTastePoisson(ChangepointModel): \"\"\" ** Model to fit changepoint to all tastes ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoisson passed\") return True","title":"AllTastePoisson"},{"location":"api/#pytau.changepoint_model.AllTastePoisson.__init__","text":"Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.AllTastePoisson.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.AllTastePoisson.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoisson passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.AllTastePoissonTrialSwitch","text":"Bases: ChangepointModel Assuming only emissions change across trials Changepoint distribution remains constant Source code in pytau/changepoint_model.py class AllTastePoissonTrialSwitch(ChangepointModel): \"\"\" Assuming only emissions change across trials Changepoint distribution remains constant \"\"\" def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states tastes, trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # ================================================= # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # tastes x nrns taste_lambda = pm.Exponential( \"taste_lambda\", nrn_lambda.dimshuffle(\"x\", 0), shape=(tastes, nrn_num) ) # tastes x nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", taste_lambda.dimshuffle(0, 1, \"x\"), shape=(tastes, nrn_num, switch_components), ) # tastes x nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, 2, \"x\"), shape=(tastes, nrn_num, switch_components, n_states), ) # Define Changepoints # ================================================= # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(tastes, trial_num, n_states - 1), ).sort(axis=-1) # Tasets x Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints # a_trial = pm.HalfCauchy('a_trial', 3., shape = switch_components - 1) # b_trial = pm.HalfCauchy('b_trial', 3., shape = switch_components - 1) even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints # ================================================= tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: tastes x nrns x switch_comps x states # selected_trial_lambda : tastes x nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"tastes\" x \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", \"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, 3, \"x\"), axis=2, ), ) # First, we can \"select\" sets of emissions depending on trial_changepoints # ================================================= trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # Then, we can select state_emissions for every trial # ================================================= idx = np.arange(time_bins) # tau : Tastes x Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes, trial_num, 1, time_bins)), weight_stack], axis=2 ) inverse_stack = 1 - weight_stack[:, :, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes, trial_num, 1, time_bins))], axis=2 ) # Tastes x Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Putting everything together # ================================================= # selected_trial_lambda : tastes x nrns x states x trials # Convert selected_trial_lambda --> tastes x trials x nrns x states x \"time\" # weight_stack : tastes x trials x states x time # Convert weight_stack --> tastes x trials x \"nrns\" x states x time # tastes x trials x nrns x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 3, 1, 2, \"x\") * weight_stack.dimshuffle(0, 1, \"x\", 2, 3), axis=3, ) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames assert \"taste_lambda\" in trace.varnames print(\"Test for AllTastePoissonTrialSwitch passed\") return True","title":"AllTastePoissonTrialSwitch"},{"location":"api/#pytau.changepoint_model.AllTastePoissonTrialSwitch.__init__","text":"Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required switch_components int Number of trial switch components required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.AllTastePoissonTrialSwitch.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states tastes, trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # ================================================= # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # tastes x nrns taste_lambda = pm.Exponential( \"taste_lambda\", nrn_lambda.dimshuffle(\"x\", 0), shape=(tastes, nrn_num) ) # tastes x nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", taste_lambda.dimshuffle(0, 1, \"x\"), shape=(tastes, nrn_num, switch_components), ) # tastes x nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, 2, \"x\"), shape=(tastes, nrn_num, switch_components, n_states), ) # Define Changepoints # ================================================= # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(tastes, trial_num, n_states - 1), ).sort(axis=-1) # Tasets x Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints # a_trial = pm.HalfCauchy('a_trial', 3., shape = switch_components - 1) # b_trial = pm.HalfCauchy('b_trial', 3., shape = switch_components - 1) even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints # ================================================= tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: tastes x nrns x switch_comps x states # selected_trial_lambda : tastes x nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"tastes\" x \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", \"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, 3, \"x\"), axis=2, ), ) # First, we can \"select\" sets of emissions depending on trial_changepoints # ================================================= trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # Then, we can select state_emissions for every trial # ================================================= idx = np.arange(time_bins) # tau : Tastes x Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes, trial_num, 1, time_bins)), weight_stack], axis=2 ) inverse_stack = 1 - weight_stack[:, :, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes, trial_num, 1, time_bins))], axis=2 ) # Tastes x Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Putting everything together # ================================================= # selected_trial_lambda : tastes x nrns x states x trials # Convert selected_trial_lambda --> tastes x trials x nrns x states x \"time\" # weight_stack : tastes x trials x states x time # Convert weight_stack --> tastes x trials x \"nrns\" x states x time # tastes x trials x nrns x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 3, 1, 2, \"x\") * weight_stack.dimshuffle(0, 1, \"x\", 2, 3), axis=3, ) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.AllTastePoissonTrialSwitch.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames assert \"taste_lambda\" in trace.varnames print(\"Test for AllTastePoissonTrialSwitch passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.AllTastePoissonVarsigFixed","text":"Bases: ChangepointModel ** Model to fit changepoint to all tastes with fixed sigmoid ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" Source code in pytau/changepoint_model.py class AllTastePoissonVarsigFixed(ChangepointModel): \"\"\" ** Model to fit changepoint to all tastes with fixed sigmoid ** ** Largely taken from \"_v1/poisson_all_tastes_changepoint_model.py\" \"\"\" def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model inds_span(float): Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = sigmoid(idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoissonVarsigFixed passed\") return True","title":"AllTastePoissonVarsigFixed"},{"location":"api/#pytau.changepoint_model.AllTastePoissonVarsigFixed.__init__","text":"Parameters: Name Type Description Default data_array 4D Numpy array tastes, trials, neurons, time_bins required n_states int Number of states to model required inds_span float Number of indices to cover 5-95% change in sigmoid 1 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (4D Numpy array): tastes, trials, neurons, time_bins n_states (int): Number of states to model inds_span(float): Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span","title":"__init__"},{"location":"api/#pytau.changepoint_model.AllTastePoissonVarsigFixed.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span # Unroll arrays along taste axis data_array_long = np.concatenate(data_array, axis=0) # Find mean firing for initial values tastes = data_array.shape[0] length = data_array.shape[-1] nrns = data_array.shape[2] trials = data_array.shape[1] split_list = np.array_split(data_array, n_states, axis=-1) # Cut all to the same size min_val = min([x.shape[-1] for x in split_list]) split_array = np.array([x[..., :min_val] for x in split_list]) mean_vals = np.mean(split_array, axis=(2, -1)).swapaxes(0, 1) mean_vals += 0.01 # To avoid zero starting prob mean_nrn_vals = np.mean(mean_vals, axis=(0, 1)) # Find evenly spaces switchpoints for initial values idx = np.arange(data_array.shape[-1]) # Index array_idx = np.broadcast_to(idx, data_array_long.shape) even_switches = np.linspace(0, idx.max(), n_states + 1) even_switches_normal = even_switches / np.max(even_switches) taste_label = np.repeat( np.arange(data_array.shape[0]), data_array.shape[1]) trial_num = array_idx.shape[0] # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) # Being constructing model with pm.Model() as model: # Hierarchical firing rates # Refer to model diagram # Mean firing rate of neuron AT ALL TIMES lambda_nrn = pm.Exponential( \"lambda_nrn\", 1 / mean_nrn_vals, shape=(mean_vals.shape[-1]) ) # Priors for each state, derived from each neuron # Mean firing rate of neuron IN EACH STATE (averaged across tastes) lambda_state = pm.Exponential( \"lambda_state\", lambda_nrn, shape=(mean_vals.shape[1:])) # Mean firing rate of neuron PER STATE PER TASTE lambda_latent = pm.Exponential( \"lambda\", lambda_state[np.newaxis, :, :], initval=mean_vals, shape=(mean_vals.shape), ) # Changepoint time variable # INDEPENDENT TAU FOR EVERY TRIAL a = pm.HalfNormal(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfNormal(\"b_tau\", 3.0, shape=n_states - 1) # Stack produces n_states x trials --> That gets transposed # to trials x n_states and gets sorted along n_states (axis=-1) # Sort should work the same way as the Ordered transform --> # see rv_sort_test.ipynb tau_latent = pm.Beta( \"tau_latent\", a, b, shape=(trial_num, n_states - 1), initval=tt.tile(even_switches_normal[1:( n_states)], (array_idx.shape[0], 1)), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = sigmoid(idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((tastes * trials, 1, length)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((tastes * trials, 1, length))], axis=1 ) weight_stack = weight_stack * inverse_stack weight_stack = tt.tile( weight_stack[:, :, None, :], (1, 1, nrns, 1)) lambda_latent = lambda_latent.dimshuffle(2, 0, 1) lambda_latent = tt.repeat(lambda_latent, trials, axis=1) lambda_latent = tt.tile( lambda_latent[..., None], (1, 1, 1, length)) lambda_latent = lambda_latent.dimshuffle(1, 2, 0, 3) lambda_ = tt.sum(lambda_latent * weight_stack, axis=1) observation = pm.Poisson(\"obs\", lambda_, observed=data_array_long) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.AllTastePoissonVarsigFixed.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (2, 5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = AllTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"lambda_nrn\" in trace.varnames assert \"lambda_state\" in trace.varnames print(\"Test for AllTastePoissonVarsigFixed passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.CategoricalChangepoint2D","text":"Bases: ChangepointModel Model for categorical data changepoint detection on 2D arrays. Source code in pytau/changepoint_model.py class CategoricalChangepoint2D(ChangepointModel): \"\"\"Model for categorical data changepoint detection on 2D arrays.\"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): trials x length - Each element is a postive integer representing a category n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) # Make sure data array is int if not np.issubdtype(data_array.dtype, np.integer): raise ValueError( \"Data array must contain integer category values.\") # Check that data_array is 2D if data_array.ndim != 2: # If 3D, take the first trial/dimension to make it 2D if data_array.ndim == 3: data_array = data_array[0] else: raise ValueError(\"Data array must be 2D (trials x length).\") self.data_array = data_array self.n_states = n_states def generate_model(self): data_array = self.data_array n_states = self.n_states trials, length = data_array.shape features = len(np.unique(data_array)) # If features in data_array are not continuous integer values, map them feature_set = np.unique(data_array) if not np.array_equal(feature_set, np.arange(len(feature_set))): # Create a mapping from original categories to continuous integers category_map = {cat: i for i, cat in enumerate(feature_set)} data_array = np.vectorize(category_map.get)(data_array) idx = np.arange(length) flat_data_array = data_array.reshape((trials * length,)) with pm.Model() as model: p = pm.Dirichlet(\"p\", a=np.ones( (n_states, features)), shape=(n_states, features)) # Infer changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Shape: trials x changepoints tau_latent = pm.Beta(\"tau_latent\", a_tau, b_tau, shape=(trials, n_states - 1)).sort( axis=-1 ) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) # shapes: # - weight_stack: trials x states x length # - p : states x features # shape: trials x length x features lambda_ = tt.tensordot(weight_stack, p, [1, 0]) flat_lambda = lambda_.reshape((trials * length, features)) # Use categorical likelihood # data_array = trials x length category = pm.Categorical( \"category\", p=flat_lambda, observed=flat_data_array) return model def test(self): test_data = np.random.randint(0, self.n_states, size=(5, 100)) test_model = CategoricalChangepoint2D(test_data, self.n_states) model = test_model.generate_model() with model: inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) assert \"p\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for CategoricalChangepoint2D passed\") return True","title":"CategoricalChangepoint2D"},{"location":"api/#pytau.changepoint_model.CategoricalChangepoint2D.__init__","text":"Parameters: Name Type Description Default data_array 2D Numpy array trials x length - Each element is a postive integer representing a category required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): trials x length - Each element is a postive integer representing a category n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) # Make sure data array is int if not np.issubdtype(data_array.dtype, np.integer): raise ValueError( \"Data array must contain integer category values.\") # Check that data_array is 2D if data_array.ndim != 2: # If 3D, take the first trial/dimension to make it 2D if data_array.ndim == 3: data_array = data_array[0] else: raise ValueError(\"Data array must be 2D (trials x length).\") self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.ChangepointModel","text":"Base class for all changepoint models Source code in pytau/changepoint_model.py class ChangepointModel: \"\"\"Base class for all changepoint models\"\"\" def __init__(self, **kwargs): \"\"\"Initialize model with keyword arguments\"\"\" self.kwargs = kwargs def generate_model(self): \"\"\"Generate pymc model - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement generate_model()\") def test(self): \"\"\"Test model functionality - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement test()\")","title":"ChangepointModel"},{"location":"api/#pytau.changepoint_model.ChangepointModel.__init__","text":"Initialize model with keyword arguments Source code in pytau/changepoint_model.py def __init__(self, **kwargs): \"\"\"Initialize model with keyword arguments\"\"\" self.kwargs = kwargs","title":"__init__"},{"location":"api/#pytau.changepoint_model.ChangepointModel.generate_model","text":"Generate pymc model - to be implemented by subclasses Source code in pytau/changepoint_model.py def generate_model(self): \"\"\"Generate pymc model - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement generate_model()\")","title":"generate_model"},{"location":"api/#pytau.changepoint_model.ChangepointModel.test","text":"Test model functionality - to be implemented by subclasses Source code in pytau/changepoint_model.py def test(self): \"\"\"Test model functionality - to be implemented by subclasses\"\"\" raise NotImplementedError(\"Subclasses must implement test()\")","title":"test"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMean2D","text":"Bases: ChangepointModel Model for gaussian data on 2D array detecting changes only in the mean. Source code in pytau/changepoint_model.py class GaussianChangepointMean2D(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes only in the mean. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dimshuffle(0, \"x\") observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMean2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMean2D passed\") return True","title":"GaussianChangepointMean2D"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMean2D.__init__","text":"Parameters: Name Type Description Default data_array 2D Numpy array x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMean2D.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dimshuffle(0, \"x\") observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMean2D.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMean2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMean2D passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanDirichlet","text":"Bases: ChangepointModel Model for gaussian data on 2D array detecting changes only in the mean. Number of states determined using dirichlet process prior. Source code in pytau/changepoint_model.py class GaussianChangepointMeanDirichlet(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes only in the mean. Number of states determined using dirichlet process prior. \"\"\" def __init__(self, data_array, max_states=15, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time max_states (int): Max number of states to include in truncated dirichlet process **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob test_std = np.std(data_array, axis=-1) with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Normal( \"lambda\", mu=mean_vals, sigma=10, shape=(y_dim, max_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", test_std, shape=(y_dim)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=max_states) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic(\"w_raw\", stick_breaking(beta)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic(\"w_latent\", w_raw / w_raw.sum()) tau = pm.Deterministic(\"tau\", tt.cumsum(w_latent * length)[:-1]) # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) # Create timeseries for latent variable (mean emission) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot( lambda_latent, weight_stack, axes=(1, 0)) ) sigma_latent = sigma.dimshuffle(0, \"x\") # Likelihood for observations observation = pm.Normal( \"obs\", mu=lambda_, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((10, 100), n_states=3, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for GaussianChangepointMeanDirichlet passed\") return True","title":"GaussianChangepointMeanDirichlet"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanDirichlet.__init__","text":"Parameters: Name Type Description Default data_array 2D Numpy array x time required max_states int Max number of states to include in truncated dirichlet process 15 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, max_states=15, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time max_states (int): Max number of states to include in truncated dirichlet process **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanDirichlet.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob test_std = np.std(data_array, axis=-1) with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Normal( \"lambda\", mu=mean_vals, sigma=10, shape=(y_dim, max_states)) # One variance for each dimension sigma = pm.HalfCauchy(\"sigma\", test_std, shape=(y_dim)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=max_states) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic(\"w_raw\", stick_breaking(beta)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic(\"w_latent\", w_raw / w_raw.sum()) tau = pm.Deterministic(\"tau\", tt.cumsum(w_latent * length)[:-1]) # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) # Create timeseries for latent variable (mean emission) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot( lambda_latent, weight_stack, axes=(1, 0)) ) sigma_latent = sigma.dimshuffle(0, \"x\") # Likelihood for observations observation = pm.Normal( \"obs\", mu=lambda_, sigma=sigma_latent, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanDirichlet.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((10, 100), n_states=3, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for GaussianChangepointMeanDirichlet passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanVar2D","text":"Bases: ChangepointModel Model for gaussian data on 2D array detecting changes in both mean and variance. Source code in pytau/changepoint_model.py class GaussianChangepointMeanVar2D(ChangepointModel): \"\"\"Model for gaussian data on 2D array detecting changes in both mean and variance. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dot(weight_stack) observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanVar2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMeanVar2D passed\") return True","title":"GaussianChangepointMeanVar2D"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanVar2D.__init__","text":"Parameters: Name Type Description Default data_array 2D Numpy array x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (2D Numpy array): <dimension> x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanVar2D.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals += 0.01 # To avoid zero starting prob y_dim = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: mu = pm.Normal(\"mu\", mu=mean_vals, sigma=1, shape=(y_dim, n_states)) sigma = pm.HalfCauchy(\"sigma\", 3.0, shape=(y_dim, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis]) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0) weight_stack = np.multiply(weight_stack, inverse_stack) mu_latent = mu.dot(weight_stack) sigma_latent = sigma.dot(weight_stack) observation = pm.Normal( \"obs\", mu=mu_latent, sigma=sigma_latent, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.GaussianChangepointMeanVar2D.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (10, 100), n_states=self.n_states, type=\"normal\") # Create model with test data test_model = GaussianChangepointMeanVar2D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"mu\" in trace.varnames assert \"sigma\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for GaussianChangepointMeanVar2D passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.PoissonChangepoint1D","text":"Bases: ChangepointModel Model for changepoint detection in 1D Poisson time series This model detects changepoints in 1D time series data using a Poisson likelihood. It assumes the data follows a Poisson distribution with different rates in different segments separated by changepoints. Source code in pytau/changepoint_model.py class PoissonChangepoint1D(ChangepointModel): \"\"\"Model for changepoint detection in 1D Poisson time series This model detects changepoints in 1D time series data using a Poisson likelihood. It assumes the data follows a Poisson distribution with different rates in different segments separated by changepoints. \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (1D Numpy array): Time series data n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = np.asarray(data_array) if self.data_array.ndim != 1: raise ValueError(\"data_array must be 1-dimensional\") self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Calculate initial lambda values by splitting data into segments mean_vals = np.array([ np.mean(x) for x in np.array_split(data_array, n_states) ]) mean_vals += 0.01 # To avoid zero starting prob idx = np.arange(len(data_array)) length = len(data_array) with pm.Model() as model: # Lambda parameters for each state (Poisson rates) lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=n_states ) # Changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Initialize changepoints evenly across the time series even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) # Convert to actual time indices tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent ) # Create weight matrix for smooth transitions between states weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis] ) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0 ) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0 ) weight_stack = weight_stack * inverse_stack # Calculate time-varying lambda lambda_t = lambda_latent.dot(weight_stack) # Observation model observation = pm.Poisson(\"obs\", lambda_t, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data - 1D array with 100 time points test_data = gen_test_array(100, n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = PoissonChangepoint1D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for PoissonChangepoint1D passed\") return True","title":"PoissonChangepoint1D"},{"location":"api/#pytau.changepoint_model.PoissonChangepoint1D.__init__","text":"Parameters: Name Type Description Default data_array 1D Numpy array Time series data required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (1D Numpy array): Time series data n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = np.asarray(data_array) if self.data_array.ndim != 1: raise ValueError(\"data_array must be 1-dimensional\") self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.PoissonChangepoint1D.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states # Calculate initial lambda values by splitting data into segments mean_vals = np.array([ np.mean(x) for x in np.array_split(data_array, n_states) ]) mean_vals += 0.01 # To avoid zero starting prob idx = np.arange(len(data_array)) length = len(data_array) with pm.Model() as model: # Lambda parameters for each state (Poisson rates) lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=n_states ) # Changepoint locations a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) # Initialize changepoints evenly across the time series even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, initval=even_switches, shape=(n_states - 1) ).sort(axis=-1) # Convert to actual time indices tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent ) # Create weight matrix for smooth transitions between states weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, np.newaxis] ) weight_stack = tt.concatenate( [np.ones((1, length)), weight_stack], axis=0 ) inverse_stack = 1 - weight_stack[1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((1, length))], axis=0 ) weight_stack = weight_stack * inverse_stack # Calculate time-varying lambda lambda_t = lambda_latent.dot(weight_stack) # Observation model observation = pm.Poisson(\"obs\", lambda_t, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.PoissonChangepoint1D.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data - 1D array with 100 time points test_data = gen_test_array(100, n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = PoissonChangepoint1D(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for PoissonChangepoint1D passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.SingleTastePoisson","text":"Bases: ChangepointModel Model for changepoint on single taste ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoisson(ChangepointModel): \"\"\"Model for changepoint on single taste ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, # initval=even_switches, shape=(trials, n_states - 1), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = tt.tensordot(weight_stack, lambda_latent, [ 1, 1]).swapaxes(1, 2) observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for SingleTastePoisson passed\") return True","title":"SingleTastePoisson"},{"location":"api/#pytau.changepoint_model.SingleTastePoisson.__init__","text":"Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.SingleTastePoisson.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, n_states)) a_tau = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b_tau = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a_tau, b_tau, # initval=even_switches, shape=(trials, n_states - 1), ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = tt.tensordot(weight_stack, lambda_latent, [ 1, 1]).swapaxes(1, 2) observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.SingleTastePoisson.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoisson(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames print(\"Test for SingleTastePoisson passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonDirichlet","text":"Bases: ChangepointModel Model for changepoint on single taste using dirichlet process prior Source code in pytau/changepoint_model.py class SingleTastePoissonDirichlet(ChangepointModel): \"\"\" Model for changepoint on single taste using dirichlet process prior \"\"\" def __init__(self, data_array, max_states=10, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time max_states (int): Maximum number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, max_states)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=(trials, max_states)) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic( \"w_raw\", stick_breaking_trial(beta, trials)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic( \"w_latent\", w_raw / w_raw.sum(axis=-1)[:, None]) tau = pm.Deterministic(\"tau\", tt.cumsum( w_latent * length, axis=-1)[:, :-1]) # ===================== # Rate over time # ===================== # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) # Trials x States x Time weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot(weight_stack, lambda_latent, [1, 1]).swapaxes(1, 2), ) # ===================== # Likelihood # ===================== observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for SingleTastePoissonDirichlet passed\") return True","title":"SingleTastePoissonDirichlet"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonDirichlet.__init__","text":"Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required max_states int Maximum number of states to model 10 **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, max_states=10, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time max_states (int): Maximum number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.max_states = max_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonDirichlet.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array max_states = self.max_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, max_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # =================== # Emissions Variables # =================== lambda_latent = pm.Exponential( \"lambda\", 1 / mean_vals, shape=(nrns, max_states)) # ===================== # Changepoint Variables # ===================== # Hyperpriors on alpha a_gamma = pm.Gamma(\"a_gamma\", 10, 1) b_gamma = pm.Gamma(\"b_gamma\", 1.5, 1) # Concentration parameter for beta alpha = pm.Gamma(\"alpha\", a_gamma, b_gamma) # Draw beta's to calculate stick lengths beta = pm.Beta(\"beta\", 1, alpha, shape=(trials, max_states)) # Calculate stick lengths using stick_breaking process w_raw = pm.Deterministic( \"w_raw\", stick_breaking_trial(beta, trials)) # Make sure lengths add to 1, and scale to length of data w_latent = pm.Deterministic( \"w_latent\", w_raw / w_raw.sum(axis=-1)[:, None]) tau = pm.Deterministic(\"tau\", tt.cumsum( w_latent * length, axis=-1)[:, :-1]) # ===================== # Rate over time # ===================== # Weight stack to assign lambda's to point in time weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trials, 1, length)), weight_stack], axis=1) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trials, 1, length))], axis=1) # Trials x States x Time weight_stack = np.multiply(weight_stack, inverse_stack) lambda_ = pm.Deterministic( \"lambda_\", tt.tensordot(weight_stack, lambda_latent, [1, 1]).swapaxes(1, 2), ) # ===================== # Likelihood # ===================== observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonDirichlet.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonDirichlet(test_data, max_states=5) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"w_latent\" in trace.varnames print(\"Test for SingleTastePoissonDirichlet passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonTrialSwitch","text":"Bases: ChangepointModel Assuming only emissions change across trials Changepoint distribution remains constant Source code in pytau/changepoint_model.py class SingleTastePoissonTrialSwitch(ChangepointModel): \"\"\" Assuming only emissions change across trials Changepoint distribution remains constant \"\"\" def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", nrn_lambda.dimshuffle(0, \"x\"), shape=(nrn_num, switch_components), ) # nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, \"x\"), shape=(nrn_num, switch_components, n_states), ) # Define Changepoints # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trial_num, n_states - 1) ).sort(axis=-1) # Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # First, we can \"select\" sets of emissions depending on trial_changepoints # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: nrns x switch_comps x states # selected_trial_lambda : nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, \"x\"), axis=1, ), ) # Then, we can select state_emissions for every trial idx = np.arange(time_bins) # tau : Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trial_num, 1, time_bins)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trial_num, 1, time_bins))], axis=1 ) # Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Convert selected_trial_lambda : nrns x trials x states x \"time\" # nrns x trials x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 2, 1, \"x\") * weight_stack.dimshuffle(\"x\", 0, 1, 2), axis=2, ) # Convert to : trials x nrns x time lambda_ = lambda_.dimshuffle(1, 0, 2) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames print(\"Test for SingleTastePoissonTrialSwitch passed\") return True","title":"SingleTastePoissonTrialSwitch"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonTrialSwitch.__init__","text":"Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required switch_components int Number of trial switch components required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, switch_components, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time switch_components (int): Number of trial switch components n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.switch_components = switch_components self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonTrialSwitch.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array switch_components = self.switch_components n_states = self.n_states trial_num, nrn_num, time_bins = data_array.shape with pm.Model() as model: # Define Emissions # nrns nrn_lambda = pm.Exponential(\"nrn_lambda\", 10, shape=(nrn_num)) # nrns x switch_comps trial_lambda = pm.Exponential( \"trial_lambda\", nrn_lambda.dimshuffle(0, \"x\"), shape=(nrn_num, switch_components), ) # nrns x switch_comps x n_states state_lambda = pm.Exponential( \"state_lambda\", trial_lambda.dimshuffle(0, 1, \"x\"), shape=(nrn_num, switch_components, n_states), ) # Define Changepoints # Assuming distribution of changepoints remains # the same across all trials a = pm.HalfCauchy(\"a_tau\", 3.0, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 3.0, shape=n_states - 1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trial_num, n_states - 1) ).sort(axis=-1) # Trials x Changepoints tau = pm.Deterministic(\"tau\", time_bins * tau_latent) # Define trial switches # Will have same structure as regular changepoints even_trial_switches = np.linspace( 0, 1, switch_components + 1)[1:-1] tau_trial_latent = pm.Beta( \"tau_trial_latent\", 1, 1, initval=even_trial_switches, shape=(switch_components - 1), ).sort(axis=-1) # Trial_changepoints tau_trial = pm.Deterministic( \"tau_trial\", trial_num * tau_trial_latent) trial_idx = np.arange(trial_num) trial_selector = tt.math.sigmoid( trial_idx[np.newaxis, :] - tau_trial.dimshuffle(0, \"x\") ) trial_selector = tt.concatenate( [np.ones((1, trial_num)), trial_selector], axis=0) inverse_trial_selector = 1 - trial_selector[1:, :] inverse_trial_selector = tt.concatenate( [inverse_trial_selector, np.ones((1, trial_num))], axis=0 ) # First, we can \"select\" sets of emissions depending on trial_changepoints # switch_comps x trials trial_selector = np.multiply( trial_selector, inverse_trial_selector) # state_lambda: nrns x switch_comps x states # selected_trial_lambda : nrns x states x trials selected_trial_lambda = pm.Deterministic( \"selected_trial_lambda\", tt.sum( # \"nrns\" x switch_comps x \"states\" x trials trial_selector.dimshuffle(\"x\", 0, \"x\", 1) * state_lambda.dimshuffle(0, 1, 2, \"x\"), axis=1, ), ) # Then, we can select state_emissions for every trial idx = np.arange(time_bins) # tau : Trials x Changepoints weight_stack = tt.math.sigmoid( idx[np.newaxis, :] - tau[:, :, np.newaxis]) weight_stack = tt.concatenate( [np.ones((trial_num, 1, time_bins)), weight_stack], axis=1 ) inverse_stack = 1 - weight_stack[:, 1:] inverse_stack = tt.concatenate( [inverse_stack, np.ones((trial_num, 1, time_bins))], axis=1 ) # Trials x states x Time weight_stack = np.multiply(weight_stack, inverse_stack) # Convert selected_trial_lambda : nrns x trials x states x \"time\" # nrns x trials x time lambda_ = tt.sum( selected_trial_lambda.dimshuffle(0, 2, 1, \"x\") * weight_stack.dimshuffle(\"x\", 0, 1, 2), axis=2, ) # Convert to : trials x nrns x time lambda_ = lambda_.dimshuffle(1, 0, 2) # Add observations observation = pm.Poisson(\"obs\", lambda_, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonTrialSwitch.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonTrialSwitch( test_data, self.switch_components, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"nrn_lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"tau_trial\" in trace.varnames assert \"state_lambda\" in trace.varnames print(\"Test for SingleTastePoissonTrialSwitch passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsig","text":"Bases: ChangepointModel Model for changepoint on single taste **Uses variables sigmoid slope inferred from data ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoissonVarsig(ChangepointModel): \"\"\"Model for changepoint on single taste **Uses variables sigmoid slope inferred from data ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # Sigmoid slope sig_b = pm.Normal(\"sig_b\", -1, 2, shape=n_states - 1) # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) sig_b_temp = tt.tile( sig_b[np.newaxis, :, np.newaxis], (trials, 1, len(idx))) weight_stack = var_sig_exp_tt(idx_temp - tau_temp, sig_b_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsig(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"sig_b\" in trace.varnames print(\"Test for SingleTastePoissonVarsig passed\") return True","title":"SingleTastePoissonVarsig"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsig.__init__","text":"Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states","title":"__init__"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsig.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 with pm.Model() as model: # Sigmoid slope sig_b = pm.Normal(\"sig_b\", -1, 2, shape=n_states - 1) # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) sig_b_temp = tt.tile( sig_b[np.newaxis, :, np.newaxis], (trials, 1, len(idx))) weight_stack = var_sig_exp_tt(idx_temp - tau_temp, sig_b_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsig.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsig(test_data, self.n_states) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"sig_b\" in trace.varnames print(\"Test for SingleTastePoissonVarsig passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsigFixed","text":"Bases: ChangepointModel Model for changepoint on single taste **Uses sigmoid with given slope ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions Source code in pytau/changepoint_model.py class SingleTastePoissonVarsigFixed(ChangepointModel): \"\"\"Model for changepoint on single taste **Uses sigmoid with given slope ** Largely taken from \"non_hardcoded_changepoint_test_3d.ipynb\" ** Note : This model does not have hierarchical structure for emissions \"\"\" def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model inds_span(float) : Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) with pm.Model() as model: # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) weight_stack = sigmoid(idx_temp - tau_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"state0\" in trace.varnames print(\"Test for SingleTastePoissonVarsigFixed passed\") return True","title":"SingleTastePoissonVarsigFixed"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsigFixed.__init__","text":"Parameters: Name Type Description Default data_array 3D Numpy array trials x neurons x time required n_states int Number of states to model required inds_span(float) Number of indices to cover 5-95% change in sigmoid required **kwargs Additional arguments {} Source code in pytau/changepoint_model.py def __init__(self, data_array, n_states, inds_span=1, **kwargs): \"\"\" Args: data_array (3D Numpy array): trials x neurons x time n_states (int): Number of states to model inds_span(float) : Number of indices to cover 5-95% change in sigmoid **kwargs: Additional arguments \"\"\" super().__init__(**kwargs) self.data_array = data_array self.n_states = n_states self.inds_span = inds_span","title":"__init__"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsigFixed.generate_model","text":"Returns: Type Description pymc model: Model class containing graph to run inference on Source code in pytau/changepoint_model.py def generate_model(self): \"\"\" Returns: pymc model: Model class containing graph to run inference on \"\"\" data_array = self.data_array n_states = self.n_states inds_span = self.inds_span mean_vals = np.array( [np.mean(x, axis=-1) for x in np.array_split(data_array, n_states, axis=-1)] ).T mean_vals = np.mean(mean_vals, axis=1) mean_vals += 0.01 # To avoid zero starting prob lambda_test_vals = np.diff(mean_vals, axis=-1) even_switches = np.linspace(0, 1, n_states + 1)[1:-1] nrns = data_array.shape[1] trials = data_array.shape[0] idx = np.arange(data_array.shape[-1]) length = idx.max() + 1 # Define sigmoid with given sharpness sig_b = inds_to_b(inds_span) def sigmoid(x): b_temp = tt.tile( np.array(sig_b)[None, None, None], x.tag.test_value.shape) return 1 / (1 + tt.exp(-b_temp * x)) with pm.Model() as model: # Initial value s0 = pm.Exponential( \"state0\", 1 / (np.mean(mean_vals)), shape=nrns, initval=mean_vals[:, 0] ) # Changes to lambda lambda_diff = pm.Normal( \"lambda_diff\", mu=0, sigma=10, shape=(nrns, n_states - 1), initval=lambda_test_vals, ) # This is only here to be extracted at the end of sampling # NOT USED DIRECTLY IN MODEL lambda_fin = pm.Deterministic( \"lambda\", tt.concatenate( [s0[:, np.newaxis], lambda_diff], axis=-1) ) # Changepoint positions a = pm.HalfCauchy(\"a_tau\", 10, shape=n_states - 1) b = pm.HalfCauchy(\"b_tau\", 10, shape=n_states - 1) tau_latent = pm.Beta( \"tau_latent\", a, b, # initval=even_switches, shape=(trials, n_states - 1) ).sort(axis=-1) tau = pm.Deterministic( \"tau\", idx.min() + (idx.max() - idx.min()) * tau_latent) # Mechanical manipulations to generate firing rates idx_temp = np.tile( idx[np.newaxis, np.newaxis, :], (trials, n_states - 1, 1)) tau_temp = tt.tile(tau[:, :, np.newaxis], (1, 1, len(idx))) weight_stack = sigmoid(idx_temp - tau_temp) weight_stack_temp = tt.tile( weight_stack[:, np.newaxis, :, :], (1, nrns, 1, 1)) s0_temp = tt.tile( s0[np.newaxis, :, np.newaxis, np.newaxis], (trials, 1, n_states - 1, len(idx)), ) lambda_diff_temp = tt.tile( lambda_diff[np.newaxis, :, :, np.newaxis], (trials, 1, 1, len(idx)) ) # Calculate lambda lambda_ = pm.Deterministic( \"lambda_\", tt.sum(s0_temp + (weight_stack_temp * lambda_diff_temp), axis=2), ) # Bound lambda to prevent the diffs from making it negative # Don't let it go down to zero otherwise we have trouble with probabilities lambda_bounded = pm.Deterministic( \"lambda_bounded\", tt.switch(lambda_ >= 0.01, lambda_, 0.01) ) # Add observations observation = pm.Poisson( \"obs\", lambda_bounded, observed=data_array) return model","title":"generate_model"},{"location":"api/#pytau.changepoint_model.SingleTastePoissonVarsigFixed.test","text":"Test the model with synthetic data Source code in pytau/changepoint_model.py def test(self): \"\"\"Test the model with synthetic data\"\"\" # Generate test data test_data = gen_test_array( (5, 10, 100), n_states=self.n_states, type=\"poisson\") # Create model with test data test_model = SingleTastePoissonVarsigFixed( test_data, self.n_states, self.inds_span) model = test_model.generate_model() # Run a minimal inference to verify model works with model: # Just do a few iterations to test functionality inference = pm.ADVI() approx = pm.fit(n=10, method=inference) trace = approx.sample(draws=10) # Check if expected variables are in the trace assert \"lambda\" in trace.varnames assert \"tau\" in trace.varnames assert \"state0\" in trace.varnames print(\"Test for SingleTastePoissonVarsigFixed passed\") return True","title":"test"},{"location":"api/#pytau.changepoint_model.advi_fit","text":"Convenience function to perform ADVI fit on model Parameters: Name Type Description Default model pymc model model object to run inference on required fit int Number of iterationst to fit the model for required samples int Number of samples to draw from fitted model required Returns: Name Type Description model original model on which inference was run, approx fitted model, lambda_stack array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run Source code in pytau/changepoint_model.py def advi_fit(model, fit, samples, convergence_tol=None): \"\"\"Convenience function to perform ADVI fit on model Args: model (pymc model): model object to run inference on fit (int): Number of iterationst to fit the model for samples (int): Number of samples to draw from fitted model Returns: model: original model on which inference was run, approx: fitted model, lambda_stack: array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run \"\"\" if convergence_tol is not None: callbacks = [pm.callbacks.CheckParametersConvergence( tolerance=convergence_tol)] print(\"Using convergence callback with tolerance:\", convergence_tol) else: callbacks = None with model: inference = pm.ADVI(\"full-rank\") approx = pm.fit(n=fit, method=inference, callbacks=callbacks) idata = approx.sample(draws=samples) # Check if tau exists in posterior samples (PyMC5 uses InferenceData) if \"tau\" not in idata.posterior.data_vars: available_vars = list(idata.posterior.data_vars.keys()) raise KeyError( f\"'tau' not found in posterior samples. Available variables: {available_vars}\") # Extract relevant variables from InferenceData posterior try: tau_samples = idata.posterior[\"tau\"].values # Handle potential dimension issues if tau_samples.ndim > 2: tau_samples = tau_samples.reshape(-1, tau_samples.shape[-1]) except Exception as e: print(f\"Error extracting tau samples: {e}\") tau_samples = None # Get observed data from model (PyMC5 compatible) # Since notebooks don't use fit_data, return None to avoid compatibility issues observed_data = None if \"lambda\" in idata.posterior.data_vars: try: lambda_stack = idata.posterior[\"lambda\"].values # Handle potential dimension issues if lambda_stack.ndim > 3: lambda_stack = lambda_stack.reshape(-1, *lambda_stack.shape[-2:]) lambda_stack = lambda_stack.swapaxes(0, 1) return model, approx, lambda_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting lambda samples: {e}\") return model, approx, None, tau_samples, observed_data if \"mu\" in idata.posterior.data_vars: try: mu_stack = idata.posterior[\"mu\"].values sigma_stack = idata.posterior[\"sigma\"].values # Handle potential dimension issues if mu_stack.ndim > 3: mu_stack = mu_stack.reshape(-1, *mu_stack.shape[-2:]) if sigma_stack.ndim > 3: sigma_stack = sigma_stack.reshape(-1, *sigma_stack.shape[-2:]) mu_stack = mu_stack.swapaxes(0, 1) sigma_stack = sigma_stack.swapaxes(0, 1) return model, approx, mu_stack, sigma_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting mu/sigma samples: {e}\") return model, approx, None, None, tau_samples, observed_data # Fallback - return what we can return model, approx, None, tau_samples, observed_data","title":"advi_fit"},{"location":"api/#pytau.changepoint_model.all_taste_poisson","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoisson(data_array, n_states, **kwargs) return model_class.generate_model()","title":"all_taste_poisson"},{"location":"api/#pytau.changepoint_model.all_taste_poisson_trial_switch","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoissonTrialSwitch( data_array, switch_components, n_states, **kwargs) return model_class.generate_model()","title":"all_taste_poisson_trial_switch"},{"location":"api/#pytau.changepoint_model.all_taste_poisson_varsig_fixed","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def all_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = AllTastePoissonVarsigFixed( data_array, n_states, inds_span, **kwargs) return model_class.generate_model()","title":"all_taste_poisson_varsig_fixed"},{"location":"api/#pytau.changepoint_model.dpp_fit","text":"Convenience function to fit DPP model Source code in pytau/changepoint_model.py def dpp_fit(model, n_chains=24, n_cores=1, tune=500, draws=500, use_numpyro=False): \"\"\"Convenience function to fit DPP model\"\"\" if not use_numpyro: with model: dpp_trace = pm.sample( tune=tune, draws=draws, target_accept=0.95, chains=n_chains, cores=n_cores, return_inferencedata=False, ) else: with model: dpp_trace = pm.sample( nuts_sampler=\"numpyro\", tune=tune, draws=draws, target_accept=0.95, chains=n_chains, cores=n_cores, return_inferencedata=False, ) return dpp_trace","title":"dpp_fit"},{"location":"api/#pytau.changepoint_model.extract_inferred_values","text":"Convenience function to extract inferred values from ADVI fit Parameters: Name Type Description Default trace dict trace required Returns: Name Type Description dict dictionary of inferred values Source code in pytau/changepoint_model.py def extract_inferred_values(trace): \"\"\"Convenience function to extract inferred values from ADVI fit Args: trace (dict): trace Returns: dict: dictionary of inferred values \"\"\" # Extract relevant variables from trace out_dict = dict(tau_samples=trace[\"tau\"]) if \"lambda\" in trace.varnames: out_dict[\"lambda_stack\"] = trace[\"lambda\"].swapaxes(0, 1) if \"mu\" in trace.varnames: out_dict[\"mu_stack\"] = trace[\"mu\"].swapaxes(0, 1) out_dict[\"sigma_stack\"] = trace[\"sigma\"].swapaxes(0, 1) return out_dict","title":"extract_inferred_values"},{"location":"api/#pytau.changepoint_model.find_best_states","text":"Convenience function to find best number of states for model Parameters: Name Type Description Default data array array on which to run inference required model_generator function function that generates model required n_fit int Number of iterationst to fit the model for required n_samples int Number of samples to draw from fitted model required min_states int Minimum number of states to test 2 max_states int Maximum number of states to test 10 convergence_tol float Tolerance for convergence. If None, will not check for convergence. None Returns: Name Type Description best_model model with best number of states, model_list list of models with different number of states, elbo_values list of elbo values for different number of states Source code in pytau/changepoint_model.py def find_best_states( data, model_generator, n_fit, n_samples, min_states=2, max_states=10, convergence_tol=None, ): \"\"\"Convenience function to find best number of states for model Args: data (array): array on which to run inference model_generator (function): function that generates model n_fit (int): Number of iterationst to fit the model for n_samples (int): Number of samples to draw from fitted model min_states (int): Minimum number of states to test max_states (int): Maximum number of states to test convergence_tol (float): Tolerance for convergence. If None, will not check for convergence. Returns: best_model: model with best number of states, model_list: list of models with different number of states, elbo_values: list of elbo values for different number of states \"\"\" n_state_array = np.arange(min_states, max_states + 1) elbo_values = [] model_list = [] for n_states in tqdm(n_state_array): print(f\"Fitting model with {n_states} states\") # Have to use int instead of np.int64 model = model_generator(data, int(n_states)) model, approx = advi_fit(model, n_fit, n_samples, convergence_tol)[:2] elbo_values.append(approx.hist[-1]) model_list.append(model) best_model = model_list[np.argmin(elbo_values)] return best_model, model_list, elbo_values","title":"find_best_states"},{"location":"api/#pytau.changepoint_model.gaussian_changepoint_mean_2d","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_2d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMean2D(data_array, n_states, **kwargs) return model_class.generate_model()","title":"gaussian_changepoint_mean_2d"},{"location":"api/#pytau.changepoint_model.gaussian_changepoint_mean_dirichlet","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_dirichlet(data_array, max_states=15, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMeanDirichlet( data_array, max_states, **kwargs) return model_class.generate_model()","title":"gaussian_changepoint_mean_dirichlet"},{"location":"api/#pytau.changepoint_model.gaussian_changepoint_mean_var_2d","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def gaussian_changepoint_mean_var_2d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = GaussianChangepointMeanVar2D(data_array, n_states, **kwargs) return model_class.generate_model()","title":"gaussian_changepoint_mean_var_2d"},{"location":"api/#pytau.changepoint_model.gen_test_array","text":"Generate test array for model fitting Last 2 dimensions consist of a single trial Time will always be last dimension Parameters: Name Type Description Default array_size tuple or int Size of array to generate. If int, generates 1D array. required n_states int Number of states to generate required type str Type of data to generate - normal - poisson 'poisson' Source code in pytau/changepoint_model.py def gen_test_array(array_size, n_states, type=\"poisson\"): \"\"\" Generate test array for model fitting Last 2 dimensions consist of a single trial Time will always be last dimension Args: array_size (tuple or int): Size of array to generate. If int, generates 1D array. n_states (int): Number of states to generate type (str): Type of data to generate - normal - poisson \"\"\" # Handle 1D case if isinstance(array_size, int): assert array_size > n_states, \"Array too small for states\" assert type in [ \"normal\", \"poisson\"], \"Invalid type, please use normal or poisson\" # Generate transition times for 1D case transition_times = np.random.random(n_states) transition_times = np.cumsum(transition_times) transition_times = transition_times / transition_times.max() transition_times *= array_size transition_times = transition_times.astype(int) # Generate state bounds state_bounds = np.zeros(n_states + 1, dtype=int) state_bounds[1:] = transition_times state_bounds[-1] = array_size # Generate state rates lambda_vals = np.random.exponential(2.0, n_states) + 0.5 # Generate 1D array rate_array = np.zeros(array_size) for i in range(n_states): start_idx = state_bounds[i] end_idx = state_bounds[i + 1] rate_array[start_idx:end_idx] = lambda_vals[i] if type == \"poisson\": return np.random.poisson(rate_array) else: return np.random.normal(loc=rate_array, scale=0.1) # Handle multi-dimensional case (existing code) assert array_size[-1] > n_states, \"Array too small for states\" assert type in [ \"normal\", \"poisson\"], \"Invalid type, please use normal or poisson\" # Generate transition times transition_times = np.random.random((*array_size[:-2], n_states)) transition_times = np.cumsum(transition_times, axis=-1) transition_times = transition_times / \\ transition_times.max(axis=-1, keepdims=True) transition_times *= array_size[-1] transition_times = np.vectorize(int)(transition_times) # Generate state bounds state_bounds = np.zeros((*array_size[:-2], n_states + 1), dtype=int) state_bounds[..., 1:] = transition_times # Generate state rates lambda_vals = np.random.random((*array_size[:-1], n_states)) # Generate array rate_array = np.zeros(array_size) inds = list(np.ndindex(lambda_vals.shape)) for this_ind in inds: this_lambda = lambda_vals[this_ind[:-2]][:, this_ind[-1]] this_state_bounds = [ state_bounds[(*this_ind[:-2], this_ind[-1])], state_bounds[(*this_ind[:-2], this_ind[-1] + 1)], ] rate_array[this_ind[:-2]][:, slice(*this_state_bounds)] = this_lambda[:, None] if type == \"poisson\": return np.random.poisson(rate_array) else: return np.random.normal(loc=rate_array, scale=0.1)","title":"gen_test_array"},{"location":"api/#pytau.changepoint_model.mcmc_fit","text":"Convenience function to perform ADVI fit on model Parameters: Name Type Description Default model pymc model model object to run inference on required samples int Number of samples to draw using MCMC required Returns: Name Type Description model original model on which inference was run, trace samples drawn from MCMC, lambda_stack array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run Source code in pytau/changepoint_model.py def mcmc_fit(model, samples): \"\"\"Convenience function to perform ADVI fit on model Args: model (pymc model): model object to run inference on samples (int): Number of samples to draw using MCMC Returns: model: original model on which inference was run, trace: samples drawn from MCMC, lambda_stack: array containing lambda (emission) values, tau_samples,: array containing samples from changepoint distribution model.obs.observations: processed array on which fit was run \"\"\" with model: sampler_kwargs = {\"cores\": 1, \"chains\": 4} idata = pm.sample(draws=samples, **sampler_kwargs) # Thin the samples (every 10th sample) idata_thinned = idata.sel(draw=slice(None, None, 10)) # Extract relevant variables from InferenceData posterior try: tau_samples = idata_thinned.posterior[\"tau\"].values # Handle potential dimension issues if tau_samples.ndim > 2: tau_samples = tau_samples.reshape(-1, tau_samples.shape[-1]) except Exception as e: print(f\"Error extracting tau samples: {e}\") tau_samples = None # Get observed data from model (PyMC5 compatible) # Since notebooks don't use fit_data, return None to avoid compatibility issues observed_data = None if \"lambda\" in idata_thinned.posterior.data_vars: try: lambda_stack = idata_thinned.posterior[\"lambda\"].values # Handle potential dimension issues if lambda_stack.ndim > 3: lambda_stack = lambda_stack.reshape(-1, *lambda_stack.shape[-2:]) lambda_stack = lambda_stack.swapaxes(0, 1) return model, idata_thinned, lambda_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting lambda samples: {e}\") return model, idata_thinned, None, tau_samples, observed_data if \"mu\" in idata_thinned.posterior.data_vars: try: mu_stack = idata_thinned.posterior[\"mu\"].values sigma_stack = idata_thinned.posterior[\"sigma\"].values # Handle potential dimension issues if mu_stack.ndim > 3: mu_stack = mu_stack.reshape(-1, *mu_stack.shape[-2:]) if sigma_stack.ndim > 3: sigma_stack = sigma_stack.reshape(-1, *sigma_stack.shape[-2:]) mu_stack = mu_stack.swapaxes(0, 1) sigma_stack = sigma_stack.swapaxes(0, 1) return model, idata_thinned, mu_stack, sigma_stack, tau_samples, observed_data except Exception as e: print(f\"Error extracting mu/sigma samples: {e}\") return model, idata_thinned, None, None, tau_samples, observed_data # Fallback - return what we can return model, idata_thinned, None, tau_samples, observed_data","title":"mcmc_fit"},{"location":"api/#pytau.changepoint_model.poisson_changepoint_1d","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def poisson_changepoint_1d(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = PoissonChangepoint1D(data_array, n_states, **kwargs) return model_class.generate_model()","title":"poisson_changepoint_1d"},{"location":"api/#pytau.changepoint_model.run_all_tests","text":"Run tests for all model classes Source code in pytau/changepoint_model.py def run_all_tests(): \"\"\"Run tests for all model classes\"\"\" # Create test data test_data_1d = gen_test_array(100, n_states=3, type=\"poisson\") test_data_2d = gen_test_array((10, 100), n_states=3, type=\"normal\") test_data_3d = gen_test_array((5, 10, 100), n_states=3, type=\"poisson\") test_data_4d = gen_test_array((2, 5, 10, 100), n_states=3, type=\"poisson\") # Test each model class models_to_test = [ PoissonChangepoint1D(test_data_1d, 3), GaussianChangepointMeanVar2D(test_data_2d, 3), GaussianChangepointMeanDirichlet(test_data_2d, 5), GaussianChangepointMean2D(test_data_2d, 3), SingleTastePoissonDirichlet(test_data_3d, 5), SingleTastePoisson(test_data_3d, 3), SingleTastePoissonVarsig(test_data_3d, 3), SingleTastePoissonVarsigFixed(test_data_3d, 3, 1), SingleTastePoissonTrialSwitch(test_data_3d, 2, 3), AllTastePoisson(test_data_4d, 3), AllTastePoissonVarsigFixed(test_data_4d, 3, 1), AllTastePoissonTrialSwitch(test_data_4d, 2, 3), ] failed_tests = [] pbar = tqdm(models_to_test, total=len(models_to_test)) for model in pbar: try: model.test() pbar.set_description(f\"Test passed for {model.__class__.__name__}\") except Exception as e: failed_tests.append(model.__class__.__name__) print(f\"Test failed for {model.__class__.__name__}: {str(e)}\") print(\"All tests completed\") if failed_tests: print(\"Failed tests:\", failed_tests)","title":"run_all_tests"},{"location":"api/#pytau.changepoint_model.single_taste_poisson","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoisson(data_array, n_states, **kwargs) return model_class.generate_model()","title":"single_taste_poisson"},{"location":"api/#pytau.changepoint_model.single_taste_poisson_dirichlet","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_dirichlet(data_array, max_states=10, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonDirichlet(data_array, max_states, **kwargs) return model_class.generate_model()","title":"single_taste_poisson_dirichlet"},{"location":"api/#pytau.changepoint_model.single_taste_poisson_trial_switch","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_trial_switch(data_array, switch_components, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonTrialSwitch( data_array, switch_components, n_states, **kwargs) return model_class.generate_model()","title":"single_taste_poisson_trial_switch"},{"location":"api/#pytau.changepoint_model.single_taste_poisson_varsig","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_varsig(data_array, n_states, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonVarsig(data_array, n_states, **kwargs) return model_class.generate_model()","title":"single_taste_poisson_varsig"},{"location":"api/#pytau.changepoint_model.single_taste_poisson_varsig_fixed","text":"Wrapper function for backward compatibility Source code in pytau/changepoint_model.py def single_taste_poisson_varsig_fixed(data_array, n_states, inds_span=1, **kwargs): \"\"\"Wrapper function for backward compatibility\"\"\" model_class = SingleTastePoissonVarsigFixed( data_array, n_states, inds_span, **kwargs) return model_class.generate_model()","title":"single_taste_poisson_varsig_fixed"},{"location":"api/#pytau.changepoint_model.var_sig_exp_tt","text":"x --> b --> Source code in pytau/changepoint_model.py def var_sig_exp_tt(x, b): \"\"\" x --> b --> \"\"\" return 1 / (1 + tt.exp(-tt.exp(b) * x))","title":"var_sig_exp_tt"},{"location":"api/#pytau.changepoint_model.var_sig_tt","text":"x --> b --> Source code in pytau/changepoint_model.py def var_sig_tt(x, b): \"\"\" x --> b --> \"\"\" return 1 / (1 + tt.exp(-b * x))","title":"var_sig_tt"},{"location":"api/#preprocessing-functions","text":"Code to preprocess spike trains before feeding into model","title":"=== Preprocessing functions ==="},{"location":"api/#pytau.changepoint_preprocess.preprocess_all_taste","text":"Preprocess array containing trials for all tastes (in blocks) concatenated Parameters: Name Type Description Default spike_array 4D Numpy Array Taste x Trials x Neurons x Time required time_lims List/Tuple/Numpy Array 2-element object indicating limits of array required bin_width int Width to use for binning required data_transform str Data-type to return {actual, shuffled, simulated} required Raises: Type Description Exception If transforms do not belong to ['shuffled','simulated','None',None] Returns: Type Description 4D Numpy Array Of processed data Source code in pytau/changepoint_preprocess.py def preprocess_all_taste(spike_array, time_lims, bin_width, data_transform): \"\"\"Preprocess array containing trials for all tastes (in blocks) concatenated Args: spike_array (4D Numpy Array): Taste x Trials x Neurons x Time time_lims (List/Tuple/Numpy Array): 2-element object indicating limits of array bin_width (int): Width to use for binning data_transform (str): Data-type to return {actual, shuffled, simulated} Raises: Exception: If transforms do not belong to ['shuffled','simulated','None',None] Returns: (4D Numpy Array): Of processed data \"\"\" accepted_transforms = [ \"trial_shuffled\", \"spike_shuffled\", \"simulated\", \"None\", None, ] if data_transform not in accepted_transforms: raise Exception( f\"data_transform must be of type {accepted_transforms}\") ################################################## # Create shuffled data ################################################## # Shuffle neurons across trials FOR SAME TASTE if data_transform == \"trial_shuffled\": transformed_dat = np.array( [np.random.permutation(neuron) for neuron in np.swapaxes(spike_array, 2, 0)] ) transformed_dat = np.swapaxes(transformed_dat, 0, 2) if data_transform == \"spike_shuffled\": transformed_dat = spike_array.swapaxes(-1, 0) transformed_dat = np.stack([np.random.permutation(x) for x in transformed_dat]) transformed_dat = transformed_dat.swapaxes(0, -1) ################################################## # Create simulated data ################################################## # Inhomogeneous poisson process using mean firing rates elif data_transform == \"simulated\": mean_firing = np.mean(spike_array, axis=1) mean_firing = np.broadcast_to(mean_firing[:, None], spike_array.shape) # Simulate spikes transformed_dat = (np.random.random( spike_array.shape) < mean_firing) * 1 ################################################## # Null Transform Case ################################################## elif data_transform == None or data_transform == \"None\": transformed_dat = spike_array ################################################## # Bin Data ################################################## spike_binned = np.sum( transformed_dat[..., time_lims[0]: time_lims[1]].reshape( *transformed_dat.shape[:-1], -1, bin_width ), axis=-1, ) spike_binned = spike_binned.astype(int) return spike_binned","title":"preprocess_all_taste"},{"location":"api/#pytau.changepoint_preprocess.preprocess_single_taste","text":"Preprocess array containing trials for all tastes (in blocks) concatenated ** Note, it may be useful to use x-arrays here to keep track of coordinates Parameters: Name Type Description Default spike_array 3D Numpy array trials x neurons x time required time_lims List/Tuple/Numpy Array 2-element object indicating limits of array required bin_width int Width to use for binning required data_transform str Data-type to return {actual, trial_shuffled, spike_shuffled, simulated} required Raises: Type Description Exception If transforms do not belong to Returns: Type Description 3D Numpy Array Of processed data Source code in pytau/changepoint_preprocess.py def preprocess_single_taste(spike_array, time_lims, bin_width, data_transform): \"\"\"Preprocess array containing trials for all tastes (in blocks) concatenated ** Note, it may be useful to use x-arrays here to keep track of coordinates Args: spike_array (3D Numpy array): trials x neurons x time time_lims (List/Tuple/Numpy Array): 2-element object indicating limits of array bin_width (int): Width to use for binning data_transform (str): Data-type to return {actual, trial_shuffled, spike_shuffled, simulated} Raises: Exception: If transforms do not belong to ['trial_shuffled','spike_shuffled','simulated','None',None] Returns: (3D Numpy Array): Of processed data \"\"\" accepted_transforms = [ \"trial_shuffled\", \"spike_shuffled\", \"simulated\", \"None\", None, ] if data_transform not in accepted_transforms: raise Exception( f\"data_transform must be of type {accepted_transforms}\") ################################################## # Create shuffled data ################################################## # Shuffle neurons across trials FOR SAME TASTE if data_transform == \"trial_shuffled\": transformed_dat = np.array( [np.random.permutation(neuron) for neuron in np.swapaxes(spike_array, 1, 0)] ) transformed_dat = np.swapaxes(transformed_dat, 0, 1) if data_transform == \"spike_shuffled\": transformed_dat = np.moveaxis(spike_array, -1, 0) transformed_dat = np.stack([np.random.permutation(x) for x in transformed_dat]) transformed_dat = np.moveaxis(transformed_dat, 0, -1) ################################################## # Create simulated data ################################################## # Inhomogeneous poisson process using mean firing rates elif data_transform == \"simulated\": mean_firing = np.mean(spike_array, axis=0) # Simulate spikes transformed_dat = ( np.array( [ np.random.random(mean_firing.shape) < mean_firing for trial in range(spike_array.shape[0]) ] ) * 1 ) ################################################## # Null Transform Case ################################################## elif data_transform in (None, \"None\"): transformed_dat = spike_array ################################################## # Bin Data ################################################## spike_binned = np.sum( transformed_dat[..., time_lims[0]: time_lims[1]].reshape( *spike_array.shape[:-1], -1, bin_width ), axis=-1, ) spike_binned = spike_binned.astype(int) return spike_binned","title":"preprocess_single_taste"},{"location":"pymc_concepts/","text":"Understanding PyMC Concepts in PyTau This guide explains the key PyMC concepts used in PyTau for users who may not be familiar with Bayesian modeling or PyMC. For more detailed information, refer to the official PyMC documentation . Overview PyTau uses PyMC , a probabilistic programming library, to perform Bayesian inference on changepoint models. This approach allows us to: Quantify uncertainty in changepoint locations Incorporate prior knowledge about neural dynamics Obtain full posterior distributions rather than point estimates Core Concepts 1. Model A model in PyMC is a probabilistic representation of your data and assumptions. It consists of: Prior distributions : What we believe about parameters before seeing data Likelihood : How the data is generated given the parameters Observed data : The actual measurements we want to explain In PyTau, models are defined as classes that inherit from ChangepointModel and implement a generate_model() method. Example: Single Taste Poisson Model from pytau.changepoint_model import SingleTastePoisson # Create a model for spike train data # spike_array shape: (trials, neurons, time) model_obj = SingleTastePoisson( data_array=spike_array, n_states=3 # Number of states to detect ) # Generate the PyMC model model = model_obj.generate_model() What happens inside the model: Priors on changepoint times (tau) : Where state transitions might occur Priors on emission rates (lambda) : Firing rates in each state Likelihood : Poisson distribution connecting rates to observed spike counts The model structure looks like this: with pm.Model() as model: # Prior on changepoint times (sorted) tau = pm.Uniform('tau', lower=0, upper=time_bins, shape=states-1) tau_sorted = tt.sort(tau) # Prior on emission rates for each state lambda_latent = pm.Exponential('lambda', lam=1.0, shape=states) # Assign rates to time bins based on changepoints # (simplified - actual implementation more complex) lambda_t = assign_rates_by_changepoints(tau_sorted, lambda_latent) # Likelihood: observed spikes follow Poisson distribution obs = pm.Poisson('obs', mu=lambda_t, observed=spike_array) For more details on PyMC models, see the PyMC Model Building Guide . 2. Inference: Samples and Trace Once we have a model, we need to perform inference to learn about the parameters from the data. PyMC offers two main approaches: Variational Inference (ADVI) ADVI (Automatic Differentiation Variational Inference) is a fast approximate inference method that: Finds a simpler distribution that approximates the true posterior Is much faster than MCMC but less accurate Good for initial exploration or large datasets from pytau.changepoint_model import advi_fit # Fit the model using ADVI model, approx, lambda_stack, tau_samples, data = advi_fit( model, fit=10000, # Number of optimization iterations samples=5000 # Number of samples to draw from approximation ) What you get: - approx : The fitted approximation (a simpler distribution) - lambda_stack : Samples of emission rates (shape: [samples, states, ...] ) - tau_samples : Samples of changepoint times (shape: [samples, states-1] ) MCMC Sampling MCMC (Markov Chain Monte Carlo) generates samples by: Exploring the parameter space using random walks Spending more time in high-probability regions Providing high-quality samples but slower than ADVI from pytau.changepoint_model import mcmc_fit # Fit the model using MCMC model, trace, lambda_stack, tau_samples, data = mcmc_fit( model, samples=1000 # Number of samples per chain ) What you get: - trace : An InferenceData object containing all samples - lambda_stack : Samples of emission rates - tau_samples : Samples of changepoint times For more on inference methods, see: - ADVI in PyMC - MCMC in PyMC 3. Trace / InferenceData A trace (or InferenceData in modern PyMC) is a container for all the samples drawn during inference. Think of it as a table where: Each row is a sample from the posterior distribution Each column is a parameter in your model # After fitting with MCMC print(trace.posterior) # Output shows available variables: tau, lambda, etc. # Access specific parameter samples tau_samples = trace.posterior['tau'].values # Shape: (chains, draws, states-1) lambda_samples = trace.posterior['lambda'].values # Shape: (chains, draws, states, ...) Key operations with traces: import arviz as az # Summary statistics summary = az.summary(trace) print(summary) # Visualize posterior distributions az.plot_posterior(trace, var_names=['tau']) # Check convergence diagnostics az.plot_trace(trace, var_names=['tau', 'lambda']) Understanding the samples: Each sample represents one plausible set of parameter values given the data. By collecting many samples, we can: Estimate parameter means and medians Quantify uncertainty with credible intervals Visualize the full posterior distribution For more on working with traces, see the ArviZ documentation . 4. Posterior Predictive The posterior predictive distribution answers: \"If my model is correct, what new data would I expect to see?\" It's generated by: Taking parameter samples from the posterior (trace) Simulating new data using those parameters Collecting the simulated datasets # Generate posterior predictive samples with model: posterior_predictive = pm.sample_posterior_predictive( trace, var_names=['obs'] # Variable to predict ) # Access the predictions predicted_spikes = posterior_predictive.posterior_predictive['obs'].values # Shape: (chains, draws, trials, neurons, time) Why is this useful? Model checking : Compare predicted data to actual data If they look very different, the model may be misspecified Uncertainty quantification : See the range of possible outcomes Validation : Check if the model captures important data features import matplotlib.pyplot as plt # Compare actual vs predicted data actual_mean = spike_array.mean(axis=0) # Average across trials predicted_mean = predicted_spikes.mean(axis=(0,1)) # Average across chains and draws plt.figure(figsize=(12, 4)) plt.plot(actual_mean.T, alpha=0.5, label='Actual') plt.plot(predicted_mean.T, alpha=0.5, linestyle='--', label='Predicted') plt.xlabel('Time') plt.ylabel('Spike Count') plt.legend() plt.title('Actual vs Posterior Predictive') plt.show() For more on posterior predictive checks, see the PyMC Posterior Predictive Guide . How Everything Fits Together Here's a complete workflow showing how these concepts connect: from pytau.changepoint_model import SingleTastePoisson, advi_fit import arviz as az import matplotlib.pyplot as plt # 1. CREATE MODEL # Define the probabilistic model structure model_obj = SingleTastePoisson( data_array=spike_array, # Your data: (trials, neurons, time) n_states=3 ) model = model_obj.generate_model() # 2. PERFORM INFERENCE # Learn parameter distributions from data model, approx, lambda_stack, tau_samples, data = advi_fit( model, fit=10000, samples=5000 ) # 3. ANALYZE TRACE # Examine the posterior distribution print(\"Changepoint locations (mean \u00b1 std):\") for i in range(tau_samples.shape[1]): mean_tau = tau_samples[:, i].mean() std_tau = tau_samples[:, i].std() print(f\" Tau {i+1}: {mean_tau:.2f} \u00b1 {std_tau:.2f}\") # Visualize posterior distributions fig, axes = plt.subplots(1, 2, figsize=(12, 4)) # Plot changepoint distributions axes[0].hist(tau_samples[:, 0], bins=50, alpha=0.7, label='Tau 1') axes[0].hist(tau_samples[:, 1], bins=50, alpha=0.7, label='Tau 2') axes[0].set_xlabel('Time (bins)') axes[0].set_ylabel('Frequency') axes[0].set_title('Posterior Distribution of Changepoints') axes[0].legend() # Plot emission rate distributions for state in range(lambda_stack.shape[1]): axes[1].hist(lambda_stack[:, state, 0, 0], bins=50, alpha=0.5, label=f'State {state+1}') axes[1].set_xlabel('Firing Rate (Hz)') axes[1].set_ylabel('Frequency') axes[1].set_title('Posterior Distribution of Emission Rates') axes[1].legend() plt.tight_layout() plt.show() # 4. POSTERIOR PREDICTIVE CHECK # Validate the model by simulating new data with model: # Convert approx to trace for posterior predictive sampling idata = approx.sample(draws=1000) posterior_predictive = pm.sample_posterior_predictive( idata, var_names=['obs'] ) # Compare actual vs predicted actual_mean = spike_array.mean(axis=0) predicted_mean = posterior_predictive.posterior_predictive['obs'].values.mean(axis=(0,1)) plt.figure(figsize=(12, 4)) plt.plot(actual_mean[0], label='Actual', linewidth=2) plt.plot(predicted_mean[0], label='Predicted', linestyle='--', linewidth=2) plt.xlabel('Time (bins)') plt.ylabel('Mean Spike Count') plt.title('Posterior Predictive Check') plt.legend() plt.show() Key Takeaways Concept What It Is Why It Matters Model Mathematical representation of data generation Encodes assumptions and structure Inference Process of learning from data Produces parameter estimates Samples Individual draws from posterior Represent uncertainty Trace Collection of all samples Contains full posterior information Posterior Predictive Simulated data from fitted model Validates model quality Common Patterns in PyTau Pattern 1: Quick Model Fitting from pytau.changepoint_model import SingleTastePoisson, advi_fit # Fit and get results in one go model_obj = SingleTastePoisson(data_array=data, n_states=3) model = model_obj.generate_model() model, approx, lambda_stack, tau_samples, _ = advi_fit(model, fit=10000, samples=5000) # Analyze changepoints mean_changepoints = tau_samples.mean(axis=0) print(f\"Detected changepoints at: {mean_changepoints}\") Pattern 2: Model Comparison from pytau.changepoint_model import find_best_states # Automatically find optimal number of states best_model, model_list, elbo_values = find_best_states( data=spike_array, model_generator=SingleTastePoisson, n_fit=5000, n_samples=1000, min_states=2, max_states=6 ) print(f\"Best model has {best_model.n_states} states\") Pattern 3: High-Quality Sampling from pytau.changepoint_model import mcmc_fit # Use MCMC for publication-quality results model_obj = SingleTastePoisson(data_array=data, n_states=3) model = model_obj.generate_model() model, trace, lambda_stack, tau_samples, _ = mcmc_fit(model, samples=2000) # Check convergence import arviz as az print(az.summary(trace, var_names=['tau', 'lambda'])) Additional Resources PyMC Documentation - Official PyMC guide PyMC Examples - Gallery of example models Bayesian Methods for Hackers - Practical introduction to Bayesian methods ArviZ Documentation - Tools for analyzing Bayesian models PyMC Discourse - Community forum for questions Troubleshooting \"My samples look weird\" Check convergence: import arviz as az az.plot_trace(trace) # Look for \"hairy caterpillar\" patterns print(az.summary(trace)) # Check r_hat values (should be ~1.0) \"Inference is too slow\" Try these approaches: 1. Use ADVI instead of MCMC for initial exploration 2. Reduce the number of samples 3. Simplify the model (fewer states) 4. Use NumPyro backend for MCMC: pm.sample(nuts_sampler='numpyro') \"Results don't match my expectations\" Validate your model: 1. Check posterior predictive samples 2. Visualize the fitted changepoints on your data 3. Try different prior specifications 4. Ensure data preprocessing is correct Next Steps Now that you understand the core PyMC concepts, you can: Explore the API documentation for detailed function references Try the example notebooks in pytau/how_to/notebooks/ Experiment with different model types for your data Customize models for your specific research questions For questions or issues, please visit the PyTau GitHub repository .","title":"PyMC Concepts"},{"location":"pymc_concepts/#understanding-pymc-concepts-in-pytau","text":"This guide explains the key PyMC concepts used in PyTau for users who may not be familiar with Bayesian modeling or PyMC. For more detailed information, refer to the official PyMC documentation .","title":"Understanding PyMC Concepts in PyTau"},{"location":"pymc_concepts/#overview","text":"PyTau uses PyMC , a probabilistic programming library, to perform Bayesian inference on changepoint models. This approach allows us to: Quantify uncertainty in changepoint locations Incorporate prior knowledge about neural dynamics Obtain full posterior distributions rather than point estimates","title":"Overview"},{"location":"pymc_concepts/#core-concepts","text":"","title":"Core Concepts"},{"location":"pymc_concepts/#1-model","text":"A model in PyMC is a probabilistic representation of your data and assumptions. It consists of: Prior distributions : What we believe about parameters before seeing data Likelihood : How the data is generated given the parameters Observed data : The actual measurements we want to explain In PyTau, models are defined as classes that inherit from ChangepointModel and implement a generate_model() method.","title":"1. Model"},{"location":"pymc_concepts/#example-single-taste-poisson-model","text":"from pytau.changepoint_model import SingleTastePoisson # Create a model for spike train data # spike_array shape: (trials, neurons, time) model_obj = SingleTastePoisson( data_array=spike_array, n_states=3 # Number of states to detect ) # Generate the PyMC model model = model_obj.generate_model() What happens inside the model: Priors on changepoint times (tau) : Where state transitions might occur Priors on emission rates (lambda) : Firing rates in each state Likelihood : Poisson distribution connecting rates to observed spike counts The model structure looks like this: with pm.Model() as model: # Prior on changepoint times (sorted) tau = pm.Uniform('tau', lower=0, upper=time_bins, shape=states-1) tau_sorted = tt.sort(tau) # Prior on emission rates for each state lambda_latent = pm.Exponential('lambda', lam=1.0, shape=states) # Assign rates to time bins based on changepoints # (simplified - actual implementation more complex) lambda_t = assign_rates_by_changepoints(tau_sorted, lambda_latent) # Likelihood: observed spikes follow Poisson distribution obs = pm.Poisson('obs', mu=lambda_t, observed=spike_array) For more details on PyMC models, see the PyMC Model Building Guide .","title":"Example: Single Taste Poisson Model"},{"location":"pymc_concepts/#2-inference-samples-and-trace","text":"Once we have a model, we need to perform inference to learn about the parameters from the data. PyMC offers two main approaches:","title":"2. Inference: Samples and Trace"},{"location":"pymc_concepts/#variational-inference-advi","text":"ADVI (Automatic Differentiation Variational Inference) is a fast approximate inference method that: Finds a simpler distribution that approximates the true posterior Is much faster than MCMC but less accurate Good for initial exploration or large datasets from pytau.changepoint_model import advi_fit # Fit the model using ADVI model, approx, lambda_stack, tau_samples, data = advi_fit( model, fit=10000, # Number of optimization iterations samples=5000 # Number of samples to draw from approximation ) What you get: - approx : The fitted approximation (a simpler distribution) - lambda_stack : Samples of emission rates (shape: [samples, states, ...] ) - tau_samples : Samples of changepoint times (shape: [samples, states-1] )","title":"Variational Inference (ADVI)"},{"location":"pymc_concepts/#mcmc-sampling","text":"MCMC (Markov Chain Monte Carlo) generates samples by: Exploring the parameter space using random walks Spending more time in high-probability regions Providing high-quality samples but slower than ADVI from pytau.changepoint_model import mcmc_fit # Fit the model using MCMC model, trace, lambda_stack, tau_samples, data = mcmc_fit( model, samples=1000 # Number of samples per chain ) What you get: - trace : An InferenceData object containing all samples - lambda_stack : Samples of emission rates - tau_samples : Samples of changepoint times For more on inference methods, see: - ADVI in PyMC - MCMC in PyMC","title":"MCMC Sampling"},{"location":"pymc_concepts/#3-trace-inferencedata","text":"A trace (or InferenceData in modern PyMC) is a container for all the samples drawn during inference. Think of it as a table where: Each row is a sample from the posterior distribution Each column is a parameter in your model # After fitting with MCMC print(trace.posterior) # Output shows available variables: tau, lambda, etc. # Access specific parameter samples tau_samples = trace.posterior['tau'].values # Shape: (chains, draws, states-1) lambda_samples = trace.posterior['lambda'].values # Shape: (chains, draws, states, ...) Key operations with traces: import arviz as az # Summary statistics summary = az.summary(trace) print(summary) # Visualize posterior distributions az.plot_posterior(trace, var_names=['tau']) # Check convergence diagnostics az.plot_trace(trace, var_names=['tau', 'lambda']) Understanding the samples: Each sample represents one plausible set of parameter values given the data. By collecting many samples, we can: Estimate parameter means and medians Quantify uncertainty with credible intervals Visualize the full posterior distribution For more on working with traces, see the ArviZ documentation .","title":"3. Trace / InferenceData"},{"location":"pymc_concepts/#4-posterior-predictive","text":"The posterior predictive distribution answers: \"If my model is correct, what new data would I expect to see?\" It's generated by: Taking parameter samples from the posterior (trace) Simulating new data using those parameters Collecting the simulated datasets # Generate posterior predictive samples with model: posterior_predictive = pm.sample_posterior_predictive( trace, var_names=['obs'] # Variable to predict ) # Access the predictions predicted_spikes = posterior_predictive.posterior_predictive['obs'].values # Shape: (chains, draws, trials, neurons, time) Why is this useful? Model checking : Compare predicted data to actual data If they look very different, the model may be misspecified Uncertainty quantification : See the range of possible outcomes Validation : Check if the model captures important data features import matplotlib.pyplot as plt # Compare actual vs predicted data actual_mean = spike_array.mean(axis=0) # Average across trials predicted_mean = predicted_spikes.mean(axis=(0,1)) # Average across chains and draws plt.figure(figsize=(12, 4)) plt.plot(actual_mean.T, alpha=0.5, label='Actual') plt.plot(predicted_mean.T, alpha=0.5, linestyle='--', label='Predicted') plt.xlabel('Time') plt.ylabel('Spike Count') plt.legend() plt.title('Actual vs Posterior Predictive') plt.show() For more on posterior predictive checks, see the PyMC Posterior Predictive Guide .","title":"4. Posterior Predictive"},{"location":"pymc_concepts/#how-everything-fits-together","text":"Here's a complete workflow showing how these concepts connect: from pytau.changepoint_model import SingleTastePoisson, advi_fit import arviz as az import matplotlib.pyplot as plt # 1. CREATE MODEL # Define the probabilistic model structure model_obj = SingleTastePoisson( data_array=spike_array, # Your data: (trials, neurons, time) n_states=3 ) model = model_obj.generate_model() # 2. PERFORM INFERENCE # Learn parameter distributions from data model, approx, lambda_stack, tau_samples, data = advi_fit( model, fit=10000, samples=5000 ) # 3. ANALYZE TRACE # Examine the posterior distribution print(\"Changepoint locations (mean \u00b1 std):\") for i in range(tau_samples.shape[1]): mean_tau = tau_samples[:, i].mean() std_tau = tau_samples[:, i].std() print(f\" Tau {i+1}: {mean_tau:.2f} \u00b1 {std_tau:.2f}\") # Visualize posterior distributions fig, axes = plt.subplots(1, 2, figsize=(12, 4)) # Plot changepoint distributions axes[0].hist(tau_samples[:, 0], bins=50, alpha=0.7, label='Tau 1') axes[0].hist(tau_samples[:, 1], bins=50, alpha=0.7, label='Tau 2') axes[0].set_xlabel('Time (bins)') axes[0].set_ylabel('Frequency') axes[0].set_title('Posterior Distribution of Changepoints') axes[0].legend() # Plot emission rate distributions for state in range(lambda_stack.shape[1]): axes[1].hist(lambda_stack[:, state, 0, 0], bins=50, alpha=0.5, label=f'State {state+1}') axes[1].set_xlabel('Firing Rate (Hz)') axes[1].set_ylabel('Frequency') axes[1].set_title('Posterior Distribution of Emission Rates') axes[1].legend() plt.tight_layout() plt.show() # 4. POSTERIOR PREDICTIVE CHECK # Validate the model by simulating new data with model: # Convert approx to trace for posterior predictive sampling idata = approx.sample(draws=1000) posterior_predictive = pm.sample_posterior_predictive( idata, var_names=['obs'] ) # Compare actual vs predicted actual_mean = spike_array.mean(axis=0) predicted_mean = posterior_predictive.posterior_predictive['obs'].values.mean(axis=(0,1)) plt.figure(figsize=(12, 4)) plt.plot(actual_mean[0], label='Actual', linewidth=2) plt.plot(predicted_mean[0], label='Predicted', linestyle='--', linewidth=2) plt.xlabel('Time (bins)') plt.ylabel('Mean Spike Count') plt.title('Posterior Predictive Check') plt.legend() plt.show()","title":"How Everything Fits Together"},{"location":"pymc_concepts/#key-takeaways","text":"Concept What It Is Why It Matters Model Mathematical representation of data generation Encodes assumptions and structure Inference Process of learning from data Produces parameter estimates Samples Individual draws from posterior Represent uncertainty Trace Collection of all samples Contains full posterior information Posterior Predictive Simulated data from fitted model Validates model quality","title":"Key Takeaways"},{"location":"pymc_concepts/#common-patterns-in-pytau","text":"","title":"Common Patterns in PyTau"},{"location":"pymc_concepts/#pattern-1-quick-model-fitting","text":"from pytau.changepoint_model import SingleTastePoisson, advi_fit # Fit and get results in one go model_obj = SingleTastePoisson(data_array=data, n_states=3) model = model_obj.generate_model() model, approx, lambda_stack, tau_samples, _ = advi_fit(model, fit=10000, samples=5000) # Analyze changepoints mean_changepoints = tau_samples.mean(axis=0) print(f\"Detected changepoints at: {mean_changepoints}\")","title":"Pattern 1: Quick Model Fitting"},{"location":"pymc_concepts/#pattern-2-model-comparison","text":"from pytau.changepoint_model import find_best_states # Automatically find optimal number of states best_model, model_list, elbo_values = find_best_states( data=spike_array, model_generator=SingleTastePoisson, n_fit=5000, n_samples=1000, min_states=2, max_states=6 ) print(f\"Best model has {best_model.n_states} states\")","title":"Pattern 2: Model Comparison"},{"location":"pymc_concepts/#pattern-3-high-quality-sampling","text":"from pytau.changepoint_model import mcmc_fit # Use MCMC for publication-quality results model_obj = SingleTastePoisson(data_array=data, n_states=3) model = model_obj.generate_model() model, trace, lambda_stack, tau_samples, _ = mcmc_fit(model, samples=2000) # Check convergence import arviz as az print(az.summary(trace, var_names=['tau', 'lambda']))","title":"Pattern 3: High-Quality Sampling"},{"location":"pymc_concepts/#additional-resources","text":"PyMC Documentation - Official PyMC guide PyMC Examples - Gallery of example models Bayesian Methods for Hackers - Practical introduction to Bayesian methods ArviZ Documentation - Tools for analyzing Bayesian models PyMC Discourse - Community forum for questions","title":"Additional Resources"},{"location":"pymc_concepts/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"pymc_concepts/#my-samples-look-weird","text":"Check convergence: import arviz as az az.plot_trace(trace) # Look for \"hairy caterpillar\" patterns print(az.summary(trace)) # Check r_hat values (should be ~1.0)","title":"\"My samples look weird\""},{"location":"pymc_concepts/#inference-is-too-slow","text":"Try these approaches: 1. Use ADVI instead of MCMC for initial exploration 2. Reduce the number of samples 3. Simplify the model (fewer states) 4. Use NumPyro backend for MCMC: pm.sample(nuts_sampler='numpyro')","title":"\"Inference is too slow\""},{"location":"pymc_concepts/#results-dont-match-my-expectations","text":"Validate your model: 1. Check posterior predictive samples 2. Visualize the fitted changepoints on your data 3. Try different prior specifications 4. Ensure data preprocessing is correct","title":"\"Results don't match my expectations\""},{"location":"pymc_concepts/#next-steps","text":"Now that you understand the core PyMC concepts, you can: Explore the API documentation for detailed function references Try the example notebooks in pytau/how_to/notebooks/ Experiment with different model types for your data Customize models for your specific research questions For questions or issues, please visit the PyTau GitHub repository .","title":"Next Steps"}]}